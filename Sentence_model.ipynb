{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A generative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we did was to download Hans Christian Andersen fairytales from Gutenberg.\n",
    "\n",
    "What do we need to look out for when cleaning the data:\n",
    "\n",
    "- Punctuation \n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "- Capital letters as headers, might want to delete these\n",
    "- Change everything to lower caser \n",
    "- Illustrations, delete them, written as ([Illustration: _His limbs were numbed, his beautiful eyes were closing.-])\n",
    "\n",
    "\n",
    "We have to decide the input sequences, which is the number of words that the model will train on. This is also the number of words which will be used as the seed text, when using the model to actually generate new sequences. \n",
    "\n",
    "In the tutorial 50 words are suggested. We will start with that and then in the fifth lesson we might try to only use sequences inside sentences, though this will minimize the amount of training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on the following two coding blocks:\n",
    "https://www.pythonforbeginners.com/files/reading-and-writing-files-in-python\n",
    "- How to open text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import re\n",
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "from random import randint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a function to load the text into memory\n",
    "def load_text(filename): #we decide that the function is called load_text\n",
    "    file = open(filename, 'r', encoding = 'utf-8') #open() is used to open/loading a filename, the mode indicates how it should be opened, r = read, w = writing, a = appending \n",
    "    text = file.read() # read the file and assign it to the variable text\n",
    "    file.close() #close the file again\n",
    "    return text #ends function and specify what output the want, we want text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set path and working directory\n",
    "os.chdir(\"C:\\\\Users\\\\au581290\\\\Desktop\\\\Data\")\n",
    "path = \"C:\\\\Users\\\\au581290\\\\Desktop\\\\Data\"\n",
    "\n",
    "#concatenate text files\n",
    "filenames = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "filenames\n",
    "\n",
    "with open('adventures.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHAPTER 1\n",
      "\n",
      "How it happened that Mastro Cherry, carpenter, found a piece of wood\n",
      "that wept and laughed like a child.\n",
      "\n",
      "\n",
      "Centuries ago there lived--\n",
      "\n",
      "“A king!” my little readers will say immediately.\n",
      "\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load document\n",
    "in_filename = 'Data/fairytales.txt'\n",
    "doc = load_text(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      "\n",
      "How it happened that Mastro Cherry, carpenter, found a piece of wood\n",
      "that wept and laughed like a child.\n",
      "\n",
      "\n",
      "Centuries ago there lived--\n",
      "\n",
      "“A king!” my little readers will say immediately.\n",
      "\n",
      "No, children, you are mistaken. Once upon a time there was a piece of\n",
      "wood. It was not an expensive piece of wood. Far from it. Just a common\n",
      "block of firewood, one of those thick, solid logs that are put on the\n",
      "fire in winter to make cold rooms cozy and warm.\n",
      "\n",
      "I do not know how this really happened, yet the fact remains that\n",
      "one fine day this piece of wood found itself in the shop of an old\n",
      "carpenter. His real name was Mastro Antonio, but everyone called him\n",
      "Mastro Cherry, for the tip of his nose was so round and red and shiny\n",
      "that it looked like a ripe cherry.\n",
      "\n",
      "As soon as he saw that piece of wood, Mastro Cherry was filled with joy.\n",
      "Rubbing his hands together happily, he mumbled half to himself:\n",
      "\n",
      "“This has come in the nick of time. I shall use it to make the leg of a\n",
      "table.”\n",
      "\n",
      "He grasped the hatc\n"
     ]
    }
   ],
   "source": [
    "#remove headers\n",
    "doc = re.sub(r'[A-Z]{2,}', '', doc) #remove capital letters everytime there is more than two \n",
    "\n",
    "print(doc[:1000]) #lets look at it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the following code: https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\n",
    "How to use translate together with the helper function maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn the document into clean tokens\n",
    "\n",
    "\n",
    "def clean_doc(doc): #make a function called clean_doc\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split() #get a list with all words (tokens)\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation) #third argument specifies the character we want to delete\n",
    "\ttokens = [w.translate(table) for w in tokens] #use translation mapping from table and loop through all words in token\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #isalpha() checks whether the string consists of alphabetic characters only. Replace word with word if it is alphabetical. \n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens] #replace word in lower case with word in token \n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following coding blocks are part of lesson 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/signeklovekjaer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "35188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Far from it.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load nltk library\n",
    "nltk.download('punkt')\n",
    "\n",
    "#define tokenizer, use pretrained from the corpus\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#assign doc to data\n",
    "data = doc\n",
    "\n",
    "#divide into sentences\n",
    "sentence_list = sent_tokenize(data)\n",
    "\n",
    "#print the number of sentences\n",
    "print(len(sentence_list))\n",
    "\n",
    "#check the type, it's a list\n",
    "type(sentence_list)\n",
    "\n",
    "#index to see an example\n",
    "sentence_list[5] #need to remove the punctuation and have in lower case\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'upon', 'a', 'time', 'there', 'was', 'a', 'piece', 'of', 'wood']\n",
      "['far', 'from', 'it']\n"
     ]
    }
   ],
   "source": [
    "#check if it works on one sentence in the sentence list\n",
    "print(clean_doc(sentence_list[3])) #it does\n",
    "\n",
    "#make an empty list\n",
    "sentence_clean_list = []\n",
    "\n",
    "#make a loop which runs through the list of sentences and uses the clean_doc() function\n",
    "for sentence in sentence_list:\n",
    "    y = clean_doc(sentence) #save in y and append to empty list\n",
    "    sentence_clean_list.append(y)\n",
    "    \n",
    "\n",
    "#print(sentence_clean_list[:10])\n",
    "\n",
    "#check to see if the punctuation is removed and if it's in lower case\n",
    "print(sentence_clean_list[5]) #it is\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check the length of the sentences, as we need to use this then either padding or truncating the sentences into sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "348\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check the length of a random sentence\n",
    "print(len(sentence_clean_list[3]))\n",
    "\n",
    "length = []\n",
    "for sentence in sentence_clean_list:\n",
    "    l = len(sentence)\n",
    "    length.append(l)\n",
    "\n",
    "    \n",
    "print(max(length))\n",
    "\n",
    "print(min(length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "1\n",
      "Sentences removed after deleting sentences shorter than 1: 151\n",
      "Sentences removed after deleting sentences longer than : 67\n"
     ]
    }
   ],
   "source": [
    "#remove sentences less than 1 word long and longer than 130\n",
    "sentence_new_list1 = [s for s in sentence_clean_list if len(s) >= 1] \n",
    "sentence_new_list = [s for s in sentence_new_list1 if len(s) <= 130]\n",
    "\n",
    "\n",
    "length_new = []\n",
    "for sentence in sentence_new_list:\n",
    "    l = len(sentence)\n",
    "    length_new.append(l)\n",
    "\n",
    "    \n",
    "print(max(length_new))\n",
    "\n",
    "print(min(length_new))\n",
    "\n",
    "\n",
    "#check how many sentences we have removed\n",
    "print('Sentences removed after deleting sentences shorter than 1:', len(sentence_clean_list) - len(sentence_new_list1) )\n",
    "print('Sentences removed after deleting sentences longer than :', len(sentence_new_list1) - len(sentence_new_list)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "far from it\n",
      "how it happened that mastro cherry carpenter found a piece of wood that wept and laughed like a child\n"
     ]
    }
   ],
   "source": [
    "# turn into (not separated) sentence, check method on number 5 in the list\n",
    "line = ' '.join(sentence_new_list[5]) \n",
    "\n",
    "print(line) #it works\n",
    "\n",
    "#create empty list\n",
    "sequences_sent = []\n",
    "\n",
    "#create a loop to do it at all the sentences\n",
    "for sentence in sentence_new_list:\n",
    "    line = ' '.join(sentence)\n",
    "    sequences_sent.append(line)\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the sentences to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w', encoding = 'utf-8')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save sentences to file\n",
    "out_filename = 'Data/clean_text_sentences.txt'\n",
    "save_doc(sequences_sent, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how it happened that mastro cherry carpenter found a piece of wood that wept and laughed like a child\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "34970"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the cleaned data with sentences\n",
    "doc = load_text('Data/clean_text_sentences.txt')\n",
    "\n",
    "#split the text according to line shift\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list\n",
    "\n",
    "\n",
    "print(lines[0])\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82, 13, 259, 8, 795, 1622, 1623, 117, 4, 356, 6, 224, 8, 938, 2, 730, 88, 4, 770]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "sequences_sent = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "#print(sequences_sent[:100]) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model all the input sequences need to be the same length. We therefore need to pad or truncate the different sentences. When you pad the sequences, you add zeros, so that they all become the same length (lenght of the longest sentence. In that case we will have to create an masking input layer which will ignore padded values. This means that padded inputs have no impact on learning. But we will start by padding all the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   88    4  770]\n",
      " [   0    0    0 ...   51  209  308]\n",
      " [   0    0    0 ...   11   66 3699]\n",
      " ...\n",
      " [   0    0    0 ... 3498  179   92]\n",
      " [   0    0    0 ...  782   22   80]\n",
      " [   0    0    0 ...  471   55  290]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will padd zeros in front, pre is the default option\n",
    "padded = pad_sequences(sequences_sent)\n",
    "print(padded)\n",
    "\n",
    "\n",
    "len(padded[3]) #all sequences are now 130 long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7905"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  730   88    4]\n",
      " [   0    0    0 ... 3026   51  209]\n",
      " [   0    0    0 ...  217   11   66]\n",
      " ...\n",
      " [   0    0    0 ...    1 3498  179]\n",
      " [   0    0    0 ... 4804  782   22]\n",
      " [   0    0    0 ...    1  471   55]]\n",
      "[ 770  308 3699 ...   92   80  290]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "1.0\n",
      "0.0\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences_sent = array(padded) #create an array with the sequences\n",
    "\n",
    "X, y = sequences_sent[:,:-1], sequences_sent[:,-1] #separate into input and output sequences\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding\n",
    "print(y)\n",
    "seq_length_sent = X.shape[1] #set sequence length to the number of columns in X\n",
    "\n",
    "len(X[1])\n",
    "len(y)\n",
    "\n",
    "print(y[[3]])\n",
    "\n",
    "print(max(y[3])) #check if there is one-hot encoding for the output word\n",
    "\n",
    "print(min(y[3]))\n",
    "\n",
    "print(len(X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_17 (Masking)         (None, 129)               0         \n",
      "_________________________________________________________________\n",
      "embedding_16 (Embedding)     (None, 129, 50)           395250    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 129, 100)          60400     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 7905)              798405    \n",
      "=================================================================\n",
      "Total params: 1,344,555\n",
      "Trainable params: 1,344,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential() #define model, use sequential model function\n",
    "model.add(Masking(mask_value=0, input_shape=(seq_length_sent,)))\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length_sent)) #add embedding layer, 50 is the dimensions which will be used to represent each word vector\n",
    "model.add(LSTM(100, return_sequences=True)) #add hidden layer, long short term memory layer\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu')) #dense specifies the structure of the neural network, 100 refer to that there are 100 neurons in the first hidden layer, as defined above. Relu is an argument of rectified linear unit.  \n",
    "model.add(Dense(vocab_size, activation='softmax')) #softmax transform to probabilities \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6287/6287 [==============================] - 70s 11ms/step - loss: 8.1819 - acc: 0.0247\n",
      "Epoch 2/100\n",
      "6287/6287 [==============================] - 75s 12ms/step - loss: 7.0873 - acc: 0.0282\n",
      "Epoch 3/100\n",
      "6287/6287 [==============================] - 67s 11ms/step - loss: 6.9626 - acc: 0.0310\n",
      "Epoch 4/100\n",
      "6287/6287 [==============================] - 68s 11ms/step - loss: 6.9271 - acc: 0.0310\n",
      "Epoch 5/100\n",
      "6287/6287 [==============================] - 69s 11ms/step - loss: 6.9125 - acc: 0.0310\n",
      "Epoch 6/100\n",
      "6287/6287 [==============================] - 72s 11ms/step - loss: 6.9026 - acc: 0.0310\n",
      "Epoch 7/100\n",
      "6287/6287 [==============================] - 71s 11ms/step - loss: 6.8800 - acc: 0.0310\n",
      "Epoch 8/100\n",
      "6287/6287 [==============================] - 71s 11ms/step - loss: 6.7940 - acc: 0.0312\n",
      "Epoch 9/100\n",
      "6287/6287 [==============================] - 90s 14ms/step - loss: 6.7209 - acc: 0.0321\n",
      "Epoch 10/100\n",
      "6287/6287 [==============================] - 112s 18ms/step - loss: 6.6588 - acc: 0.0321\n",
      "Epoch 11/100\n",
      "6287/6287 [==============================] - 99s 16ms/step - loss: 6.5894 - acc: 0.0329\n",
      "Epoch 12/100\n",
      "6287/6287 [==============================] - 113s 18ms/step - loss: 6.5216 - acc: 0.0344\n",
      "Epoch 13/100\n",
      "6287/6287 [==============================] - 110s 17ms/step - loss: 6.4561 - acc: 0.0317\n",
      "Epoch 14/100\n",
      "6287/6287 [==============================] - 94s 15ms/step - loss: 6.3883 - acc: 0.0326\n",
      "Epoch 15/100\n",
      "6287/6287 [==============================] - 120s 19ms/step - loss: 6.3241 - acc: 0.0345\n",
      "Epoch 16/100\n",
      "6287/6287 [==============================] - 100s 16ms/step - loss: 6.2547 - acc: 0.0348\n",
      "Epoch 17/100\n",
      "6287/6287 [==============================] - 100s 16ms/step - loss: 6.1709 - acc: 0.0380\n",
      "Epoch 18/100\n",
      "6287/6287 [==============================] - 128s 20ms/step - loss: 6.0790 - acc: 0.0371\n",
      "Epoch 19/100\n",
      "6287/6287 [==============================] - 107s 17ms/step - loss: 5.9974 - acc: 0.0364\n",
      "Epoch 20/100\n",
      "6287/6287 [==============================] - 111s 18ms/step - loss: 5.9290 - acc: 0.0399\n",
      "Epoch 21/100\n",
      "6287/6287 [==============================] - 103s 16ms/step - loss: 5.8588 - acc: 0.0426\n",
      "Epoch 22/100\n",
      "6287/6287 [==============================] - 102s 16ms/step - loss: 5.7937 - acc: 0.0441\n",
      "Epoch 23/100\n",
      "6287/6287 [==============================] - 32746s 5s/step - loss: 5.7422 - acc: 0.0472\n",
      "Epoch 24/100\n",
      "6287/6287 [==============================] - 58s 9ms/step - loss: 5.6847 - acc: 0.0484\n",
      "Epoch 25/100\n",
      "6287/6287 [==============================] - 72s 12ms/step - loss: 5.6419 - acc: 0.0496\n",
      "Epoch 26/100\n",
      "6287/6287 [==============================] - 70s 11ms/step - loss: 5.5816 - acc: 0.0525\n",
      "Epoch 27/100\n",
      "6287/6287 [==============================] - 72s 12ms/step - loss: 5.5313 - acc: 0.0519\n",
      "Epoch 28/100\n",
      "6287/6287 [==============================] - 66s 11ms/step - loss: 5.4847 - acc: 0.0498\n",
      "Epoch 29/100\n",
      "6287/6287 [==============================] - 70s 11ms/step - loss: 5.4378 - acc: 0.0520\n",
      "Epoch 30/100\n",
      "6287/6287 [==============================] - 69s 11ms/step - loss: 5.3862 - acc: 0.0619\n",
      "Epoch 31/100\n",
      "6287/6287 [==============================] - 75s 12ms/step - loss: 5.3463 - acc: 0.0585\n",
      "Epoch 32/100\n",
      "6287/6287 [==============================] - 80s 13ms/step - loss: 5.2980 - acc: 0.0604\n",
      "Epoch 33/100\n",
      "6287/6287 [==============================] - 70s 11ms/step - loss: 5.2488 - acc: 0.0663\n",
      "Epoch 34/100\n",
      "6287/6287 [==============================] - 68s 11ms/step - loss: 5.2096 - acc: 0.0627\n",
      "Epoch 35/100\n",
      "6287/6287 [==============================] - 68s 11ms/step - loss: 5.1580 - acc: 0.0724\n",
      "Epoch 36/100\n",
      "6287/6287 [==============================] - 69s 11ms/step - loss: 5.1325 - acc: 0.0649\n",
      "Epoch 37/100\n",
      "6287/6287 [==============================] - 72s 11ms/step - loss: 5.0968 - acc: 0.0743\n",
      "Epoch 38/100\n",
      "6287/6287 [==============================] - 71s 11ms/step - loss: 5.0428 - acc: 0.0744\n",
      "Epoch 39/100\n",
      "6287/6287 [==============================] - 72s 11ms/step - loss: 4.9928 - acc: 0.0773\n",
      "Epoch 40/100\n",
      "6287/6287 [==============================] - 72s 11ms/step - loss: 4.9507 - acc: 0.0805\n",
      "Epoch 41/100\n",
      "6287/6287 [==============================] - 72s 12ms/step - loss: 4.8945 - acc: 0.0840\n",
      "Epoch 42/100\n",
      "6287/6287 [==============================] - 78s 12ms/step - loss: 4.8605 - acc: 0.0830\n",
      "Epoch 43/100\n",
      "6287/6287 [==============================] - 74s 12ms/step - loss: 4.8100 - acc: 0.0875\n",
      "Epoch 44/100\n",
      "6287/6287 [==============================] - 74s 12ms/step - loss: 4.7577 - acc: 0.0910\n",
      "Epoch 45/100\n",
      "6287/6287 [==============================] - 75s 12ms/step - loss: 4.7196 - acc: 0.0951\n",
      "Epoch 46/100\n",
      "6287/6287 [==============================] - 75s 12ms/step - loss: 4.6682 - acc: 0.0961\n",
      "Epoch 47/100\n",
      "6287/6287 [==============================] - 76s 12ms/step - loss: 4.6321 - acc: 0.0993\n",
      "Epoch 48/100\n",
      "6287/6287 [==============================] - 76s 12ms/step - loss: 4.6047 - acc: 0.1007\n",
      "Epoch 49/100\n",
      "6287/6287 [==============================] - 85s 14ms/step - loss: 4.5609 - acc: 0.0997\n",
      "Epoch 50/100\n",
      "6287/6287 [==============================] - 96s 15ms/step - loss: 4.5087 - acc: 0.1018\n",
      "Epoch 51/100\n",
      "6287/6287 [==============================] - 97s 15ms/step - loss: 4.4796 - acc: 0.1055\n",
      "Epoch 52/100\n",
      "6287/6287 [==============================] - 98s 16ms/step - loss: 4.4273 - acc: 0.1120\n",
      "Epoch 53/100\n",
      "6287/6287 [==============================] - 100s 16ms/step - loss: 4.3894 - acc: 0.1166\n",
      "Epoch 54/100\n",
      "6287/6287 [==============================] - 104s 17ms/step - loss: 4.3468 - acc: 0.1180\n",
      "Epoch 55/100\n",
      "6287/6287 [==============================] - 81s 13ms/step - loss: 4.3038 - acc: 0.1209\n",
      "Epoch 56/100\n",
      "6287/6287 [==============================] - 81s 13ms/step - loss: 4.2631 - acc: 0.1271\n",
      "Epoch 57/100\n",
      "6287/6287 [==============================] - 81s 13ms/step - loss: 4.2304 - acc: 0.1288\n",
      "Epoch 58/100\n",
      "6287/6287 [==============================] - 81s 13ms/step - loss: 4.1934 - acc: 0.1279\n",
      "Epoch 59/100\n",
      "6287/6287 [==============================] - 82s 13ms/step - loss: 4.1607 - acc: 0.1323\n",
      "Epoch 60/100\n",
      "6287/6287 [==============================] - 83s 13ms/step - loss: 4.1508 - acc: 0.1322\n",
      "Epoch 61/100\n",
      "6287/6287 [==============================] - 82s 13ms/step - loss: 4.0965 - acc: 0.1398\n",
      "Epoch 62/100\n",
      "6287/6287 [==============================] - 83s 13ms/step - loss: 4.0543 - acc: 0.1467\n",
      "Epoch 63/100\n",
      "6287/6287 [==============================] - 83s 13ms/step - loss: 4.0096 - acc: 0.1517\n",
      "Epoch 64/100\n",
      "6287/6287 [==============================] - 83s 13ms/step - loss: 3.9673 - acc: 0.1554\n",
      "Epoch 65/100\n",
      "6287/6287 [==============================] - 83s 13ms/step - loss: 3.9256 - acc: 0.1597\n",
      "Epoch 66/100\n",
      "6287/6287 [==============================] - 84s 13ms/step - loss: 3.8759 - acc: 0.1657\n",
      "Epoch 67/100\n",
      "6287/6287 [==============================] - 128s 20ms/step - loss: 3.8552 - acc: 0.1662\n",
      "Epoch 68/100\n",
      "6287/6287 [==============================] - 88s 14ms/step - loss: 3.8236 - acc: 0.1700\n",
      "Epoch 69/100\n",
      "6287/6287 [==============================] - 89s 14ms/step - loss: 3.7933 - acc: 0.1737\n",
      "Epoch 70/100\n",
      "6287/6287 [==============================] - 98s 16ms/step - loss: 3.7739 - acc: 0.1739\n",
      "Epoch 71/100\n",
      "6287/6287 [==============================] - 101s 16ms/step - loss: 3.7536 - acc: 0.1746\n",
      "Epoch 72/100\n",
      "6287/6287 [==============================] - 121s 19ms/step - loss: 3.6951 - acc: 0.1829\n",
      "Epoch 73/100\n",
      "6287/6287 [==============================] - 110s 17ms/step - loss: 3.6472 - acc: 0.1909\n",
      "Epoch 74/100\n",
      "6287/6287 [==============================] - 124s 20ms/step - loss: 3.6008 - acc: 0.1944\n",
      "Epoch 75/100\n",
      "6287/6287 [==============================] - 127s 20ms/step - loss: 3.5700 - acc: 0.2014\n",
      "Epoch 76/100\n",
      "6287/6287 [==============================] - 110s 18ms/step - loss: 3.5411 - acc: 0.2033\n",
      "Epoch 77/100\n",
      "6287/6287 [==============================] - 122s 19ms/step - loss: 3.5507 - acc: 0.1985\n",
      "Epoch 78/100\n",
      "6287/6287 [==============================] - 123s 20ms/step - loss: 3.4895 - acc: 0.2066\n",
      "Epoch 79/100\n",
      "6287/6287 [==============================] - 168s 27ms/step - loss: 3.4498 - acc: 0.2130\n",
      "Epoch 80/100\n",
      "6287/6287 [==============================] - 262s 42ms/step - loss: 3.4188 - acc: 0.2189\n",
      "Epoch 81/100\n",
      "6287/6287 [==============================] - 177s 28ms/step - loss: 3.3652 - acc: 0.2260\n",
      "Epoch 82/100\n",
      "6287/6287 [==============================] - 1397s 222ms/step - loss: 3.3288 - acc: 0.2356\n",
      "Epoch 83/100\n",
      "6287/6287 [==============================] - 139s 22ms/step - loss: 3.3098 - acc: 0.2359\n",
      "Epoch 84/100\n",
      "6287/6287 [==============================] - 120s 19ms/step - loss: 3.2748 - acc: 0.2333\n",
      "Epoch 85/100\n",
      "6287/6287 [==============================] - 110s 17ms/step - loss: 3.2482 - acc: 0.2422\n",
      "Epoch 86/100\n",
      "6287/6287 [==============================] - 115s 18ms/step - loss: 3.2132 - acc: 0.2461\n",
      "Epoch 87/100\n",
      "6287/6287 [==============================] - 115s 18ms/step - loss: 3.1605 - acc: 0.2567\n",
      "Epoch 88/100\n",
      "6287/6287 [==============================] - 109s 17ms/step - loss: 3.1212 - acc: 0.2609\n",
      "Epoch 89/100\n",
      "6287/6287 [==============================] - 108s 17ms/step - loss: 3.0925 - acc: 0.2710\n",
      "Epoch 90/100\n",
      "6287/6287 [==============================] - 115s 18ms/step - loss: 3.0575 - acc: 0.2702\n",
      "Epoch 91/100\n",
      "6287/6287 [==============================] - 118s 19ms/step - loss: 3.0519 - acc: 0.2734\n",
      "Epoch 92/100\n",
      "6287/6287 [==============================] - 121s 19ms/step - loss: 3.0237 - acc: 0.2704\n",
      "Epoch 93/100\n",
      "6287/6287 [==============================] - 113s 18ms/step - loss: 2.9693 - acc: 0.2871\n",
      "Epoch 94/100\n",
      "6287/6287 [==============================] - 102s 16ms/step - loss: 2.9367 - acc: 0.2893\n",
      "Epoch 95/100\n",
      "6287/6287 [==============================] - 109s 17ms/step - loss: 2.8893 - acc: 0.3059\n",
      "Epoch 96/100\n",
      "6287/6287 [==============================] - 102s 16ms/step - loss: 2.8566 - acc: 0.3084\n",
      "Epoch 97/100\n",
      "6287/6287 [==============================] - 109s 17ms/step - loss: 2.8257 - acc: 0.3094\n",
      "Epoch 98/100\n",
      "6287/6287 [==============================] - 112s 18ms/step - loss: 2.7845 - acc: 0.3270\n",
      "Epoch 99/100\n",
      "6287/6287 [==============================] - 99s 16ms/step - loss: 2.7682 - acc: 0.3289\n",
      "Epoch 100/100\n",
      "6287/6287 [==============================] - 103s 16ms/step - loss: 2.7396 - acc: 0.3251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2667b240>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #it's a multilevel predicter, because it's categorical, we want a measure which is accuracy, and a cost function which is optimized by adam\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5_sentence')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl_sentence', 'wb')) #w stands for write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'far from it'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need the text, so we can use a sequence as a source input for generating the text\n",
    "doc = load_text('Data/clean_text_sentences.txt') #load cleaned sequences \n",
    "\n",
    "lines = doc.split('\\n') #split by new line\n",
    "\n",
    "\n",
    "lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "#specify the sequence length\n",
    "seq_length_sent = X.shape[1]\n",
    "\n",
    "print(seq_length_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = load_model('model.h5_sentence')\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl_sentence', 'rb')) #r stands for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you can imagine how fast he traveled when i tell you that they reached the palace in just half the time it had taken the wooden horse to get there\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select a seed text\n",
    "seed_text_sentence = lines[randint(0,len(lines))] #randint returns a random integer from 0 to the length of lines, index it\n",
    "print(seed_text_sentence + '\\n') #print the selected seed text and make a line shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0   11  114  523   83  387    5 1124   33   12  144   11    8   22\n",
      "   256    1  154    9  129  284    1   68   13   17  324    1  587  208\n",
      "     3  149   53]]\n"
     ]
    }
   ],
   "source": [
    "#use tokenizer to assign an integer to every word in the seed text\n",
    "encoded_sentence = tokenizer.texts_to_sequences([seed_text_sentence])[0]\n",
    "\n",
    "#pad the seed text\n",
    "encoded_sentence = pad_sequences([encoded_sentence], maxlen=seq_length_sent, truncating='pre')\n",
    "\n",
    "print(encoded_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the model.preict_classes will return the index of the word with the highest probability \n",
    "yhat = model.predict_classes(encoded_sentence, verbose=0) #verbose specifies how  you want to 'see' the training progress for each epoch\n",
    "\n",
    "yhat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'more'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look up in the tokenizer what word the indexed word corresponds to\n",
    "\n",
    "out_word = '' #empty variable to store the word output word in \n",
    "for word, index in tokenizer.word_index.items(): #loop through the tokenizers\n",
    "\tif index == yhat: #if the index is equally to yhat, assign the word to the variable out_word\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "        \n",
    "out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the input sequences will get too long, when adding the output word, need to make sure that it's always 129 words long\n",
    "#encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "def generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text_sentence #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length_sentence, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more man man themselves gone free gone do go man ask place fastened heard me afterward before you themselves about\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, 20)\n",
    "print(generated)\n",
    "\n",
    "\n",
    "#save the generated text\n",
    "\n",
    "out_filename = 'Data/first_output_sentence.txt'\n",
    "\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to look at in lecture 5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input sequences\n",
    "\n",
    "\n",
    "We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "- Instead, to keep the example brief, we will let all of the text flow together and train the model to predict the next word across sentences, paragraphs, and even books or chapters in the text.\n",
    "\n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "\n",
    "- FInd more fairytales, data is not big enough\n",
    "\n",
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp_project_keras]",
   "language": "python",
   "name": "conda-env-nlp_project_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

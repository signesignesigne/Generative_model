{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A generative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Load data and get acquainted with the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step by step:\n",
    "\n",
    "- Choose and download considerably amount of data from Gutenberg.org\n",
    "- Look into the data, review some of the text\n",
    "- Observe things (e.g. strange names), which must be considered when preparing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we did was to download Hans Christian Andersen fairytales from Gutenberg and look into some of the texts.\n",
    "\n",
    "What do we need to look out for when cleaning the data:\n",
    "\n",
    "- Punctuation \n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "- Capital letters as headers, might want to delete these\n",
    "- Change everything to lower caser \n",
    "- Illustrations, delete them, written as ([Illustration: _His limbs were numbed, his beautiful eyes were closing.-])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "import re\n",
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "from random import randint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Masking\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on the following coding block:\n",
    "https://www.pythonforbeginners.com/files/reading-and-writing-files-in-python\n",
    "- How to open text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a function to load the text into memory\n",
    "def load_text(filename): #we decide that the function is called load_text\n",
    "    file = open(filename, 'r') #open() is used to open/loading a filename \n",
    "    #the mode indicates how it should be opened, r = read, w = writing, a = appending \n",
    "    text = file.read() # read the file and assign it to the variable text\n",
    "    file.close() #close the file again\n",
    "    return text #ends function and specify what output the want, we want text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved all the text files in a local folder on the computer. We need to first load these and then combine them into a single file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define a path, the folder where the text files are placed\n",
    "path = \"/Users/signeklovekjaer/Documents/CognitiveScience/3.semester/Learning_with_digital_media/Exam_project/Jupiter_notebook/Data/\"\n",
    "\n",
    "#make a list that contains all the names of the files in the directory and assign it to a variable\n",
    "filenames = os.listdir( path )\n",
    "\n",
    "# print all the files in the directory \n",
    "\n",
    "#for file in filenames:\n",
    "   #print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use .remove() to remove filenames \n",
    "\n",
    "filenames.remove('.DS_Store');\n",
    "\n",
    "filenames.remove('fairytales.txt');\n",
    "\n",
    "filenames.remove('clean_text_sentences.txt');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable called filenames, now contains all the text files in the folder 'Data'. We ran the model on a different computer than mine, as the processing power isn't strong enough to handle so much data. But to show how the code works, I will just overwrite the variable filenames with just two textfile names.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['32571-0.txt', '2435-0.txt']\n"
     ]
    }
   ],
   "source": [
    "#filenames with less data\n",
    "filenames = ['32571-0.txt','2435-0.txt']\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#change working directory to where the text files are placed\n",
    "os.chdir('/Users/signeklovekjaer/Documents/CognitiveScience/3.semester/Learning_with_digital_media/Exam_project/Jupiter_notebook/Data/')\n",
    "\n",
    "#combine all the text files into a single file called \"fairytales.text\"\n",
    "with open('fairytales.txt', 'w') as outfile: #specify where the file should be saved and what it should be called (if a file with that name already exists, it will be overwritten, 'w' is used to edit and write new information to a file \n",
    "    for text in filenames: #for all the text in the filenames, do the following...\n",
    "        with open(text) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿THE FIR TREE\n",
      "\n",
      "\n",
      "FAR away in the forest, where the warm sun and the fresh air made a\n",
      "sweet resting place, grew a pretty little fir tree. The situation was\n",
      "all that could be desired; and yet the tree was not happy, it wished so\n",
      "much to be like its tall companions, the pines and firs which grew\n",
      "around \n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'fairytales.txt'\n",
    "doc = load_text(in_filename)\n",
    "print(doc[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Prepare the training data\n",
    "\n",
    "Step by step:\n",
    "- Clean the text e.g. normalize words to lowercase and delete punctuation from words, to reduce vocabulary size\n",
    "- Organize the words into sequences and save these to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿  \n",
      "\n",
      "\n",
      " away in the forest, where the warm sun and the fresh air made a\n",
      "sweet resting place, grew a pretty little fir tree. The situation was\n",
      "all that could be desired; and yet the tree was not happy, it wished so\n",
      "much to be like its tall companions, the pines and firs which grew\n",
      "around it.\n",
      "\n",
      "The sun \n"
     ]
    }
   ],
   "source": [
    "#remove headers\n",
    "doc = re.sub(r'[A-Z]{2,}', '', doc) #remove capital letters everytime there is more than two \n",
    "\n",
    "print(doc[0:300]) #lets look at it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the following code: https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\n",
    "- How to use translate together with the helper function maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create function that can clean the text\n",
    "def clean_doc(doc): #make a function called clean_doc\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split() #get a list with all words (tokens)\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation) #third argument specifies the character we want to delete\n",
    "\ttokens = [w.translate(table) for w in tokens] #use translation mapping from table and loop through all words in token\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #isalpha() checks whether the string consists of alphabetic characters only. Replace word with word if it is alphabetical. \n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens] #replace word in lower case with word in token \n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['away', 'in', 'the', 'forest', 'where', 'the', 'warm', 'sun', 'and', 'the', 'fresh', 'air', 'made', 'a', 'sweet', 'resting', 'place', 'grew', 'a', 'pretty']\n",
      "Total Tokens: 39079\n",
      "Unique Tokens: 3974\n"
     ]
    }
   ],
   "source": [
    "#use the cleaning function on the whole document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens)) #print total number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #group similar tokens and print the number\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text is cleaned and we need to divide into sequences.\n",
    "\n",
    "We have to decide the length of the input sequences, which is the number of words that the model will train on. This is also the number of words which will be used as the seed text, when using the model to actually generate new sequences. \n",
    "\n",
    "In the tutorial 50 words are suggested. We will start with that and then in the fifth lesson we might try to only use sequences inside sentences, though this will minimize the amount of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 39028\n",
      "the forest where the warm sun and the fresh air made a sweet resting place grew a pretty little fir tree the situation was all that could be desired and yet the tree was not happy it wished so much to be like its tall companions the pines and firs which\n"
     ]
    }
   ],
   "source": [
    "# organize the text into sequences of tokens\n",
    "length = 50 + 1 #define the length of the sequences, 1 is the output word\n",
    "sequences = list() #create an empty list, where we will store all the sequences\n",
    "\n",
    "for i in range(length, len(tokens)): #range([start], stop[, step])\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i] #make sequences with length of 50 + 1\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #join the tokens into a line in the sequence\n",
    "\t# store\n",
    "\tsequences.append(line) #append all the lines to the empty list created before the loop\n",
    "print('Total Sequences: %d' % len(sequences)) #print the total amount (length of sequence) of sequences \n",
    "\n",
    "\n",
    "print(sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename): #define function called save_doc() which takes two inputs\n",
    "\tdata = '\\n'.join(lines) #join lines, separate by line shift \n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'clean_text.txt'\n",
    "save_doc(sequences, out_filename)#use the function on the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 3 - Train the generative model\n",
    "\n",
    "Step by step:\n",
    " \n",
    "- Load the cleaned data and divide it into training sequences\n",
    "- Map each word to a unique integer i.e. word to vector representation\n",
    "- Separate the input sequences into input and output elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the cleaned data\n",
    "doc = load_text('clean_text.txt')\n",
    "\n",
    "#split the text according to line shift (every string is 51 words)\n",
    "lines = doc.split('\\n') #use split() function, \\n is a regular expression for line shift\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for getting keras and tensorflow packages to work:\n",
    "\n",
    "https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "https://www.tensorflow.org/install/source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93, 7, 1, 179, 67, 1, 246, 285, 2, 1, 332, 456, 103, 4, 676, 2093, 148, 261, 4, 270, 33, 260, 59, 1, 2092, 8, 28, 13, 48, 29, 1132, 2, 211, 1, 59, 8, 27, 302, 9, 491, 26, 104, 3, 29, 70, 140, 398, 1130, 1, 3973, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'of': 5,\n",
       " 'he': 6,\n",
       " 'in': 7,\n",
       " 'was': 8,\n",
       " 'it': 9,\n",
       " 'his': 10,\n",
       " 'they': 11,\n",
       " 'had': 12,\n",
       " 'that': 13,\n",
       " 'as': 14,\n",
       " 'with': 15,\n",
       " 'but': 16,\n",
       " 'for': 17,\n",
       " 'you': 18,\n",
       " 'she': 19,\n",
       " 'her': 20,\n",
       " 'i': 21,\n",
       " 'at': 22,\n",
       " 'on': 23,\n",
       " 'said': 24,\n",
       " 'him': 25,\n",
       " 'so': 26,\n",
       " 'not': 27,\n",
       " 'all': 28,\n",
       " 'be': 29,\n",
       " 'were': 30,\n",
       " 'is': 31,\n",
       " 'one': 32,\n",
       " 'little': 33,\n",
       " 'them': 34,\n",
       " 'when': 35,\n",
       " 'have': 36,\n",
       " 'their': 37,\n",
       " 'which': 38,\n",
       " 'this': 39,\n",
       " 'old': 40,\n",
       " 'who': 41,\n",
       " 'out': 42,\n",
       " 'there': 43,\n",
       " 'up': 44,\n",
       " 'then': 45,\n",
       " 'by': 46,\n",
       " 'from': 47,\n",
       " 'could': 48,\n",
       " 'very': 49,\n",
       " 'my': 50,\n",
       " 'are': 51,\n",
       " 'will': 52,\n",
       " 'we': 53,\n",
       " 'what': 54,\n",
       " 'no': 55,\n",
       " 'some': 56,\n",
       " 'if': 57,\n",
       " 'man': 58,\n",
       " 'tree': 59,\n",
       " 'into': 60,\n",
       " 'would': 61,\n",
       " 'do': 62,\n",
       " 'came': 63,\n",
       " 'been': 64,\n",
       " 'now': 65,\n",
       " 'me': 66,\n",
       " 'where': 67,\n",
       " 'time': 68,\n",
       " 'about': 69,\n",
       " 'like': 70,\n",
       " 'an': 71,\n",
       " 'did': 72,\n",
       " 'down': 73,\n",
       " 'how': 74,\n",
       " 'see': 75,\n",
       " 'before': 76,\n",
       " 'only': 77,\n",
       " 'son': 78,\n",
       " 'over': 79,\n",
       " 'two': 80,\n",
       " 'young': 81,\n",
       " 'beautiful': 82,\n",
       " 'king': 83,\n",
       " 'after': 84,\n",
       " 'thought': 85,\n",
       " 'your': 86,\n",
       " 'went': 87,\n",
       " 'took': 88,\n",
       " 'us': 89,\n",
       " 'or': 90,\n",
       " 'must': 91,\n",
       " 'than': 92,\n",
       " 'away': 93,\n",
       " 'day': 94,\n",
       " 'mother': 95,\n",
       " 'more': 96,\n",
       " 'other': 97,\n",
       " 'long': 98,\n",
       " 'can': 99,\n",
       " 'well': 100,\n",
       " 'great': 101,\n",
       " 'again': 102,\n",
       " 'made': 103,\n",
       " 'much': 104,\n",
       " 'himself': 105,\n",
       " 'once': 106,\n",
       " 'off': 107,\n",
       " 'am': 108,\n",
       " 'good': 109,\n",
       " 'shall': 110,\n",
       " 'niels': 111,\n",
       " 'prince': 112,\n",
       " 'round': 113,\n",
       " 'asked': 114,\n",
       " 'back': 115,\n",
       " 'home': 116,\n",
       " 'last': 117,\n",
       " 'never': 118,\n",
       " 'upon': 119,\n",
       " 'here': 120,\n",
       " 'stood': 121,\n",
       " 'looked': 122,\n",
       " 'house': 123,\n",
       " 'soon': 124,\n",
       " 'way': 125,\n",
       " 'quite': 126,\n",
       " 'paul': 127,\n",
       " 'come': 128,\n",
       " 'know': 129,\n",
       " 'told': 130,\n",
       " 'found': 131,\n",
       " 'get': 132,\n",
       " 'till': 133,\n",
       " 'head': 134,\n",
       " 'go': 135,\n",
       " 'look': 136,\n",
       " 'too': 137,\n",
       " 'while': 138,\n",
       " 'should': 139,\n",
       " 'its': 140,\n",
       " 'large': 141,\n",
       " 'these': 142,\n",
       " 'each': 143,\n",
       " 'nothing': 144,\n",
       " 'many': 145,\n",
       " 'cried': 146,\n",
       " 'left': 147,\n",
       " 'place': 148,\n",
       " 'even': 149,\n",
       " 'put': 150,\n",
       " 'tell': 151,\n",
       " 'saw': 152,\n",
       " 'father': 153,\n",
       " 'own': 154,\n",
       " 'might': 155,\n",
       " 'set': 156,\n",
       " 'first': 157,\n",
       " 'just': 158,\n",
       " 'people': 159,\n",
       " 'children': 160,\n",
       " 'trees': 161,\n",
       " 'morning': 162,\n",
       " 'lay': 163,\n",
       " 'room': 164,\n",
       " 'any': 165,\n",
       " 'through': 166,\n",
       " 'flowers': 167,\n",
       " 'eyes': 168,\n",
       " 'roses': 169,\n",
       " 'has': 170,\n",
       " 'water': 171,\n",
       " 'three': 172,\n",
       " 'friend': 173,\n",
       " 'labakan': 174,\n",
       " 'knew': 175,\n",
       " 'our': 176,\n",
       " 'got': 177,\n",
       " 'hand': 178,\n",
       " 'forest': 179,\n",
       " 'every': 180,\n",
       " 'white': 181,\n",
       " 'make': 182,\n",
       " 'woman': 183,\n",
       " 'let': 184,\n",
       " 'queen': 185,\n",
       " 'nest': 186,\n",
       " 'same': 187,\n",
       " 'whole': 188,\n",
       " 'elder': 189,\n",
       " 'kettle': 190,\n",
       " 'brought': 191,\n",
       " 'fell': 192,\n",
       " 'flew': 193,\n",
       " 'seen': 194,\n",
       " 'green': 195,\n",
       " 'apple': 196,\n",
       " 'seemed': 197,\n",
       " 'wife': 198,\n",
       " 'answered': 199,\n",
       " 'still': 200,\n",
       " 'such': 201,\n",
       " 'story': 202,\n",
       " 'began': 203,\n",
       " 'maiden': 204,\n",
       " 'side': 205,\n",
       " 'sparrows': 206,\n",
       " 'next': 207,\n",
       " 'heard': 208,\n",
       " 'ones': 209,\n",
       " 'castle': 210,\n",
       " 'yet': 211,\n",
       " 'passed': 212,\n",
       " 'world': 213,\n",
       " 'another': 214,\n",
       " 'take': 215,\n",
       " 'done': 216,\n",
       " 'under': 217,\n",
       " 'open': 218,\n",
       " 'name': 219,\n",
       " 'boy': 220,\n",
       " 'lived': 221,\n",
       " 'rose': 222,\n",
       " 'tweet': 223,\n",
       " 'birds': 224,\n",
       " 'going': 225,\n",
       " 'far': 226,\n",
       " 'ever': 227,\n",
       " 'better': 228,\n",
       " 'poor': 229,\n",
       " 'girl': 230,\n",
       " 'think': 231,\n",
       " 'something': 232,\n",
       " 'though': 233,\n",
       " 'princess': 234,\n",
       " 'sat': 235,\n",
       " 'together': 236,\n",
       " 'eat': 237,\n",
       " 'gave': 238,\n",
       " 'say': 239,\n",
       " 'branches': 240,\n",
       " 'horse': 241,\n",
       " 'give': 242,\n",
       " 'replied': 243,\n",
       " 'omar': 244,\n",
       " 'lizina': 245,\n",
       " 'warm': 246,\n",
       " 'oh': 247,\n",
       " 'town': 248,\n",
       " 'cannot': 249,\n",
       " 'others': 250,\n",
       " 'night': 251,\n",
       " 'heart': 252,\n",
       " 'herself': 253,\n",
       " 'felt': 254,\n",
       " 'bird': 255,\n",
       " 'bed': 256,\n",
       " 'golden': 257,\n",
       " 'door': 258,\n",
       " 'heads': 259,\n",
       " 'fir': 260,\n",
       " 'grew': 261,\n",
       " 'ground': 262,\n",
       " 'enough': 263,\n",
       " 'most': 264,\n",
       " 'fly': 265,\n",
       " 'moment': 266,\n",
       " 'days': 267,\n",
       " 'called': 268,\n",
       " 'gold': 269,\n",
       " 'pretty': 270,\n",
       " 'near': 271,\n",
       " 'also': 272,\n",
       " 'journey': 273,\n",
       " 'hung': 274,\n",
       " 'men': 275,\n",
       " 'things': 276,\n",
       " 'among': 277,\n",
       " 'however': 278,\n",
       " 'rode': 279,\n",
       " 'turned': 280,\n",
       " 'dead': 281,\n",
       " 'years': 282,\n",
       " 'happened': 283,\n",
       " 'reached': 284,\n",
       " 'sun': 285,\n",
       " 'times': 286,\n",
       " 'may': 287,\n",
       " 'always': 288,\n",
       " 'field': 289,\n",
       " 'find': 290,\n",
       " 'both': 291,\n",
       " 'leaves': 292,\n",
       " 'evening': 293,\n",
       " 'third': 294,\n",
       " 'cut': 295,\n",
       " 'full': 296,\n",
       " 'help': 297,\n",
       " 'thumbelina': 298,\n",
       " 'whom': 299,\n",
       " 'sent': 300,\n",
       " 'foot': 301,\n",
       " 'happy': 302,\n",
       " 'placed': 303,\n",
       " 'new': 304,\n",
       " 'fine': 305,\n",
       " 'rest': 306,\n",
       " 'branch': 307,\n",
       " 'hear': 308,\n",
       " 'tuk': 309,\n",
       " 'child': 310,\n",
       " 'really': 311,\n",
       " 'looking': 312,\n",
       " 'work': 313,\n",
       " 'sparrow': 314,\n",
       " 'returned': 315,\n",
       " 'sword': 316,\n",
       " 'few': 317,\n",
       " 'everything': 318,\n",
       " 'themselves': 319,\n",
       " 'spread': 320,\n",
       " 'yes': 321,\n",
       " 'life': 322,\n",
       " 'carried': 323,\n",
       " 'above': 324,\n",
       " 'fire': 325,\n",
       " 'sight': 326,\n",
       " 'sure': 327,\n",
       " 'high': 328,\n",
       " 'without': 329,\n",
       " 'country': 330,\n",
       " 'ogre': 331,\n",
       " 'fresh': 332,\n",
       " 'winter': 333,\n",
       " 'right': 334,\n",
       " 'earth': 335,\n",
       " 'taken': 336,\n",
       " 'kissed': 337,\n",
       " 'nor': 338,\n",
       " 'indeed': 339,\n",
       " 'anything': 340,\n",
       " 'love': 341,\n",
       " 'top': 342,\n",
       " 'towards': 343,\n",
       " 'became': 344,\n",
       " 'sleep': 345,\n",
       " 'stay': 346,\n",
       " 'spoke': 347,\n",
       " 'flower': 348,\n",
       " 'words': 349,\n",
       " 'cats': 350,\n",
       " 'keep': 351,\n",
       " 'meet': 352,\n",
       " 'quickly': 353,\n",
       " 'drew': 354,\n",
       " 'bright': 355,\n",
       " 'itself': 356,\n",
       " 'covered': 357,\n",
       " 'joy': 358,\n",
       " 'tried': 359,\n",
       " 'corner': 360,\n",
       " 'care': 361,\n",
       " 'sister': 362,\n",
       " 'thing': 363,\n",
       " 'wood': 364,\n",
       " 'cold': 365,\n",
       " 'behind': 366,\n",
       " 'box': 367,\n",
       " 'declared': 368,\n",
       " 'appeared': 369,\n",
       " 'pocket': 370,\n",
       " 'between': 371,\n",
       " 'big': 372,\n",
       " 'giants': 373,\n",
       " 'rasmus': 374,\n",
       " 'giant': 375,\n",
       " 'dragon': 376,\n",
       " 'shone': 377,\n",
       " 'wish': 378,\n",
       " 'summer': 379,\n",
       " 'real': 380,\n",
       " 'wanted': 381,\n",
       " 'dark': 382,\n",
       " 'call': 383,\n",
       " 'ran': 384,\n",
       " 'master': 385,\n",
       " 'close': 386,\n",
       " 'coming': 387,\n",
       " 'ugly': 388,\n",
       " 'given': 389,\n",
       " 'fast': 390,\n",
       " 'leaf': 391,\n",
       " 'mouse': 392,\n",
       " 'often': 393,\n",
       " 'strong': 394,\n",
       " 'true': 395,\n",
       " 'length': 396,\n",
       " 'entered': 397,\n",
       " 'tall': 398,\n",
       " 'remained': 399,\n",
       " 'become': 400,\n",
       " 'leave': 401,\n",
       " 'laid': 402,\n",
       " 'standing': 403,\n",
       " 'middle': 404,\n",
       " 'whether': 405,\n",
       " 'perhaps': 406,\n",
       " 'want': 407,\n",
       " 'red': 408,\n",
       " 'married': 409,\n",
       " 'hands': 410,\n",
       " 'against': 411,\n",
       " 'wall': 412,\n",
       " 'light': 413,\n",
       " 'alone': 414,\n",
       " 'voice': 415,\n",
       " 'hill': 416,\n",
       " 'pleased': 417,\n",
       " 'black': 418,\n",
       " 'opened': 419,\n",
       " 'olelukoie': 420,\n",
       " 'ask': 421,\n",
       " 'wait': 422,\n",
       " 'daughter': 423,\n",
       " 'wildrose': 424,\n",
       " 'jimmu': 425,\n",
       " 'year': 426,\n",
       " 'pleasure': 427,\n",
       " 'spring': 428,\n",
       " 'those': 429,\n",
       " 'almost': 430,\n",
       " 'being': 431,\n",
       " 'blue': 432,\n",
       " 'use': 433,\n",
       " 'followed': 434,\n",
       " 'beside': 435,\n",
       " 'able': 436,\n",
       " 'kind': 437,\n",
       " 'longer': 438,\n",
       " 'live': 439,\n",
       " 'matter': 440,\n",
       " 'stories': 441,\n",
       " 'husband': 442,\n",
       " 'led': 443,\n",
       " 'feet': 444,\n",
       " 'rather': 445,\n",
       " 'turn': 446,\n",
       " 'blooming': 447,\n",
       " 'half': 448,\n",
       " 'fields': 449,\n",
       " 'lovely': 450,\n",
       " 'dinner': 451,\n",
       " 'face': 452,\n",
       " 'held': 453,\n",
       " 'baby': 454,\n",
       " 'shepherd': 455,\n",
       " 'air': 456,\n",
       " 'crown': 457,\n",
       " 'along': 458,\n",
       " 'grown': 459,\n",
       " 'run': 460,\n",
       " 'horses': 461,\n",
       " 'beauty': 462,\n",
       " 'why': 463,\n",
       " 'splendid': 464,\n",
       " 'dear': 465,\n",
       " 'quiet': 466,\n",
       " 'threw': 467,\n",
       " 'boxes': 468,\n",
       " 'used': 469,\n",
       " 'read': 470,\n",
       " 'court': 471,\n",
       " 'spot': 472,\n",
       " 'hair': 473,\n",
       " 'corn': 474,\n",
       " 'mole': 475,\n",
       " 'born': 476,\n",
       " 'stopped': 477,\n",
       " 'farewell': 478,\n",
       " 'wedding': 479,\n",
       " 'rosebush': 480,\n",
       " 'having': 481,\n",
       " 'gone': 482,\n",
       " 'neck': 483,\n",
       " 'best': 484,\n",
       " 'ilonka': 485,\n",
       " 'hut': 486,\n",
       " 'managed': 487,\n",
       " 'dwarf': 488,\n",
       " 'tailor': 489,\n",
       " 'pieces': 490,\n",
       " 'wished': 491,\n",
       " 'basket': 492,\n",
       " 'feel': 493,\n",
       " 'arrived': 494,\n",
       " 'youth': 495,\n",
       " 'manner': 496,\n",
       " 'star': 497,\n",
       " 'exclaimed': 498,\n",
       " 'seated': 499,\n",
       " 'because': 500,\n",
       " 'marry': 501,\n",
       " 'carry': 502,\n",
       " 'across': 503,\n",
       " 'creature': 504,\n",
       " 'remember': 505,\n",
       " 'mouth': 506,\n",
       " 'living': 507,\n",
       " 'grass': 508,\n",
       " 'changed': 509,\n",
       " 'asleep': 510,\n",
       " 'rich': 511,\n",
       " 'sprang': 512,\n",
       " 'yellow': 513,\n",
       " 'clothes': 514,\n",
       " 'small': 515,\n",
       " 'feathers': 516,\n",
       " 'swallow': 517,\n",
       " 'inside': 518,\n",
       " 'wonderful': 519,\n",
       " 'lives': 520,\n",
       " 'table': 521,\n",
       " 'magic': 522,\n",
       " 'picture': 523,\n",
       " 'beneath': 524,\n",
       " 'six': 525,\n",
       " 'part': 526,\n",
       " 'laughed': 527,\n",
       " 'strange': 528,\n",
       " 'outside': 529,\n",
       " 'forth': 530,\n",
       " 'since': 531,\n",
       " 'four': 532,\n",
       " 'tired': 533,\n",
       " 'second': 534,\n",
       " 'rage': 535,\n",
       " 'throne': 536,\n",
       " 'shot': 537,\n",
       " 'getting': 538,\n",
       " 'inn': 539,\n",
       " 'gatto': 540,\n",
       " 'peppina': 541,\n",
       " 'sometimes': 542,\n",
       " 'wind': 543,\n",
       " 'grow': 544,\n",
       " 'autumn': 545,\n",
       " 'swallows': 546,\n",
       " 'met': 547,\n",
       " 'does': 548,\n",
       " 'sunbeam': 549,\n",
       " 'tears': 550,\n",
       " 'kept': 551,\n",
       " 'apples': 552,\n",
       " 'walls': 553,\n",
       " 'during': 554,\n",
       " 'forgotten': 555,\n",
       " 'noise': 556,\n",
       " 'begin': 557,\n",
       " 'speak': 558,\n",
       " 'hard': 559,\n",
       " 'hardly': 560,\n",
       " 'thinking': 561,\n",
       " 'book': 562,\n",
       " 'suddenly': 563,\n",
       " 'nearly': 564,\n",
       " 'wet': 565,\n",
       " 'wings': 566,\n",
       " 'human': 567,\n",
       " 'piece': 568,\n",
       " 'mine': 569,\n",
       " 'lying': 570,\n",
       " 'none': 571,\n",
       " 'either': 572,\n",
       " 'bade': 573,\n",
       " 'immediately': 574,\n",
       " 'drink': 575,\n",
       " 'whose': 576,\n",
       " 'hold': 577,\n",
       " 'wild': 578,\n",
       " 'forehead': 579,\n",
       " 'making': 580,\n",
       " 'money': 581,\n",
       " 'moneypig': 582,\n",
       " 'certainly': 583,\n",
       " 'beginning': 584,\n",
       " 'end': 585,\n",
       " 'ride': 586,\n",
       " 'seized': 587,\n",
       " 'neighbors': 588,\n",
       " 'cottage': 589,\n",
       " 'tail': 590,\n",
       " 'show': 591,\n",
       " 'running': 592,\n",
       " 'saying': 593,\n",
       " 'difference': 594,\n",
       " 'stranger': 595,\n",
       " 'eaglets': 596,\n",
       " 'fight': 597,\n",
       " 'gun': 598,\n",
       " 'vow': 599,\n",
       " 'shrine': 600,\n",
       " 'tanuki': 601,\n",
       " 'died': 602,\n",
       " 'bring': 603,\n",
       " 'wide': 604,\n",
       " 'sunshine': 605,\n",
       " 'windows': 606,\n",
       " 'dressed': 607,\n",
       " 'fall': 608,\n",
       " 'happiness': 609,\n",
       " 'servants': 610,\n",
       " 'handsome': 611,\n",
       " 'helped': 612,\n",
       " 'filled': 613,\n",
       " 'fastened': 614,\n",
       " 'bad': 615,\n",
       " 'thrown': 616,\n",
       " 'danced': 617,\n",
       " 'received': 618,\n",
       " 'ought': 619,\n",
       " 'already': 620,\n",
       " 'ah': 621,\n",
       " 'floor': 622,\n",
       " 'arms': 623,\n",
       " 'window': 624,\n",
       " 'wooden': 625,\n",
       " 'jumped': 626,\n",
       " 'proud': 627,\n",
       " 'within': 628,\n",
       " 'grave': 629,\n",
       " 'woke': 630,\n",
       " 'pay': 631,\n",
       " 'delicate': 632,\n",
       " 'food': 633,\n",
       " 'kitchen': 634,\n",
       " 'rooms': 635,\n",
       " 'cry': 636,\n",
       " 'allowed': 637,\n",
       " 'greatest': 638,\n",
       " 'beings': 639,\n",
       " 'order': 640,\n",
       " 'fairy': 641,\n",
       " 'burst': 642,\n",
       " 'hjalmar': 643,\n",
       " 'seven': 644,\n",
       " 'dog': 645,\n",
       " 'silver': 646,\n",
       " 'talk': 647,\n",
       " 'showed': 648,\n",
       " 'play': 649,\n",
       " 'garden': 650,\n",
       " 'walked': 651,\n",
       " 'sound': 652,\n",
       " 'easily': 653,\n",
       " 'wondered': 654,\n",
       " 'word': 655,\n",
       " 'gathered': 656,\n",
       " 'sign': 657,\n",
       " 'friends': 658,\n",
       " 'whilst': 659,\n",
       " 'palace': 660,\n",
       " 'answer': 661,\n",
       " 'offered': 662,\n",
       " 'search': 663,\n",
       " 'eagle': 664,\n",
       " 'ordered': 665,\n",
       " 'bones': 666,\n",
       " 'finished': 667,\n",
       " 'comber': 668,\n",
       " 'alexandria': 669,\n",
       " 'needle': 670,\n",
       " 'robe': 671,\n",
       " 'elfi': 672,\n",
       " 'bey': 673,\n",
       " 'dagger': 674,\n",
       " 'cat': 675,\n",
       " 'sweet': 676,\n",
       " 'around': 677,\n",
       " 'number': 678,\n",
       " 'rosy': 679,\n",
       " 'else': 680,\n",
       " 'scarcely': 681,\n",
       " 'sea': 682,\n",
       " 'deal': 683,\n",
       " 'flying': 684,\n",
       " 'younger': 685,\n",
       " 'neither': 686,\n",
       " 'longing': 687,\n",
       " 'sang': 688,\n",
       " 'pleasant': 689,\n",
       " 'prettiest': 690,\n",
       " 'women': 691,\n",
       " 'trying': 692,\n",
       " 'burned': 693,\n",
       " 'frightened': 694,\n",
       " 'rushed': 695,\n",
       " 'except': 696,\n",
       " 'humpty': 697,\n",
       " 'knows': 698,\n",
       " 'tomorrow': 699,\n",
       " 'boys': 700,\n",
       " 'sky': 701,\n",
       " 'royal': 702,\n",
       " 'sailor': 703,\n",
       " 'broad': 704,\n",
       " 'clear': 705,\n",
       " 'lie': 706,\n",
       " 'yours': 707,\n",
       " 'understand': 708,\n",
       " 'quietly': 709,\n",
       " 'sleeping': 710,\n",
       " 'lady': 711,\n",
       " 'legs': 712,\n",
       " 'death': 713,\n",
       " 'learned': 714,\n",
       " 'larger': 715,\n",
       " 'pulled': 716,\n",
       " 'sides': 717,\n",
       " 'die': 718,\n",
       " 'clever': 719,\n",
       " 'sick': 720,\n",
       " 'sit': 721,\n",
       " 'thick': 722,\n",
       " 'early': 723,\n",
       " 'yourselves': 724,\n",
       " 'dust': 725,\n",
       " 'touched': 726,\n",
       " 'although': 727,\n",
       " 'therefore': 728,\n",
       " 'mind': 729,\n",
       " 'suppose': 730,\n",
       " 'handkerchief': 731,\n",
       " 'youngest': 732,\n",
       " 'stick': 733,\n",
       " 'tied': 734,\n",
       " 'noblemans': 735,\n",
       " 'crying': 736,\n",
       " 'pointed': 737,\n",
       " 'ruins': 738,\n",
       " 'colors': 739,\n",
       " 'minutes': 740,\n",
       " 'start': 741,\n",
       " 'loudly': 742,\n",
       " 'passing': 743,\n",
       " 'plucked': 744,\n",
       " 'instead': 745,\n",
       " 'pigeons': 746,\n",
       " 'continued': 747,\n",
       " 'remain': 748,\n",
       " 'ready': 749,\n",
       " 'sank': 750,\n",
       " 'angry': 751,\n",
       " 'brother': 752,\n",
       " 'trouble': 753,\n",
       " 'front': 754,\n",
       " 'draw': 755,\n",
       " 'seeing': 756,\n",
       " 'pure': 757,\n",
       " 'plants': 758,\n",
       " 'certain': 759,\n",
       " 'bough': 760,\n",
       " 'course': 761,\n",
       " 'carefully': 762,\n",
       " 'wandered': 763,\n",
       " 'try': 764,\n",
       " 'bulrushes': 765,\n",
       " 'further': 766,\n",
       " 'started': 767,\n",
       " 'longed': 768,\n",
       " 'supper': 769,\n",
       " 'hungry': 770,\n",
       " 'hermit': 771,\n",
       " 'surprise': 772,\n",
       " 'news': 773,\n",
       " 'despair': 774,\n",
       " 'lindworm': 775,\n",
       " 'horn': 776,\n",
       " 'emperor': 777,\n",
       " 'fourth': 778,\n",
       " 'send': 779,\n",
       " 'rome': 780,\n",
       " 'shut': 781,\n",
       " 'gate': 782,\n",
       " 'hall': 783,\n",
       " 'wine': 784,\n",
       " 'return': 785,\n",
       " 'steward': 786,\n",
       " 'fulfilled': 787,\n",
       " 'flung': 788,\n",
       " 'rid': 789,\n",
       " 'servant': 790,\n",
       " 'bit': 791,\n",
       " 'hundred': 792,\n",
       " 'price': 793,\n",
       " 'snow': 794,\n",
       " 'obliged': 795,\n",
       " 'worth': 796,\n",
       " 'several': 797,\n",
       " 'drawn': 798,\n",
       " 'chosen': 799,\n",
       " 'cakes': 800,\n",
       " 'playthings': 801,\n",
       " 'hundreds': 802,\n",
       " 'happen': 803,\n",
       " 'courtyard': 804,\n",
       " 'least': 805,\n",
       " 'ladies': 806,\n",
       " 'exactly': 807,\n",
       " 'doors': 808,\n",
       " 'present': 809,\n",
       " 'doing': 810,\n",
       " 'ceiling': 811,\n",
       " 'noticed': 812,\n",
       " 'maid': 813,\n",
       " 'dumpty': 814,\n",
       " 'raised': 815,\n",
       " 'tales': 816,\n",
       " 'nights': 817,\n",
       " 'lesson': 818,\n",
       " 'heavy': 819,\n",
       " 'pail': 820,\n",
       " 'lord': 821,\n",
       " 'kings': 822,\n",
       " 'dance': 823,\n",
       " 'birth': 824,\n",
       " 'fragrant': 825,\n",
       " 'smell': 826,\n",
       " 'vanished': 827,\n",
       " 'deep': 828,\n",
       " 'buried': 829,\n",
       " 'turning': 830,\n",
       " 'dress': 831,\n",
       " 'weather': 832,\n",
       " 'sounded': 833,\n",
       " 'need': 834,\n",
       " 'cockchafers': 835,\n",
       " 'visit': 836,\n",
       " 'looks': 837,\n",
       " 'waist': 838,\n",
       " 'wept': 839,\n",
       " 'honey': 840,\n",
       " 'dwelt': 841,\n",
       " 'doubt': 842,\n",
       " 'passage': 843,\n",
       " 'sad': 844,\n",
       " 'comes': 845,\n",
       " 'closed': 846,\n",
       " 'delightful': 847,\n",
       " 'countries': 848,\n",
       " 'holding': 849,\n",
       " 'formed': 850,\n",
       " 'listen': 851,\n",
       " 'creatures': 852,\n",
       " 'wrong': 853,\n",
       " 'seem': 854,\n",
       " 'soap': 855,\n",
       " 'tale': 856,\n",
       " 'until': 857,\n",
       " 'stuff': 858,\n",
       " 'arm': 859,\n",
       " 'copy': 860,\n",
       " 'letters': 861,\n",
       " 'stream': 862,\n",
       " 'boat': 863,\n",
       " 'bought': 864,\n",
       " 'moon': 865,\n",
       " 'afraid': 866,\n",
       " 'beyond': 867,\n",
       " 'thoughts': 868,\n",
       " 'supposed': 869,\n",
       " 'fellow': 870,\n",
       " 'easy': 871,\n",
       " 'blossoms': 872,\n",
       " 'midst': 873,\n",
       " 'remembered': 874,\n",
       " 'hot': 875,\n",
       " 'bush': 876,\n",
       " 'worn': 877,\n",
       " 'iron': 878,\n",
       " 'case': 879,\n",
       " 'behold': 880,\n",
       " 'carrying': 881,\n",
       " 'below': 882,\n",
       " 'parents': 883,\n",
       " 'questions': 884,\n",
       " 'nice': 885,\n",
       " 'glass': 886,\n",
       " 'eldest': 887,\n",
       " 'agreed': 888,\n",
       " 'steps': 889,\n",
       " 'gray': 890,\n",
       " 'venture': 891,\n",
       " 'built': 892,\n",
       " 'pink': 893,\n",
       " 'surprised': 894,\n",
       " 'carriage': 895,\n",
       " 'road': 896,\n",
       " 'position': 897,\n",
       " 'despised': 898,\n",
       " 'waited': 899,\n",
       " 'begged': 900,\n",
       " 'consented': 901,\n",
       " 'waiting': 902,\n",
       " 'vowed': 903,\n",
       " 'stepped': 904,\n",
       " 'straight': 905,\n",
       " 'holy': 906,\n",
       " 'nearer': 907,\n",
       " 'caught': 908,\n",
       " 'killed': 909,\n",
       " 'happily': 910,\n",
       " 'watching': 911,\n",
       " 'reach': 912,\n",
       " 'person': 913,\n",
       " 'girls': 914,\n",
       " 'distance': 915,\n",
       " 'lost': 916,\n",
       " 'move': 917,\n",
       " 'pot': 918,\n",
       " 'motikatika': 919,\n",
       " 'huge': 920,\n",
       " 'yourself': 921,\n",
       " 'travel': 922,\n",
       " 'badly': 923,\n",
       " 'wrestle': 924,\n",
       " 'shirt': 925,\n",
       " 'glad': 926,\n",
       " 'birthday': 927,\n",
       " 'pillar': 928,\n",
       " 'ivory': 929,\n",
       " 'st': 930,\n",
       " 'james': 931,\n",
       " 'eighteenth': 932,\n",
       " 'luck': 933,\n",
       " 'unhappy': 934,\n",
       " 'taller': 935,\n",
       " 'discover': 936,\n",
       " 'age': 937,\n",
       " 'blew': 938,\n",
       " 'noble': 939,\n",
       " 'nodded': 940,\n",
       " 'sailed': 941,\n",
       " 'rejoice': 942,\n",
       " 'christmas': 943,\n",
       " 'peace': 944,\n",
       " 'sorts': 945,\n",
       " 'tapers': 946,\n",
       " 'brilliant': 947,\n",
       " 'pain': 948,\n",
       " 'wagon': 949,\n",
       " 'splendor': 950,\n",
       " 'decked': 951,\n",
       " 'enjoy': 952,\n",
       " 'short': 953,\n",
       " 'dreams': 954,\n",
       " 'bushes': 955,\n",
       " 'grand': 956,\n",
       " 'pictures': 957,\n",
       " 'trembled': 958,\n",
       " 'lighted': 959,\n",
       " 'dazzled': 960,\n",
       " 'silent': 961,\n",
       " 'ivedeavede': 962,\n",
       " 'downstairs': 963,\n",
       " 'attention': 964,\n",
       " 'push': 965,\n",
       " 'hidden': 966,\n",
       " 'bear': 967,\n",
       " 'slept': 968,\n",
       " 'shame': 969,\n",
       " 'inhabitants': 970,\n",
       " 'shooting': 971,\n",
       " 'præstö': 972,\n",
       " 'body': 973,\n",
       " 'knight': 974,\n",
       " 'city': 975,\n",
       " 'busy': 976,\n",
       " 'greeting': 977,\n",
       " 'korsör': 978,\n",
       " 'bloom': 979,\n",
       " 'perfect': 980,\n",
       " 'grove': 981,\n",
       " 'waters': 982,\n",
       " 'church': 983,\n",
       " 'bottom': 984,\n",
       " 'dream': 985,\n",
       " 'woods': 986,\n",
       " 'cock': 987,\n",
       " 'hunger': 988,\n",
       " 'thank': 989,\n",
       " 'rain': 990,\n",
       " 'drank': 991,\n",
       " 'sweetly': 992,\n",
       " 'rolled': 993,\n",
       " 'frozen': 994,\n",
       " 'sing': 995,\n",
       " 'whenever': 996,\n",
       " 'warned': 997,\n",
       " 'alarmed': 998,\n",
       " 'glittered': 999,\n",
       " 'pushed': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "#tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "#sequences = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "print(sequences[:1]) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3975"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have divided the cleaned text into training sequences and assigned each to work to an integer, so that all the sequences are represented by vectors of intergers. \n",
    "\n",
    "Next step is to divide it into input and output sequences. The outputs are the +1, which we have added to each sequences in lesson 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  93    7    1 ... 1130    1 3973]\n",
      " [   7    1  179 ...    1 3973    2]\n",
      " [   1  179   67 ... 3973    2 2094]\n",
      " ...\n",
      " [  52  290    4 ...   11   30   49]\n",
      " [ 290    4  792 ...   30   49   40]\n",
      " [   4  792  269 ...   49   40 3974]]\n",
      "[   2 2094   38 ...   40 3974   46]\n",
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences) #create an array with the sequences\n",
    "#separate into input and output sequences\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #X is the whole sequences without the last integer, y is the last integer in the sequence\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding, represent by 1 and 0\n",
    "print(y)\n",
    "seq_length = X.shape[1] #set sequence length to the number of columns in X, so to 50 in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.a - Fit and use the generative model\n",
    "\n",
    "\n",
    "Step by step:\n",
    " \n",
    "- Fit and save the model\n",
    "- Load the data and the model\n",
    "- Generate text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create the model, by adding layers to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            198750    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3975)              401475    \n",
      "=================================================================\n",
      "Total params: 751,125\n",
      "Trainable params: 751,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() #define model, use sequential model function\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #add embedding layer, 50 is the dimensions which will be used to represent each word vector\n",
    "model.add(LSTM(100, return_sequences=True)) #add hidden layer, long short term memory layer\n",
    "model.add(LSTM(100)) #100 is the number of units in our hidden layer\n",
    "model.add(Dense(100, activation='relu')) #dense specifies the structure of the neural network, 100 refer to that there are 100 neurons in the first hidden layer, as defined above. Relu is an argument of rectified linear unit.  \n",
    "model.add(Dense(vocab_size, activation='softmax')) #softmax transform to probabilities \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it's a multilevel predicter, because it's categorical, we want a measure which is accuracy, and a cost function which is optimized by adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of showing how the model works I have just putted epochs=1. In the real model we ran it one 100 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "39028/39028 [==============================] - 203s 5ms/step - loss: 6.4798 - acc: 0.0712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a217c2a58>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model took a long time, between 20 and 40 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb')) #w stands for write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.b - Fit and use the generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready for use. First we will try just to generate one word at a time. After this we will create a function which generates a specified number of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need the text, so we can use a sequence as a source input for generating the text\n",
    "doc = load_text('clean_text.txt') #load cleaned sequences \n",
    "\n",
    "lines = doc.split('\\n') #split by new line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify the sequence length\n",
    "seq_length = len(lines[0].split()) - 1 #look at the first line, split sequences by words, count and minus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r stands for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they had thought he would never come back to reproach them for their wickedness know what to paul said to them quietly shall never see me again off with he next took the three apples out of his pocket and placed them all in the prettiest places he could find after\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #randint returns a random integer from 0 to the length of lines, index it\n",
    "print(seed_text + '\\n') #print the selected seed text and make a line shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use tokenizer to assign an integer to every word in the seed text\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "#exclude the output word, so take the last 50 words\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the model.preict_classes will return the index of the word with the highest probability \n",
    "\n",
    "yhat = model.predict_classes(encoded, verbose=0) #verbose specifies how  you want to 'see' the training progress for each epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look up in the tokenizer what word the indexed word corresponds to\n",
    "out_word = '' #empty variable to store the word output word in \n",
    "for word, index in tokenizer.word_index.items(): #loop through the tokenizers\n",
    "\tif index == yhat: #if the index is equally to yhat, assign the word to the variable out_word\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the input sequences will get too long, when adding the output word, need to make sure that it's always 50 words long\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "#the function takes 5 inputs\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate new text, use the function on the model\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n",
    "\n",
    "\n",
    "#save the generated text\n",
    "out_filename = 'first_output1.txt'\n",
    "\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in appendix D, \"Output from the simple model\", the first output was better than the one above. This is due to the low amount of data in this example and because we only ran one epoch when training the model on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5.a - Improve Language Model\n",
    "\n",
    "\n",
    "I have decided to create sequences which correspond to single sentences, instead of having sequences extending over different sentences. But for the model to work, all the sequences need to be the same length (e.g. some sentences have a length of 3 and others a length of 130). This means that we need to create padded sequences, where we add zeros in the beginning of each sequence, so all of them ends up being the same length.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/signeklovekjaer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "80842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Originally they\\ngave to their gods of their best.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load nltk library\n",
    "nltk.download('punkt') #the library can use punctuation to divide the text into sentences\n",
    "\n",
    "#define tokenizer, use pretrained from the corpus\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#assign doc to data\n",
    "data = doc\n",
    "\n",
    "#use function sent_tokenize from the nltk package to divide the text into sentences\n",
    "sentence_list = sent_tokenize(data)\n",
    "\n",
    "#print the number of sentences\n",
    "print(len(sentence_list))\n",
    "\n",
    "#check the type, it's a list\n",
    "type(sentence_list)\n",
    "\n",
    "#index to see an example\n",
    "sentence_list[5] #need to remove the punctuation and have in lower case\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to clean the text and prepare the sequences as we did in lesson 2 and 3. We cannot use the exact same code, as the lines are now in unequal lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'portion', 'of', 'the', 'ancient', 'aryan', 'race', 'which', 'poured', 'from', 'the', 'central', 'plain', 'of', 'asia', 'through', 'the', 'rocky', 'defiles', 'of', 'what', 'we', 'now', 'call', 'the', 'frontier', 'to', 'populate', 'the', 'fertile', 'lowlands', 'of', 'india', 'had', 'gods', 'who', 'must', 'once', 'have', 'been', 'wholly', 'heroic', 'but', 'who', 'came', 'in', 'time', 'to', 'be', 'more', 'degraded', 'than', 'the', 'most', 'vicious', 'of', 'lustful', 'criminals']\n",
      "['originally', 'they', 'gave', 'to', 'their', 'gods', 'of', 'their', 'best']\n"
     ]
    }
   ],
   "source": [
    "#check if cleaning function works on one sentence in the sentence list\n",
    "print(clean_doc(sentence_list[3])) #it does\n",
    "\n",
    "#make an empty list\n",
    "sentence_clean_list = []\n",
    "\n",
    "#make a loop which runs through the list of sentences and uses the clean_doc() function\n",
    "for sentence in sentence_list:\n",
    "    y = clean_doc(sentence) #save in y and append to empty list\n",
    "    sentence_clean_list.append(y)\n",
    "    \n",
    "\n",
    "#print(sentence_clean_list[:10])\n",
    "\n",
    "#check to see if the punctuation is removed and if it's in lower case\n",
    "print(sentence_clean_list[5]) #it is\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check the length of the sentences, as we need to use this information then padding the sentences into sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "348\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check the length of a random sentence\n",
    "print(len(sentence_clean_list[3]))\n",
    "\n",
    "length = []\n",
    "for sentence in sentence_clean_list:\n",
    "    l = len(sentence)\n",
    "    length.append(l)\n",
    "\n",
    "    \n",
    "print(max(length))#print the maximum length of a sentence\n",
    "\n",
    "print(min(length))#print the minimum length of a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reasons, some of the sentences are on a length of zero. I will delete these. In relation to this, some of the sentences are really long. Because we will use padded sequences, all sequences will end up being the same length as the longest sentence. I will delete the few sentences which are longer than 130 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "1\n",
      "Sentences removed after deleting sentences shorter than 1: 393\n",
      "Sentences removed after deleting sentences longer than : 97\n"
     ]
    }
   ],
   "source": [
    "#remove sentences less than 1 word long and longer than 130\n",
    "sentence_new_list1 = [s for s in sentence_clean_list if len(s) >= 1] \n",
    "sentence_new_list = [s for s in sentence_new_list1 if len(s) <= 130]\n",
    "\n",
    "\n",
    "length_new = []\n",
    "for sentence in sentence_new_list:\n",
    "    l = len(sentence)\n",
    "    length_new.append(l)\n",
    "\n",
    "    \n",
    "print(max(length_new))\n",
    "\n",
    "print(min(length_new))\n",
    "\n",
    "\n",
    "#check how many sentences we have removed\n",
    "print('Sentences removed after deleting sentences shorter than 1:', len(sentence_clean_list) - len(sentence_new_list1) )\n",
    "print('Sentences removed after deleting sentences longer than :', len(sentence_new_list1) - len(sentence_new_list)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally they gave to their gods of their best\n",
      "as a little child holds out its hands to catch the sunbeams to feel and to grasp what so its eyes tell it is actually there so down through the ages men have stretched out their hands in eager endeavour to know their god\n"
     ]
    }
   ],
   "source": [
    "# turn into (not separated) sentence, check method on number 5 in the list\n",
    "line = ' '.join(sentence_new_list[5]) \n",
    "\n",
    "print(line) #it works\n",
    "\n",
    "#create empty list\n",
    "sequences_sent = []\n",
    "\n",
    "#create a loop to do it at all the sentences\n",
    "for sentence in sentence_new_list:\n",
    "    line = ' '.join(sentence)\n",
    "    sequences_sent.append(line)\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the sentences are ready and we can continue as we did in lesson 2 and 3 with saving the sentences and assigning an integer to each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the sentences to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save sentences to file\n",
    "out_filename = 'clean_text_sentences.txt'\n",
    "save_doc(sequences_sent, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as a little child holds out its hands to catch the sunbeams to feel and to grasp what so its eyes tell it is actually there so down through the ages men have stretched out their hands in eager endeavour to know their god\n"
     ]
    }
   ],
   "source": [
    "#load the cleaned data with sentences\n",
    "doc = load_text('clean_text_sentences.txt')\n",
    "\n",
    "#split the text according to line shift\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list\n",
    "\n",
    "\n",
    "print(lines[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 5, 49, 265, 4280, 36, 135, 262, 3, 860, 1, 4529, 3, 616, 2, 3, 4401, 51, 27, 135, 168, 156, 11, 31, 2426, 45, 27, 63, 145, 1, 4530, 210, 34, 832, 36, 50, 262, 8, 2387, 12099, 3, 125, 50, 639]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "sequences_sent = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "#print(sequences_sent[:100]) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5.b - Improve Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model all the input sequences need to be the same length. We therefore need to pad or truncate the different sentences. When you pad the sequences, you add zeros, so that they all become the same length (lenght of the longest sentence). In that case we will have to create an masking input layer which will ignore padded values. This means that padded inputs have no impact on learning. But we will start by padding all the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   125    50   639]\n",
      " [    0     0     0 ...     4    50 14117]\n",
      " [    0     0     0 ...     1   214   347]\n",
      " ...\n",
      " [    0     0     0 ...     2  1308    82]\n",
      " [    0     0     0 ...     1   633   863]\n",
      " [    0     0     0 ...     4    13   242]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will padd zeros in front, pre is the default option\n",
    "padded = pad_sequences(sequences_sent)\n",
    "print(padded)\n",
    "\n",
    "\n",
    "len(padded[3]) #all sequences are now 130 long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28730"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    3  125   50]\n",
      " [   0    0    0 ... 4402    4   50]\n",
      " [   0    0    0 ...  188    1  214]\n",
      " ...\n",
      " [   0    0    0 ... 1260    2 1308]\n",
      " [   0    0    0 ...   19    1  633]\n",
      " [   0    0    0 ...  255    4   13]]\n",
      "[  639 14117   347 ...    82   863   242]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "1.0\n",
      "0.0\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences_sent = array(padded) #create an array with the sequences\n",
    "\n",
    "X, y = sequences_sent[:,:-1], sequences_sent[:,-1] #separate into input and output sequences\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding\n",
    "print(y)\n",
    "seq_length_sent = X.shape[1] #set sequence length to the number of columns in X\n",
    "\n",
    "len(X[1])\n",
    "len(y)\n",
    "\n",
    "print(y[[3]])\n",
    "\n",
    "print(max(y[3])) #check if there is one-hot encoding for the output word\n",
    "\n",
    "print(min(y[3]))\n",
    "\n",
    "print(len(X[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create the model. One difference to the first model, is that we need to add a masking layer, which specify that the model should not take zeros into account (what we have padded the sequences with). Another difference is that the input sequences now is 130 words long instead of 50 as in the first model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 129)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 129, 50)           1436500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 129, 100)          60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28730)             2901730   \n",
      "=================================================================\n",
      "Total params: 4,489,130\n",
      "Trainable params: 4,489,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential() #define model, use sequential model function\n",
    "model.add(Masking(mask_value=0, input_shape=(seq_length_sent,))) #the masking layer, which is needed because of the padded sequences\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length_sent)) #add embedding layer, 50 is the dimensions which will be used to represent each word vector\n",
    "model.add(LSTM(100, return_sequences=True)) #add hidden layer, long short term memory layer\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu')) #dense specifies the structure of the neural network, 100 refer to that there are 100 neurons in the first hidden layer, as defined above. Relu is an argument of rectified linear unit.  \n",
    "model.add(Dense(vocab_size, activation='softmax')) #softmax transform to probabilities \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1664/80352 [..............................] - ETA: 24:05 - loss: 10.2595 - acc: 0.0186"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #it's a multilevel predicter, because it's categorical, we want a measure which is accuracy, and a cost function which is optimized by adam\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to processing power on my computer, the outputs for the next steps are not visible in this Jupyter notebook. We ran the model on another computer. But the steps are exactly the same as for the simple model under Lesson 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5_sentence')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl_sentence', 'wb')) #w stands for write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need the text, so we can use a sequence as a source input for generating the text\n",
    "doc = load_text('Data/clean_text_sentences.txt') #load cleaned sequences \n",
    "\n",
    "lines = doc.split('\\n') #split by new line\n",
    "\n",
    "\n",
    "lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify the sequence length\n",
    "seq_length_sent = X.shape[1]\n",
    "\n",
    "print(seq_length_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = load_model('model.h5_sentence')\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl_sentence', 'rb')) #r stands for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select a seed text\n",
    "seed_text_sentence = lines[randint(0,len(lines))] #randint returns a random integer from 0 to the length of lines, index it\n",
    "print(seed_text_sentence + '\\n') #print the selected seed text and make a line shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the input sequences will get too long, when adding the output word, need to make sure that it's always 129 words long\n",
    "#encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "def generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text_sentence #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length_sent, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, 20)\n",
    "print(generated)\n",
    "\n",
    "\n",
    "#save the generated text\n",
    "\n",
    "out_filename = 'Data/first_output_sentence.txt'\n",
    "\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text generated from this model can be seen in appendix D \"Output from model using natural sentences as training sequences\". "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp_project_keras]",
   "language": "python",
   "name": "conda-env-nlp_project_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

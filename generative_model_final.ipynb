{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A generative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Load data and get acquainted with the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Step by step:\n",
    "\n",
    "- Choose and download considerably amount of data from Gutenberg.org\n",
    "- Look into the data, review some of the text\n",
    "- Observe things (e.g. strange names), which must be considered when preparing the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we did was to download Hans Christian Andersen fairytales from Gutenberg and look into some of the texts.\n",
    "\n",
    "What do we need to look out for when cleaning the data:\n",
    "\n",
    "- Punctuation \n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "- Capital letters as headers, might want to delete these\n",
    "- Change everything to lower caser \n",
    "- Illustrations, delete them, written as ([Illustration: _His limbs were numbed, his beautiful eyes were closing.-])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import re\n",
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "from random import randint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Masking\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on the following coding block:\n",
    "https://www.pythonforbeginners.com/files/reading-and-writing-files-in-python\n",
    "- How to open text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define a function to load the text into memory\n",
    "def load_text(filename): #we decide that the function is called load_text\n",
    "    file = open(filename, 'r') #open() is used to open/loading a filename \n",
    "    #the mode indicates how it should be opened, r = read, w = writing, a = appending \n",
    "    text = file.read() # read the file and assign it to the variable text\n",
    "    file.close() #close the file again\n",
    "    return text #ends function and specify what output the want, we want text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved all the text files in a local folder on the computer. We need to first load these and then combine them into a single file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "22693-0.txt\n",
      "2435-0.txt\n",
      "2591-0.txt\n",
      "2892-0.txt\n",
      "32571-0.txt\n",
      "3454-0.txt\n",
      "43212-0.txt\n",
      "500-0.txt\n",
      "503-0.txt\n",
      "52521.txt\n",
      "5615-0.txt\n",
      "56914-0.txt\n",
      "640-0.txt\n",
      "641-0.txt\n",
      "677-0.txt\n",
      "7277-0.txt\n",
      "7439-0.txt\n",
      "902-0.txt\n",
      "fairytales.txt\n",
      "pg11027.txt\n",
      "pg128.txt\n",
      "pg14241.txt\n",
      "pg17034.txt\n",
      "pg17860.txt\n",
      "pg19068.txt\n",
      "pg19207.txt\n",
      "pg19860.txt\n",
      "pg28314.txt\n",
      "pg29021.txt\n",
      "pg3027.txt\n",
      "pg3152.txt\n",
      "pg31763.txt\n",
      "pg33511.txt\n",
      "pg33571.txt\n",
      "pg34705.txt\n",
      "pg37193.txt\n",
      "pg4018.txt\n",
      "pg4357.txt\n",
      "pg51762.txt\n",
      "pg708.txt\n",
      "pg7871.txt\n",
      "pg7885.txt\n"
     ]
    }
   ],
   "source": [
    "#define a path, the folder where the text files are placed\n",
    "path = \"/Users/signeklovekjaer/Documents/CognitiveScience/3.semester/Learning_with_digital_media/Exam_project/Jupiter_notebook/Data/\"\n",
    "#make a list that contains all the names of the files in the directory and assign it to a variable\n",
    "filenames = os.listdir( path )\n",
    "\n",
    "# print all the files in the directory \n",
    "for file in filenames:\n",
    "   print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22693-0.txt',\n",
       " '2435-0.txt',\n",
       " '2591-0.txt',\n",
       " '2892-0.txt',\n",
       " '32571-0.txt',\n",
       " '3454-0.txt',\n",
       " '43212-0.txt',\n",
       " '500-0.txt',\n",
       " '503-0.txt',\n",
       " '52521.txt',\n",
       " '5615-0.txt',\n",
       " '56914-0.txt',\n",
       " '640-0.txt',\n",
       " '641-0.txt',\n",
       " '677-0.txt',\n",
       " '7277-0.txt',\n",
       " '7439-0.txt',\n",
       " '902-0.txt',\n",
       " 'pg11027.txt',\n",
       " 'pg128.txt',\n",
       " 'pg14241.txt',\n",
       " 'pg17034.txt',\n",
       " 'pg17860.txt',\n",
       " 'pg19068.txt',\n",
       " 'pg19207.txt',\n",
       " 'pg19860.txt',\n",
       " 'pg28314.txt',\n",
       " 'pg29021.txt',\n",
       " 'pg3027.txt',\n",
       " 'pg3152.txt',\n",
       " 'pg31763.txt',\n",
       " 'pg33511.txt',\n",
       " 'pg33571.txt',\n",
       " 'pg34705.txt',\n",
       " 'pg37193.txt',\n",
       " 'pg4018.txt',\n",
       " 'pg4357.txt',\n",
       " 'pg51762.txt',\n",
       " 'pg708.txt',\n",
       " 'pg7871.txt',\n",
       " 'pg7885.txt']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use .remove() to remove filenames \n",
    "filenames.remove('.DS_Store');\n",
    "\n",
    "filenames.remove('fairytales.txt');\n",
    "\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine all the text files into a single file called \"fairytales.text\"\n",
    "with open('fairytales.txt', 'w') as outfile: #specify where the file should be saved and what it should be called (if a file with that name already exists, it will be overwritten, 'w' is used to edit and write new information to a file \n",
    "    for text in filenames: #for all the text in the filenames, do the following...\n",
    "        with open(text) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Just as a little child holds out its hands to catch the sunbeams, to\n",
      "feel and to grasp what, so its eyes tell it, is actually there, so,\n",
      "down through the ages, men have stretched out their hands in eager\n",
      "endeavour to know their God. And because only through the human was\n",
      "the divine knowable, the old peoples of the earth made gods of their\n",
      "heroes and not unfrequently endowed these gods with as many of the\n",
      "vices as of the virtues of their worshippers. As we read the myths of\n",
      "the East and the West we find ever the same story. That portion of the\n",
      "ancient Aryan race which poured from the central plain of Asia,\n",
      "through the rocky defiles of what we now call \"The Frontier,\" to\n",
      "populate the fertile lowlands of India, had gods who must once have\n",
      "been wholly heroic, but who came in time to be more degraded than the\n",
      "most vicious of lustful criminals. And the Greeks, Latins, Teutons,\n",
      "Celts, and Slavonians, who came of the same mighty Aryan stock, did\n",
      "even as those with whom they owned a common anc\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load document\n",
    "in_filename = 'fairytales.txt'\n",
    "doc = load_text(in_filename)\n",
    "print(doc[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - Prepare the training data\n",
    "\n",
    "Step by step:\n",
    "- Clean the text e.g. normalize words to lowercase and delete punctuation from words, to reduce vocabulary size\n",
    "- Organize the words into sequences and save these to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Just as a little child holds out its hands to catch the sunbeams, to\n",
      "feel and to grasp what, so its eyes tell it, is actually there, so,\n",
      "down through the ages, men have stretched out their hands in eager\n",
      "endeavour to know their God. And because only through the human was\n",
      "the divine knowable, the old peoples of the earth made gods of their\n",
      "heroes and not unfrequently endowed these gods with as many of the\n",
      "vices as of the virtues of their worshippers. As we read the myths of\n",
      "the East and the West we find ever the same story. That portion of the\n",
      "ancient Aryan race which poured from the central plain of Asia,\n",
      "through the rocky defiles of what we now call \"The Frontier,\" to\n",
      "populate the fertile lowlands of India, had gods who must once have\n",
      "been wholly heroic, but who came in time to be more degraded than the\n",
      "most vicious of lustful criminals. And the Greeks, Latins, Teutons,\n",
      "Celts, and Slavonians, who came of the same mighty Aryan stock, did\n",
      "even as those with whom they owned a common anc\n"
     ]
    }
   ],
   "source": [
    "#remove headers\n",
    "doc = re.sub(r'[A-Z]{2,}', '', doc) #remove capital letters everytime there is more than two \n",
    "\n",
    "print(doc[:1000]) #lets look at it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the following code: https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\n",
    "- How to use translate together with the helper function maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create function that can clean the text\n",
    "def clean_doc(doc): #make a function called clean_doc\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split() #get a list with all words (tokens)\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation) #third argument specifies the character we want to delete\n",
    "\ttokens = [w.translate(table) for w in tokens] #use translation mapping from table and loop through all words in token\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #isalpha() checks whether the string consists of alphabetic characters only. Replace word with word if it is alphabetical. \n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens] #replace word in lower case with word in token \n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the cleaning function on the whole document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens)) #print total number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #group similar tokens and print the number\n",
    "\n",
    "tokens[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the text is cleaned and we need to divide into sequences.\n",
    "\n",
    "We have to decide the length of the input sequences, which is the number of words that the model will train on. This is also the number of words which will be used as the seed text, when using the model to actually generate new sequences. \n",
    "\n",
    "In the tutorial 50 words are suggested. We will start with that and then in the fifth lesson we might try to only use sequences inside sentences, though this will minimize the amount of training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# organize the text into sequences of tokens\n",
    "length = 50 + 1 #define the length of the sequences, 1 is the output word\n",
    "sequences = list() #create an empty list, where we will store all the sequences\n",
    "\n",
    "for i in range(length, len(tokens)): #range([start], stop[, step])\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i] #make sequences with length of 50 + 1\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #join the tokens into a line in the sequence\n",
    "\t# store\n",
    "\tsequences.append(line) #append all the lines to the empty list created before the loop\n",
    "print('Total Sequences: %d' % len(sequences)) #print the total amount (length of sequence) of sequences \n",
    "\n",
    "\n",
    "print(sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename): #define function called save_doc() which takes two inputs\n",
    "\tdata = '\\n'.join(lines) #join lines, separate by line shift \n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'Data/clean_text.txt'\n",
    "save_doc(sequences, out_filename)#use the function on the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 3 - Train the generative model\n",
    "\n",
    "Step by step:\n",
    " \n",
    "- Load the cleaned data and divide it into training sequences\n",
    "- Map each word to a unique integer i.e. word to vector representation\n",
    "- Separate the input sequences into input and output elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the cleaned data\n",
    "doc = load_text('Data/clean_text.txt')\n",
    "\n",
    "#split the text according to line shift (every string is 51 words)\n",
    "lines = doc.split('\\n') #use split() function, \\n is a regular expression for line shift\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for getting keras and tensorflow packages to work:\n",
    "\n",
    "https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "https://www.tensorflow.org/install/source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "sequences = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "print(sequences) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have divided the cleaned text into training sequences and assigned each to work to an integer, so that all the sequences are represented by vectors of intergers. \n",
    "\n",
    "Next step is to divide it into input and output sequences. The outputs are the +1, which we have added to each sequences in lesson 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences) #create an array with the sequences\n",
    "#separate into input and output sequences\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #X is the whole sequences without the last integer, y is the last integer in the sequence\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding, represent by 1 and 0\n",
    "print(y)\n",
    "seq_length = X.shape[1] #set sequence length to the number of columns in X, so to 50 in this case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4.a - Fit and use the generative model\n",
    "\n",
    "\n",
    "Step by step:\n",
    " \n",
    "- Fit and save the model\n",
    "- Load the data and the model\n",
    "- Generate text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create the model, by adding layers to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential() #define model, use sequential model function\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #add embedding layer, 50 is the dimensions which will be used to represent each word vector\n",
    "model.add(LSTM(100, return_sequences=True)) #add hidden layer, long short term memory layer\n",
    "model.add(LSTM(100)) #100 is the number of units in our hidden layer\n",
    "model.add(Dense(100, activation='relu')) #dense specifies the structure of the neural network, 100 refer to that there are 100 neurons in the first hidden layer, as defined above. Relu is an argument of rectified linear unit.  \n",
    "model.add(Dense(vocab_size, activation='softmax')) #softmax transform to probabilities \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it's a multilevel predicter, because it's categorical, we want a measure which is accuracy, and a cost function which is optimized by adam\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model took a long time, between 20 and 40 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb')) #w stands for write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is ready for use. First we will try just to generate one word at a time. After this we will create a function which generates a specified number of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need the text, so we can use a sequence as a source input for generating the text\n",
    "doc = load_text('Data/clean_text.txt') #load cleaned sequences \n",
    "\n",
    "lines = doc.split('\\n') #split by new line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify the sequence length\n",
    "seq_length = len(lines[0].split()) - 1 #look at the first line, split sequences by words, count and minus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r stands for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #randint returns a random integer from 0 to the length of lines, index it\n",
    "print(seed_text + '\\n') #print the selected seed text and make a line shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use tokenizer to assign an integer to every word in the seed text\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "#exclude the output word, so take the last 50 words\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence\n",
    "\n",
    "len(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the model.preict_classes will return the index of the word with the highest probability \n",
    "yhat = model.predict_classes(encoded, verbose=0) #verbose specifies how  you want to 'see' the training progress for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look up in the tokenizer what word the indexed word corresponds to\n",
    "out_word = '' #empty variable to store the word output word in \n",
    "for word, index in tokenizer.word_index.items(): #loop through the tokenizers\n",
    "\tif index == yhat: #if the index is equally to yhat, assign the word to the variable out_word\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the input sequences will get too long, when adding the output word, need to make sure that it's always 50 words long\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "#the function takes 5 inputs\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate new text, use the function on the model\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n",
    "\n",
    "\n",
    "#save the generated text\n",
    "out_filename = 'Data/first_output1.txt'\n",
    "\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5 - Improve Language Model\n",
    "\n",
    "\n",
    "I have decided to create sequences which correspond to single sentences, instead of having sequences extending over different sentences. But for the model to work, all the sequences need to be the same length (e.g. some sentences have a length of 3 and others a length of 130). This means that we need to create padded sequences, where we add zeros in the beginning of each sequence, so all of them ends up being the same length.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/signeklovekjaer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "80842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Originally they\\ngave to their gods of their best.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load nltk library\n",
    "nltk.download('punkt') #the library can use punctuation to divide the text into sentences\n",
    "\n",
    "#define tokenizer, use pretrained from the corpus\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#assign doc to data\n",
    "data = doc\n",
    "\n",
    "#use function sent_tokenize from the nltk package to divide the text into sentences\n",
    "sentence_list = sent_tokenize(data)\n",
    "\n",
    "#print the number of sentences\n",
    "print(len(sentence_list))\n",
    "\n",
    "#check the type, it's a list\n",
    "type(sentence_list)\n",
    "\n",
    "#index to see an example\n",
    "sentence_list[5] #need to remove the punctuation and have in lower case\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to clean the text and prepare the sequences as we did in lesson 2 and 3. We cannot use the exact same code, as the lines are now in unequal lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'portion', 'of', 'the', 'ancient', 'aryan', 'race', 'which', 'poured', 'from', 'the', 'central', 'plain', 'of', 'asia', 'through', 'the', 'rocky', 'defiles', 'of', 'what', 'we', 'now', 'call', 'the', 'frontier', 'to', 'populate', 'the', 'fertile', 'lowlands', 'of', 'india', 'had', 'gods', 'who', 'must', 'once', 'have', 'been', 'wholly', 'heroic', 'but', 'who', 'came', 'in', 'time', 'to', 'be', 'more', 'degraded', 'than', 'the', 'most', 'vicious', 'of', 'lustful', 'criminals']\n",
      "['originally', 'they', 'gave', 'to', 'their', 'gods', 'of', 'their', 'best']\n"
     ]
    }
   ],
   "source": [
    "#check if cleaning function works on one sentence in the sentence list\n",
    "print(clean_doc(sentence_list[3])) #it does\n",
    "\n",
    "#make an empty list\n",
    "sentence_clean_list = []\n",
    "\n",
    "#make a loop which runs through the list of sentences and uses the clean_doc() function\n",
    "for sentence in sentence_list:\n",
    "    y = clean_doc(sentence) #save in y and append to empty list\n",
    "    sentence_clean_list.append(y)\n",
    "    \n",
    "\n",
    "#print(sentence_clean_list[:10])\n",
    "\n",
    "#check to see if the punctuation is removed and if it's in lower case\n",
    "print(sentence_clean_list[5]) #it is\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check the length of the sentences, as we need to use this then padding the sentences into sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "348\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check the length of a random sentence\n",
    "print(len(sentence_clean_list[3]))\n",
    "\n",
    "length = []\n",
    "for sentence in sentence_clean_list:\n",
    "    l = len(sentence)\n",
    "    length.append(l)\n",
    "\n",
    "    \n",
    "print(max(length))#print the maximum length of a sentence\n",
    "\n",
    "print(min(length))#print the minimum length of a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "1\n",
      "Sentences removed after deleting sentences shorter than 1: 393\n",
      "Sentences removed after deleting sentences longer than : 97\n"
     ]
    }
   ],
   "source": [
    "#remove sentences less than 1 word long and longer than 130\n",
    "sentence_new_list1 = [s for s in sentence_clean_list if len(s) >= 1] \n",
    "sentence_new_list = [s for s in sentence_new_list1 if len(s) <= 130]\n",
    "\n",
    "\n",
    "length_new = []\n",
    "for sentence in sentence_new_list:\n",
    "    l = len(sentence)\n",
    "    length_new.append(l)\n",
    "\n",
    "    \n",
    "print(max(length_new))\n",
    "\n",
    "print(min(length_new))\n",
    "\n",
    "\n",
    "#check how many sentences we have removed\n",
    "print('Sentences removed after deleting sentences shorter than 1:', len(sentence_clean_list) - len(sentence_new_list1) )\n",
    "print('Sentences removed after deleting sentences longer than :', len(sentence_new_list1) - len(sentence_new_list)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally they gave to their gods of their best\n",
      "as a little child holds out its hands to catch the sunbeams to feel and to grasp what so its eyes tell it is actually there so down through the ages men have stretched out their hands in eager endeavour to know their god\n"
     ]
    }
   ],
   "source": [
    "# turn into (not separated) sentence, check method on number 5 in the list\n",
    "line = ' '.join(sentence_new_list[5]) \n",
    "\n",
    "print(line) #it works\n",
    "\n",
    "#create empty list\n",
    "sequences_sent = []\n",
    "\n",
    "#create a loop to do it at all the sentences\n",
    "for sentence in sentence_new_list:\n",
    "    line = ' '.join(sentence)\n",
    "    sequences_sent.append(line)\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the sentences to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save sentences to file\n",
    "out_filename = 'clean_text_sentences.txt'\n",
    "save_doc(sequences_sent, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as a little child holds out its hands to catch the sunbeams to feel and to grasp what so its eyes tell it is actually there so down through the ages men have stretched out their hands in eager endeavour to know their god\n"
     ]
    }
   ],
   "source": [
    "#load the cleaned data with sentences\n",
    "doc = load_text('clean_text_sentences.txt')\n",
    "\n",
    "#split the text according to line shift\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list\n",
    "\n",
    "\n",
    "print(lines[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 5, 49, 265, 4280, 36, 135, 262, 3, 860, 1, 4529, 3, 616, 2, 3, 4401, 51, 27, 135, 168, 156, 11, 31, 2426, 45, 27, 63, 145, 1, 4530, 210, 34, 832, 36, 50, 262, 8, 2387, 12099, 3, 125, 50, 639]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "sequences_sent = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "#print(sequences_sent[:100]) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model all the input sequences need to be the same length. We therefore need to pad or truncate the different sentences. When you pad the sequences, you add zeros, so that they all become the same length (lenght of the longest sentence). In that case we will have to create an masking input layer which will ignore padded values. This means that padded inputs have no impact on learning. But we will start by padding all the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   125    50   639]\n",
      " [    0     0     0 ...     4    50 14117]\n",
      " [    0     0     0 ...     1   214   347]\n",
      " ...\n",
      " [    0     0     0 ...     2  1308    82]\n",
      " [    0     0     0 ...     1   633   863]\n",
      " [    0     0     0 ...     4    13   242]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will padd zeros in front, pre is the default option\n",
    "padded = pad_sequences(sequences_sent)\n",
    "print(padded)\n",
    "\n",
    "\n",
    "len(padded[3]) #all sequences are now 130 long\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28730"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    3  125   50]\n",
      " [   0    0    0 ... 4402    4   50]\n",
      " [   0    0    0 ...  188    1  214]\n",
      " ...\n",
      " [   0    0    0 ... 1260    2 1308]\n",
      " [   0    0    0 ...   19    1  633]\n",
      " [   0    0    0 ...  255    4   13]]\n",
      "[  639 14117   347 ...    82   863   242]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "1.0\n",
      "0.0\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences_sent = array(padded) #create an array with the sequences\n",
    "\n",
    "X, y = sequences_sent[:,:-1], sequences_sent[:,-1] #separate into input and output sequences\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding\n",
    "print(y)\n",
    "seq_length_sent = X.shape[1] #set sequence length to the number of columns in X\n",
    "\n",
    "len(X[1])\n",
    "len(y)\n",
    "\n",
    "print(y[[3]])\n",
    "\n",
    "print(max(y[3])) #check if there is one-hot encoding for the output word\n",
    "\n",
    "print(min(y[3]))\n",
    "\n",
    "print(len(X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, 129)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 129, 50)           1436500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 129, 100)          60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 28730)             2901730   \n",
      "=================================================================\n",
      "Total params: 4,489,130\n",
      "Trainable params: 4,489,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#create the model\n",
    "model = Sequential() #define model, use sequential model function\n",
    "model.add(Masking(mask_value=0, input_shape=(seq_length_sent,))) #the masking layer, which is needed because of the padded sequences\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length_sent)) #add embedding layer, 50 is the dimensions which will be used to represent each word vector\n",
    "model.add(LSTM(100, return_sequences=True)) #add hidden layer, long short term memory layer\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu')) #dense specifies the structure of the neural network, 100 refer to that there are 100 neurons in the first hidden layer, as defined above. Relu is an argument of rectified linear unit.  \n",
    "model.add(Dense(vocab_size, activation='softmax')) #softmax transform to probabilities \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 1664/80352 [..............................] - ETA: 24:05 - loss: 10.2595 - acc: 0.0186"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #it's a multilevel predicter, because it's categorical, we want a measure which is accuracy, and a cost function which is optimized by adam\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5_sentence')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl_sentence', 'wb')) #w stands for write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need the text, so we can use a sequence as a source input for generating the text\n",
    "doc = load_text('Data/clean_text_sentences.txt') #load cleaned sequences \n",
    "\n",
    "lines = doc.split('\\n') #split by new line\n",
    "\n",
    "\n",
    "lines[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#specify the sequence length\n",
    "seq_length_sent = X.shape[1]\n",
    "\n",
    "print(seq_length_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = load_model('model.h5_sentence')\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl_sentence', 'rb')) #r stands for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select a seed text\n",
    "seed_text_sentence = lines[randint(0,len(lines))] #randint returns a random integer from 0 to the length of lines, index it\n",
    "print(seed_text_sentence + '\\n') #print the selected seed text and make a line shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use tokenizer to assign an integer to every word in the seed text\n",
    "encoded_sentence = tokenizer.texts_to_sequences([seed_text_sentence])[0]\n",
    "\n",
    "#pad the seed text\n",
    "encoded_sentence = pad_sequences([encoded_sentence], maxlen=seq_length_sent, truncating='pre')\n",
    "\n",
    "print(encoded_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#the model.preict_classes will return the index of the word with the highest probability \n",
    "yhat = model.predict_classes(encoded_sentence, verbose=0) #verbose specifies how  you want to 'see' the training progress for each epoch\n",
    "\n",
    "yhat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#look up in the tokenizer what word the indexed word corresponds to\n",
    "\n",
    "out_word = '' #empty variable to store the word output word in \n",
    "for word, index in tokenizer.word_index.items(): #loop through the tokenizers\n",
    "\tif index == yhat: #if the index is equally to yhat, assign the word to the variable out_word\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "        \n",
    "out_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the input sequences will get too long, when adding the output word, need to make sure that it's always 129 words long\n",
    "#encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "def generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text_sentence #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length_sent, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, 20)\n",
    "print(generated)\n",
    "\n",
    "\n",
    "#save the generated text\n",
    "\n",
    "out_filename = 'Data/first_output_sentence.txt'\n",
    "\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following coding blocks are NOT part of lesson 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the function on the whole document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:20])\n",
    "print('Total Tokens: %d' % len(tokens)) #print total number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #group similar tokens and print the number\n",
    "\n",
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# organize the text into sequences of tokens\n",
    "length = 50 + 1 #define the length of the sequences\n",
    "sequences = list() #create an empty list, where we will store all the sequences\n",
    "\n",
    "for i in range(length, len(tokens)): #range([start], stop[, step])\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i] #make sequences with length of 50 + 1\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #join the tokens into a line in the sequence\n",
    "\t# store\n",
    "\tsequences.append(line) #append all the lines to the empty list created before the loop\n",
    "print('Total Sequences: %d' % len(sequences)) #print the total amount (length of sequence) of sequences \n",
    "\n",
    "\n",
    "print(sequences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'Data/clean_text.txt'\n",
    "save_doc(sequences, out_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lesson 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the cleaned data\n",
    "doc = load_text('Data/clean_text.txt')\n",
    "\n",
    "#split the text according to line shift (every string is 51 words)\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Documentation\n",
    "https://keras.io/getting-started/functional-api-guide/\n",
    "\n",
    "https://www.tensorflow.org/install/source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "sequences = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "print(sequences) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# separate into input and output\n",
    "sequences = array(sequences) #create an array with the sequences\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #separate into input and putput sequences\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding\n",
    "print(y)\n",
    "seq_length = X.shape[1] #set sequence length to the number of columns in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential() #define model, use sequential model function\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #add embedding layer, 50 is the dimensions which will be used to represent each word vector\n",
    "model.add(LSTM(100, return_sequences=True)) #add hidden layer, long short term memory layer\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu')) #dense specifies the structure of the neural network, 100 refer to that there are 100 neurons in the first hidden layer, as defined above. Relu is an argument of rectified linear unit.  \n",
    "model.add(Dense(vocab_size, activation='softmax')) #softmax transform to probabilities \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #it's a multilevel predicter, because it's categorical, we want a measure which is accuracy, and a cost function which is optimized by adam\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb')) #w stands for write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need the text, so we can use a sequence as a source input for generating the text\n",
    "doc = load_text('Data/clean_text.txt') #load cleaned sequences \n",
    "\n",
    "lines = doc.split('\\n') #split by new line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specify the sequence length\n",
    "seq_length = len(lines[0].split()) - 1 #look at the first line, split sequences by words, count and minus 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r stands for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#select a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #randint returns a random integer from 0 to the length of lines, index it\n",
    "print(seed_text + '\\n') #print the selected seed text and make a line shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use tokenizer to assign an integer to every word in the seed text\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\n",
    "#exclude the output word, so take the last 50 words\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\n",
    "len(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the model.preict_classes will return the index of the word with the highest probability \n",
    "yhat = model.predict_classes(encoded, verbose=0) #verbose specifies how  you want to 'see' the training progress for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look up in the tokenizer what word the indexed word corresponds to\n",
    "\n",
    "out_word = '' #empty variable to store the word output word in \n",
    "for word, index in tokenizer.word_index.items(): #loop through the tokenizers\n",
    "\tif index == yhat: #if the index is equally to yhat, assign the word to the variable out_word\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the input sequences will get too long, when adding the output word, need to make sure that it's always 50 words long\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #pre indicates that it should take the values from the begining of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)\n",
    "\n",
    "\n",
    "#save the generated text\n",
    "\n",
    "out_filename = 'Data/first_output1.txt'\n",
    "\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to look at in lecture 5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input sequences\n",
    "\n",
    "\n",
    "We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "- Instead, to keep the example brief, we will let all of the text flow together and train the model to predict the next word across sentences, paragraphs, and even books or chapters in the text.\n",
    "\n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "\n",
    "- FInd more fairytales, data is not big enough\n",
    "\n",
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp_project_keras]",
   "language": "python",
   "name": "conda-env-nlp_project_keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

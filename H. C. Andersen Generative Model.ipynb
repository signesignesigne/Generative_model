{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data (lesson 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing packages\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting of by downloading data and looking at the data, which we need to clean. We observe following\n",
    "- Illustration texts\n",
    "- The text file contain multiple stories, these should be split into multiple files in order for the model to know there is groupings (Extension: will be reviewed in lesson 5)\n",
    "- The file contains a number of punctuations (,;.* etc.), which are irrelevant for the model\n",
    "- Capital letters as headers\n",
    "\n",
    "The input sequences are at first at 50 across sentences, chapters and stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to load documents into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\") #loading an existing file\n",
    "\ttext = file.read() #opening the file and assigning it to the variable text\n",
    "\tfile.close() #close the file\n",
    "\treturn text #output = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate text files\n",
    "os.chdir('C:\\\\Users\\\\soil\\\\Desktop\\\\Data')\n",
    " \n",
    "files = os.listdir()\n",
    "\n",
    "filenames = ['22693-0.txt', '3454-0.txt', '52521.txt', '5615-0.txt']\n",
    "\n",
    "with open('adventures.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Just as a little child holds out its hands to catch the sunbeams, to\n",
      "feel and to grasp what, so its eyes tell it, is actually there, so,\n",
      "down through the ages, men have stretched out their hands in eager\n",
      "endeavour to know their God. And because only through the human was\n",
      "the divine knowable, the old peoples of the earth made gods of their\n",
      "heroes and not unfrequently endowed these gods with as many of the\n",
      "vices as of the virtues of their worshippers. As we read the myths of\n",
      "the East and the West we find ever the same story. That portion of the\n",
      "ancient Aryan race which poured from the central plain of Asia,\n",
      "through the rocky defiles of what we now call \"The Frontier,\" to\n",
      "populate the fertile lowlands of India, had gods who must once have\n",
      "been wholly heroic, but who came in time to be more degraded than the\n",
      "most vicious of lustful criminals. And the Greeks, Latins, Teutons,\n",
      "Celts, and Slavonians, who came of the same mighty Aryan stock, did\n",
      "even as those with whom they owned a common ancestry. Originally they\n",
      "gave to their gods of their best. All that was noblest in them, all\n",
      "that was strongest and most selfless, all the higher instincts of\n",
      "their natures were their endowment. And although their worship in time\n",
      "became corrupt and lost its beauty, there yet remains for us, in the\n",
      "old tales of the gods, a wonderful humanity that strikes a vibrant\n",
      "chord in the hearts of those who are the descendants of their\n",
      "worshippers. For though creeds and forms may change, human nature\n",
      "never changes. We are less simple than our fathers: that is all. And,\n",
      "as Professor York Powell[1] most truly says: \"It is not in a man's\n",
      "creed, but in his deeds; not in his knowledge, but in his sympathy,\n",
      "that there lies the essence of what is good and of what will last in\n",
      "human life.\"\n",
      "\n",
      "The most usual habits of mind in our own day are the theoretical and\n",
      "analytical habits. Dissection, vivisection, analysis--those are the\n",
      "processes to which all things not conclusively historical and all\n",
      "things spiritual are bound to pass. Thus we find the old myths\n",
      "classified into Sun Myths and Dawn Myths, Earth Myths and Moon Myths,\n",
      "Fire Myths and Wind Myths, until, as one of the most sane and vigorous\n",
      "thinkers of the present day[2] has justly observed: \"If you take the\n",
      "rhyme of Mary and her little lamb, and call Mary the sun and the lamb\n",
      "the moon, you will achieve astonishing results, both in religion and\n",
      "astronomy, when you find that the lamb followed Mary to school one\n",
      "day.\"\n",
      "\n",
      "In this little collection of Myths, the stories are not presented to\n",
      "the student of folklore as a fresh contribution to his knowledge.\n",
      "Rather is the book intended for those who, in the course of their\n",
      "reading, frequently come across names which possess for them no\n",
      "meaning, and who care to read some old stories, through which runs the\n",
      "same humanity that their own hearts know. For although the old worship\n",
      "has passed away, it is almost impossible for us to open a book that\n",
      "does not contain some mention of the gods of long ago. In our\n",
      "childhood we are given copies of Kingsley's _Heroes_ and of\n",
      "Hawthorne's _Tanglewood Tales_. Later on, we find in Shakespeare,\n",
      "Spenser, Milton, Keats, Shelley, Longfellow, Tennyson, Mrs. Browning,\n",
      "and a host of other writers, constant allusion to the stories of the\n",
      "gods. Scarcely a poet has ever written but makes mention of them in\n",
      "one or other of his poems. It would seem as if there were no get-away\n",
      "from them. We might expect in this twentieth century that the old gods\n",
      "of Greece and of Rome, the gods of our Northern forefathers, the gods\n",
      "of Egypt, the gods of the British race, might be forgotten. But even\n",
      "when we read in a newspaper of aeroplanes, someone is more than likely\n",
      "to quote the story of Bellerophon and his winged steed, or of Icarus,\n",
      "the flyer, and in our daily speech the names of gods and goddesses\n",
      "continually crop up. We drive--or, at least, till lately we drove--in\n",
      "Phaetons. Not only schoolboys swear by Jove or by Jupiter. The silvery\n",
      "substance in our thermometers and barometers is named Mercury.\n",
      "Blacksmiths are accustomed to being referred to as \"sons of Vulcan,\"\n",
      "and beautiful youths to being called \"young Adonises.\" We accept the\n",
      "names of newspapers and debating societies as being the \"Argus,\"\n",
      "without perhaps quite realising who was Argus, the many-eyed. We talk\n",
      "of \"a panic,\" and forget that the great god Pan is father of the word.\n",
      "Even in our religious services we go back to heathenism. Not only are\n",
      "the crockets on our cathedral spires and church pews remnants of\n",
      "fire-worship, but one of our own most beautiful Christian blessings is\n",
      "probably of Assyrian origin. \"The Lord bless thee and keep thee....\n",
      "The Lord make His face to shine upon thee.... The Lord lift up the\n",
      "light of His countenance upon thee....\" So did the priests of the\n",
      "sun-gods invoke blessings upon those who worshipped.\n",
      "\n",
      "We make many discoveries as we study the myths of the North and of the\n",
      "South. In the story of Baldur we find that the goddess Hel ultimately\n",
      "gave her name to the place of punishment precious to the Calvinistic\n",
      "mind. And because the Norseman very much disliked the bitter, cruel\n",
      "cold of the long winter, his heaven was a warm, well-fired abode, and\n",
      "his place of punishment one of terrible frigidity. Somewhere on the\n",
      "other side of the Tweed and Cheviots was the spot selected by the Celt\n",
      "of southern Britain. On the other hand, the eastern mind, which knew\n",
      "the terrors of a sun-smitten land and of a heat that was torture, had\n",
      "for a hell a fiery place of constantly burning flames.\n",
      "\n",
      "In the space permitted, it has not been possible to deal with more\n",
      "than a small number of myths, and the well-known stories of Herakles,\n",
      "of Theseus, and of the Argonauts have been purposely omitted. These\n",
      "have been so perfectly told by great writers that to retell them would\n",
      "seem absurd. The same applies to the Odyssey and the Iliad, the\n",
      "translations of which probably take rank amongst the finest\n",
      "translations in any language.\n",
      "\n",
      "The writer will feel that her object has been gained should any\n",
      "readers of these stories feel that for a little while they have left\n",
      "the toilful utilitarianism of the present day behind them, and, with\n",
      "it, its hampering restrictions of sordid actualities that are so\n",
      "murderous to imagination and to all romance.\n",
      "\n",
      "    \"Great God! I'd rather be\n",
      "    A Pagan suckled in a creed outworn;\n",
      "    So might I, standing on this pleasant lea,\n",
      "    Have glimpses that would make me less forlorn;\n",
      "    Have sight of Proteus rising from the sea;\n",
      "    Or hear old Triton blow his wreathÃ¨d horn.\"\n",
      "\n",
      "        JEAN LANG.\n",
      "\n",
      "\n",
      "POSTSCRIPT\n",
      "\n",
      "We have come, in those last long months, to date our happenings as\n",
      "they have never until now been dated by those of our own generation.\n",
      "\n",
      "We speak of things that took place \"_Before the War_\"; and between\n",
      "that time and this stands a barrier immeasurable.\n",
      "\n",
      "This book, with its Preface, was completed in 1914--\"_Before the\n",
      "War._\"\n",
      "\n",
      "Since August 1914 the finest humanity of our race has been enduring\n",
      "Promethean agonies. But even as Prometheus unflinchingly bore the\n",
      "cruelties of pain, of heat and of cold, of hunger and of thirst, and\n",
      "the tortures inflicted by an obscene bird of prey, so have endured the\n",
      "men of our nation and of those nations with whom we are proud to be\n",
      "allied. Much more remote than they seemed one little year ago, now\n",
      "seem the old stories of sunny Greece. But if we have studied the\n",
      "strange transmogrification of the ancient gods, we can look with\n",
      "interest, if with horror, at the Teuton representation of the GOD in\n",
      "whom we believe as a GOD of perfect purity, of honour, and of love.\n",
      "According to their interpretation of Him, the God of the Huns would\n",
      "seem to be as much a confederate of the vicious as the most degraded\n",
      "god of ancient worship. And if we turn with shame from the Divinity so\n",
      "often and so glibly referred to by blasphemous lips, and look on a\n",
      "picture that tears our hearts, and yet makes our hearts big with\n",
      "pride, we can understand how it was that those heroes who fought and\n",
      "died in the Valley of the Scamander came in time to be regarded not as\n",
      "men, but as gods.\n",
      "\n",
      "There is no tale in all the world's mythology finer than the tale that\n",
      "began in August 1914. How future generations will tell the tale, who\n",
      "can say?\n",
      "\n",
      "But we, for whom Life can never be the same again, can say with all\n",
      "earnestness: \"It is the memory that the soldier leaves behind him,\n",
      "like the long train of light that follows the sunken sun--that is all\n",
      "which is worth caring for, which distinguishes the death of the brave\n",
      "or the ignoble.\"\n",
      "\n",
      "And, surely, to all those who are fighting, and suffering, and dying\n",
      "for a noble cause, the GOD of gods, the GOD of battles, who is also\n",
      "the GOD of peace, and the GOD of Love, has become an ever near and\n",
      "eternally living entity.\n",
      "\n",
      "    \"Our little systems have their day;\n",
      "    They have their day and cease to be,\n",
      "    They are but broken lights of Thee,\n",
      "    And Thou, oh Lord, art more than they.\"\n",
      "\n",
      "        JEAN LANG.\n",
      "\n",
      "\n",
      "FOOTNOTES:\n",
      "\n",
      "[1] _Teutonic Heathendom._\n",
      "\n",
      "[2] John Kelman, D.D., _Among Famous Books_.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "                                                  PAGE\n",
      "\n",
      "  PROMETHEUS AND PANDORA                             1\n",
      "\n",
      "  PYGMALION                                         11\n",
      "\n",
      "  PHAETON                                           16\n",
      "\n",
      "  ENDYMION                                          26\n",
      "\n",
      "  ORPHEUS                                           31\n",
      "\n",
      "  APOLLO AND DAPHNE                                 42\n",
      "\n",
      "  PSYCHE                                            46\n",
      "\n",
      "  THE CALYDONIAN HUNT                               69\n",
      "\n",
      "  ATALANTA                                          78\n",
      "\n",
      "  ARACHNE                                           82\n",
      "\n",
      "  IDAS AND MARPESSA                                 90\n",
      "\n",
      "  ARETHUSA                                         100\n",
      "\n",
      "  PERSEUS THE HERO                                 105\n",
      "\n",
      "  NIOBE                                            124\n",
      "\n",
      "  HYACINTHUS                                       129\n",
      "\n",
      "  KING MIDAS OF THE GOLDEN TOUCH                   134\n",
      "\n",
      "  CEYX AND HALCYONE       \n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'adventures.txt'#specifying the filename of the data we wish to load\n",
    "doc = load_doc(in_filename) #loading the file\n",
    "print(doc[:10000]) #printing the first 200 characters of the loading document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data (lesson 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 1, we observed several potential issues in the data, which we need to remove from the data. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing headers\n",
    "doc = re.sub(r'[A-Z]{2,}','', doc) #replacing capital letters longer than 1 with nothing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special characters, since one of the deliminators\n",
    "#doc = doc.replace(\"'\", '')\n",
    "\n",
    "#deliminators \n",
    "#deliminators = c('THE FIR TREE', 'LITTLE TUK', 'THE UGLY DUCKLING', 'LITTLE IDA'S FLOWERS', 'THE STEADFAST TIN SOLDIER, LITTLE THUMBELINA, SUNSHINE STORIES', 'THE DARNING-NEEDLE', 'THE LITTLE MATCH GIRL', 'THE LOVING PAIR', 'THE LEAPING MATCH', 'THE HAPPY FAMILY, THE GREENIES, 'OLE-LUK-OIE', 'THE DREAM GOD', 'THE MONEY BOX', 'ELDER-TREE MOTHER', 'THE SNOW QUEEN', 'THE ROSES AND THE SPARROWS', 'THE OLD HOUSE', 'THE CONCEITED APPLE BRANCH' \n",
    "\n",
    "#splitting the file into strings consisting of one adventure\n",
    "#doc = doc.split ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc): #making a function\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token - https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\ttable = str.maketrans('', '', string.punctuation) #Third argument specifies the wished deleted items\n",
    "\ttokens = [w.translate(table) for w in tokens] #translate applies the translation table applied on the looping through the tokens list\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #Replace word with word, if the word is alphabetic\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'a', 'little', 'child', 'holds', 'out', 'its', 'hands', 'to', 'catch', 'the', 'sunbeams', 'to', 'feel', 'and', 'to', 'grasp', 'what', 'so', 'its', 'eyes', 'tell', 'it', 'is', 'actually', 'there', 'so', 'down', 'through', 'the', 'ages', 'men', 'have', 'stretched', 'out', 'their', 'hands', 'in', 'eager', 'endeavour', 'to', 'know', 'their', 'god', 'and', 'because', 'only', 'through', 'the', 'human', 'was', 'the', 'divine', 'knowable', 'the', 'old', 'peoples', 'of', 'the', 'earth', 'made', 'gods', 'of', 'their', 'heroes', 'and', 'not', 'unfrequently', 'endowed', 'these', 'gods', 'with', 'as', 'many', 'of', 'the', 'vices', 'as', 'of', 'the', 'virtues', 'of', 'their', 'worshippers', 'as', 'we', 'read', 'the', 'myths', 'of', 'the', 'east', 'and', 'the', 'west', 'we', 'find', 'ever', 'the', 'same', 'story', 'that', 'portion', 'of', 'the', 'ancient', 'aryan', 'race', 'which', 'poured', 'from', 'the', 'central', 'plain', 'of', 'asia', 'through', 'the', 'rocky', 'defiles', 'of', 'what', 'we', 'now', 'call', 'the', 'frontier', 'to', 'populate', 'the', 'fertile', 'lowlands', 'of', 'india', 'had', 'gods', 'who', 'must', 'once', 'have', 'been', 'wholly', 'heroic', 'but', 'who', 'came', 'in', 'time', 'to', 'be', 'more', 'degraded', 'than', 'the', 'most', 'vicious', 'of', 'lustful', 'criminals', 'and', 'the', 'greeks', 'latins', 'teutons', 'celts', 'and', 'slavonians', 'who', 'came', 'of', 'the', 'same', 'mighty', 'aryan', 'stock', 'did', 'even', 'as', 'those', 'with', 'whom', 'they', 'owned', 'a', 'common', 'ancestry', 'originally', 'they', 'gave', 'to', 'their', 'gods', 'of', 'their', 'best', 'all', 'that', 'was', 'noblest', 'in']\n",
      "Total Tokens: 178804\n",
      "Unique Tokens: 8766\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc(doc) #using the clean function to cleaning our document\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens)) #printing the number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #printing the number of unique tokens by grouping similair tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 178753\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1 #defining the length of the sequence\n",
    "sequences = list() #creating an empty list\n",
    "for i in range(length, len(tokens)): #range takes two arguments: start point and end point\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #-join returns a string in which the string elements of sequence have been joined by str separator\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename): #creating a saving function\n",
    "\tdata = '\\n'.join(lines) #joining the lines seperated by 'enter'\n",
    "\tfile = open(filename, 'w', encoding=\"utf8\") #create empty new file\n",
    "\tfile.write(data) #input the data in the empty file\n",
    "\tfile.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'fairytales_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model (Lesson 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the cleaned data\n",
    "in_filename = 'fairytales_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    "#split the text by lineshifts creating a list of 51 items long string\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'he': 4,\n",
       " 'of': 5,\n",
       " 'a': 6,\n",
       " 'was': 7,\n",
       " 'in': 8,\n",
       " 'that': 9,\n",
       " 'his': 10,\n",
       " 'it': 11,\n",
       " 'she': 12,\n",
       " 'her': 13,\n",
       " 'had': 14,\n",
       " 'you': 15,\n",
       " 'him': 16,\n",
       " 'as': 17,\n",
       " 'for': 18,\n",
       " 'with': 19,\n",
       " 'they': 20,\n",
       " 'but': 21,\n",
       " 'on': 22,\n",
       " 'i': 23,\n",
       " 'said': 24,\n",
       " 'at': 25,\n",
       " 'so': 26,\n",
       " 'all': 27,\n",
       " 'not': 28,\n",
       " 'when': 29,\n",
       " 'be': 30,\n",
       " 'is': 31,\n",
       " 'have': 32,\n",
       " 'then': 33,\n",
       " 'one': 34,\n",
       " 'were': 35,\n",
       " 'out': 36,\n",
       " 'there': 37,\n",
       " 'them': 38,\n",
       " 'who': 39,\n",
       " 'this': 40,\n",
       " 'will': 41,\n",
       " 'up': 42,\n",
       " 'little': 43,\n",
       " 'from': 44,\n",
       " 'my': 45,\n",
       " 'could': 46,\n",
       " 'would': 47,\n",
       " 'no': 48,\n",
       " 'into': 49,\n",
       " 'man': 50,\n",
       " 'by': 51,\n",
       " 'king': 52,\n",
       " 'came': 53,\n",
       " 'their': 54,\n",
       " 'which': 55,\n",
       " 'me': 56,\n",
       " 'if': 57,\n",
       " 'went': 58,\n",
       " 'what': 59,\n",
       " 'very': 60,\n",
       " 'did': 61,\n",
       " 'down': 62,\n",
       " 'now': 63,\n",
       " 'do': 64,\n",
       " 'old': 65,\n",
       " 'day': 66,\n",
       " 'are': 67,\n",
       " 'your': 68,\n",
       " 'before': 69,\n",
       " 'where': 70,\n",
       " 'time': 71,\n",
       " 'must': 72,\n",
       " 'only': 73,\n",
       " 'again': 74,\n",
       " 'back': 75,\n",
       " 'about': 76,\n",
       " 'more': 77,\n",
       " 'go': 78,\n",
       " 'himself': 79,\n",
       " 'come': 80,\n",
       " 'after': 81,\n",
       " 'like': 82,\n",
       " 'see': 83,\n",
       " 'we': 84,\n",
       " 'asked': 85,\n",
       " 'great': 86,\n",
       " 'over': 87,\n",
       " 'took': 88,\n",
       " 'been': 89,\n",
       " 'shall': 90,\n",
       " 'than': 91,\n",
       " 'some': 92,\n",
       " 'never': 93,\n",
       " 'off': 94,\n",
       " 'saw': 95,\n",
       " 'how': 96,\n",
       " 'thought': 97,\n",
       " 'away': 98,\n",
       " 'got': 99,\n",
       " 'told': 100,\n",
       " 'or': 101,\n",
       " 'made': 102,\n",
       " 'other': 103,\n",
       " 'princess': 104,\n",
       " 'can': 105,\n",
       " 'get': 106,\n",
       " 'young': 107,\n",
       " 'well': 108,\n",
       " 'home': 109,\n",
       " 'put': 110,\n",
       " 'should': 111,\n",
       " 'an': 112,\n",
       " 'good': 113,\n",
       " 'till': 114,\n",
       " 'answered': 115,\n",
       " 'long': 116,\n",
       " 'two': 117,\n",
       " 'last': 118,\n",
       " 'wife': 119,\n",
       " 'way': 120,\n",
       " 'beautiful': 121,\n",
       " 'heard': 122,\n",
       " 'once': 123,\n",
       " 'know': 124,\n",
       " 'much': 125,\n",
       " 'soon': 126,\n",
       " 'just': 127,\n",
       " 'give': 128,\n",
       " 'quite': 129,\n",
       " 'prince': 130,\n",
       " 'woman': 131,\n",
       " 'take': 132,\n",
       " 'found': 133,\n",
       " 'has': 134,\n",
       " 'house': 135,\n",
       " 'am': 136,\n",
       " 'three': 137,\n",
       " 'through': 138,\n",
       " 'still': 139,\n",
       " 'let': 140,\n",
       " 'set': 141,\n",
       " 'head': 142,\n",
       " 'father': 143,\n",
       " 'its': 144,\n",
       " 'first': 145,\n",
       " 'gave': 146,\n",
       " 'nothing': 147,\n",
       " 'any': 148,\n",
       " 'own': 149,\n",
       " 'heart': 150,\n",
       " 'round': 151,\n",
       " 'knew': 152,\n",
       " 'while': 153,\n",
       " 'next': 154,\n",
       " 'night': 155,\n",
       " 'son': 156,\n",
       " 'looked': 157,\n",
       " 'cried': 158,\n",
       " 'such': 159,\n",
       " 'palace': 160,\n",
       " 'upon': 161,\n",
       " 'ever': 162,\n",
       " 'lay': 163,\n",
       " 'might': 164,\n",
       " 'here': 165,\n",
       " 'another': 166,\n",
       " 'eyes': 167,\n",
       " 'tree': 168,\n",
       " 'girl': 169,\n",
       " 'tell': 170,\n",
       " 'every': 171,\n",
       " 'began': 172,\n",
       " 'horse': 173,\n",
       " 'world': 174,\n",
       " 'poor': 175,\n",
       " 'morning': 176,\n",
       " 'left': 177,\n",
       " 'daughter': 178,\n",
       " 'queen': 179,\n",
       " 'brothers': 180,\n",
       " 'whole': 181,\n",
       " 'hand': 182,\n",
       " 'stood': 183,\n",
       " 'too': 184,\n",
       " 'find': 185,\n",
       " 'men': 186,\n",
       " 'each': 187,\n",
       " 'even': 188,\n",
       " 'brother': 189,\n",
       " 'yet': 190,\n",
       " 'make': 191,\n",
       " 'door': 192,\n",
       " 'us': 193,\n",
       " 'many': 194,\n",
       " 'water': 195,\n",
       " 'bring': 196,\n",
       " 'ran': 197,\n",
       " 'place': 198,\n",
       " 'replied': 199,\n",
       " 'thing': 200,\n",
       " 'under': 201,\n",
       " 'fell': 202,\n",
       " 'these': 203,\n",
       " 'gold': 204,\n",
       " 'sea': 205,\n",
       " 'life': 206,\n",
       " 'lived': 207,\n",
       " 'brought': 208,\n",
       " 'hare': 209,\n",
       " 'something': 210,\n",
       " 'grew': 211,\n",
       " 'herself': 212,\n",
       " 'big': 213,\n",
       " 'going': 214,\n",
       " 'look': 215,\n",
       " 'help': 216,\n",
       " 'mother': 217,\n",
       " 'white': 218,\n",
       " 'called': 219,\n",
       " 'done': 220,\n",
       " 'say': 221,\n",
       " 'love': 222,\n",
       " 'evening': 223,\n",
       " 'wood': 224,\n",
       " 'both': 225,\n",
       " 'may': 226,\n",
       " 'sun': 227,\n",
       " 'without': 228,\n",
       " 'people': 229,\n",
       " 'same': 230,\n",
       " 'children': 231,\n",
       " 'money': 232,\n",
       " 'side': 233,\n",
       " 'days': 234,\n",
       " 'sat': 235,\n",
       " 'forest': 236,\n",
       " 'most': 237,\n",
       " 'enough': 238,\n",
       " 'youth': 239,\n",
       " 'dead': 240,\n",
       " 'red': 241,\n",
       " 'says': 242,\n",
       " 'nor': 243,\n",
       " 'voice': 244,\n",
       " 'eat': 245,\n",
       " 'work': 246,\n",
       " 'behind': 247,\n",
       " 'until': 248,\n",
       " 'felt': 249,\n",
       " 'happened': 250,\n",
       " 'ciccu': 251,\n",
       " 'our': 252,\n",
       " 'turned': 253,\n",
       " 'together': 254,\n",
       " 'also': 255,\n",
       " 'seemed': 256,\n",
       " 'bird': 257,\n",
       " 'reached': 258,\n",
       " 'though': 259,\n",
       " 'think': 260,\n",
       " 'better': 261,\n",
       " 'black': 262,\n",
       " 'far': 263,\n",
       " 'however': 264,\n",
       " 'face': 265,\n",
       " 'words': 266,\n",
       " 'seen': 267,\n",
       " 'room': 268,\n",
       " 'walked': 269,\n",
       " 'right': 270,\n",
       " 'moment': 271,\n",
       " 'story': 272,\n",
       " 'passed': 273,\n",
       " 'end': 274,\n",
       " 'maiden': 275,\n",
       " 'husband': 276,\n",
       " 'wanted': 277,\n",
       " 'sent': 278,\n",
       " 'gone': 279,\n",
       " 'things': 280,\n",
       " 'sight': 281,\n",
       " 'witch': 282,\n",
       " 'hands': 283,\n",
       " 'those': 284,\n",
       " 'flew': 285,\n",
       " 'leave': 286,\n",
       " 'large': 287,\n",
       " 'bed': 288,\n",
       " 'always': 289,\n",
       " 'ground': 290,\n",
       " 'anything': 291,\n",
       " 'death': 292,\n",
       " 'cut': 293,\n",
       " 'everything': 294,\n",
       " 'hans': 295,\n",
       " 'fire': 296,\n",
       " 'length': 297,\n",
       " 'years': 298,\n",
       " 'rest': 299,\n",
       " 'gerda': 300,\n",
       " 'river': 301,\n",
       " 'held': 302,\n",
       " 'tired': 303,\n",
       " 'fish': 304,\n",
       " 'castle': 305,\n",
       " 'whom': 306,\n",
       " 'whose': 307,\n",
       " 'near': 308,\n",
       " 'dark': 309,\n",
       " 'keep': 310,\n",
       " 'along': 311,\n",
       " 'lion': 312,\n",
       " 'god': 313,\n",
       " 'child': 314,\n",
       " 'gods': 315,\n",
       " 'full': 316,\n",
       " 'dear': 317,\n",
       " 'spoke': 318,\n",
       " 'ready': 319,\n",
       " 'because': 320,\n",
       " 'became': 321,\n",
       " 'alone': 322,\n",
       " 'lad': 323,\n",
       " 'pieces': 324,\n",
       " 'land': 325,\n",
       " 'being': 326,\n",
       " 'town': 327,\n",
       " 'happy': 328,\n",
       " 'open': 329,\n",
       " 'stone': 330,\n",
       " 'church': 331,\n",
       " 'golden': 332,\n",
       " 'roland': 333,\n",
       " 'feet': 334,\n",
       " 'indeed': 335,\n",
       " 'high': 336,\n",
       " 'sure': 337,\n",
       " 'boy': 338,\n",
       " 'wind': 339,\n",
       " 'married': 340,\n",
       " 'bride': 341,\n",
       " 'joy': 342,\n",
       " 'esben': 343,\n",
       " 'against': 344,\n",
       " 'hear': 345,\n",
       " 'lasse': 346,\n",
       " 'led': 347,\n",
       " 'opened': 348,\n",
       " 'trees': 349,\n",
       " 'youngest': 350,\n",
       " 'suddenly': 351,\n",
       " 'lady': 352,\n",
       " 'earth': 353,\n",
       " 'ask': 354,\n",
       " 'met': 355,\n",
       " 'sleep': 356,\n",
       " 'silver': 357,\n",
       " 'window': 358,\n",
       " 'few': 359,\n",
       " 'why': 360,\n",
       " 'chest': 361,\n",
       " 'green': 362,\n",
       " 'hair': 363,\n",
       " 'second': 364,\n",
       " 'taken': 365,\n",
       " 'seven': 366,\n",
       " 'saying': 367,\n",
       " 'really': 368,\n",
       " 'coming': 369,\n",
       " 'wish': 370,\n",
       " 'kept': 371,\n",
       " 'almost': 372,\n",
       " 'horses': 373,\n",
       " 'entered': 374,\n",
       " 'want': 375,\n",
       " 'best': 376,\n",
       " 'pan': 377,\n",
       " 'mountain': 378,\n",
       " 'troll': 379,\n",
       " 'answer': 380,\n",
       " 'laid': 381,\n",
       " 'food': 382,\n",
       " 'sister': 383,\n",
       " 'sword': 384,\n",
       " 'sons': 385,\n",
       " 'lying': 386,\n",
       " 'hold': 387,\n",
       " 'caught': 388,\n",
       " 'fast': 389,\n",
       " 'returned': 390,\n",
       " 'sprang': 391,\n",
       " 'arrived': 392,\n",
       " 'lost': 393,\n",
       " 'given': 394,\n",
       " 'thou': 395,\n",
       " 'rose': 396,\n",
       " 'towards': 397,\n",
       " 'perhaps': 398,\n",
       " 'walter': 399,\n",
       " 'among': 400,\n",
       " 'cold': 401,\n",
       " 'live': 402,\n",
       " 'true': 403,\n",
       " 'shore': 404,\n",
       " 'master': 405,\n",
       " 'fine': 406,\n",
       " 'table': 407,\n",
       " 'none': 408,\n",
       " 'able': 409,\n",
       " 'name': 410,\n",
       " 'course': 411,\n",
       " 'bade': 412,\n",
       " 'rode': 413,\n",
       " 'stopped': 414,\n",
       " 'rock': 415,\n",
       " 'arms': 416,\n",
       " 'threw': 417,\n",
       " 'kingdom': 418,\n",
       " 'standing': 419,\n",
       " 'moti': 420,\n",
       " 'beauty': 421,\n",
       " 'speak': 422,\n",
       " 'giant': 423,\n",
       " 'sweet': 424,\n",
       " 'thy': 425,\n",
       " 'others': 426,\n",
       " 'themselves': 427,\n",
       " 'country': 428,\n",
       " 'turn': 429,\n",
       " 'use': 430,\n",
       " 'neck': 431,\n",
       " 'eldest': 432,\n",
       " 'fair': 433,\n",
       " 'filled': 434,\n",
       " 'asleep': 435,\n",
       " 'cannot': 436,\n",
       " 'rich': 437,\n",
       " 'carried': 438,\n",
       " 'new': 439,\n",
       " 'mind': 440,\n",
       " 'straight': 441,\n",
       " 'hard': 442,\n",
       " 'beside': 443,\n",
       " 'longer': 444,\n",
       " 'rather': 445,\n",
       " 'looking': 446,\n",
       " 'shifty': 447,\n",
       " 'smith': 448,\n",
       " 'air': 449,\n",
       " 'past': 450,\n",
       " 'drew': 451,\n",
       " 'strong': 452,\n",
       " 'top': 453,\n",
       " 'skin': 454,\n",
       " 'between': 455,\n",
       " 'dove': 456,\n",
       " 'small': 457,\n",
       " 'third': 458,\n",
       " 'marry': 459,\n",
       " 'dog': 460,\n",
       " 'boat': 461,\n",
       " 'fall': 462,\n",
       " 'free': 463,\n",
       " 'sound': 464,\n",
       " 'half': 465,\n",
       " 'above': 466,\n",
       " 'promised': 467,\n",
       " 'eaten': 468,\n",
       " 'human': 469,\n",
       " 'seized': 470,\n",
       " 'across': 471,\n",
       " 'spring': 472,\n",
       " 'outside': 473,\n",
       " 'garden': 474,\n",
       " 'mouth': 475,\n",
       " 'close': 476,\n",
       " 'loved': 477,\n",
       " 'fear': 478,\n",
       " 'begged': 479,\n",
       " 'light': 480,\n",
       " 'run': 481,\n",
       " 'return': 482,\n",
       " 'pleased': 483,\n",
       " 'become': 484,\n",
       " 'charlemagne': 485,\n",
       " 'drink': 486,\n",
       " 'clothes': 487,\n",
       " 'kay': 488,\n",
       " 'thus': 489,\n",
       " 'followed': 490,\n",
       " 'used': 491,\n",
       " 'pretty': 492,\n",
       " 'corner': 493,\n",
       " 'tied': 494,\n",
       " 'exclaimed': 495,\n",
       " 'whether': 496,\n",
       " 'died': 497,\n",
       " 'word': 498,\n",
       " 'birds': 499,\n",
       " 'power': 500,\n",
       " 'flowers': 501,\n",
       " 'tried': 502,\n",
       " 'body': 503,\n",
       " 'else': 504,\n",
       " 'struck': 505,\n",
       " 'thee': 506,\n",
       " 'carry': 507,\n",
       " 'thinking': 508,\n",
       " 'hundred': 509,\n",
       " 'four': 510,\n",
       " 'kill': 511,\n",
       " 'living': 512,\n",
       " 'care': 513,\n",
       " 'need': 514,\n",
       " 'friend': 515,\n",
       " 'tears': 516,\n",
       " 'angry': 517,\n",
       " 'wicked': 518,\n",
       " 'sitting': 519,\n",
       " 'neither': 520,\n",
       " 'part': 521,\n",
       " 'ears': 522,\n",
       " 'lives': 523,\n",
       " 'often': 524,\n",
       " 'wished': 525,\n",
       " 'frightened': 526,\n",
       " 'music': 527,\n",
       " 'strange': 528,\n",
       " 'getting': 529,\n",
       " 'inside': 530,\n",
       " 'catherine': 531,\n",
       " 'call': 532,\n",
       " 'creature': 533,\n",
       " 'believe': 534,\n",
       " 'evil': 535,\n",
       " 'laughed': 536,\n",
       " 'stay': 537,\n",
       " 'snow': 538,\n",
       " 'hungry': 539,\n",
       " 'ship': 540,\n",
       " 'lovely': 541,\n",
       " 'killed': 542,\n",
       " 'hardly': 543,\n",
       " 'afraid': 544,\n",
       " 'matter': 545,\n",
       " 'monkey': 546,\n",
       " 'donkey': 547,\n",
       " 'hut': 548,\n",
       " 'stick': 549,\n",
       " 'known': 550,\n",
       " 'beast': 551,\n",
       " 'front': 552,\n",
       " 'stream': 553,\n",
       " 'nearly': 554,\n",
       " 'pay': 555,\n",
       " 'sheep': 556,\n",
       " 'stable': 557,\n",
       " 'anyone': 558,\n",
       " 'lindorm': 559,\n",
       " 'drove': 560,\n",
       " 'forth': 561,\n",
       " 'ill': 562,\n",
       " 'hung': 563,\n",
       " 'wall': 564,\n",
       " 'wise': 565,\n",
       " 'quickly': 566,\n",
       " 'trouble': 567,\n",
       " 'year': 568,\n",
       " 'ate': 569,\n",
       " 'lake': 570,\n",
       " 'middle': 571,\n",
       " 'safe': 572,\n",
       " 'myself': 573,\n",
       " 'dinner': 574,\n",
       " 'finished': 575,\n",
       " 'beasts': 576,\n",
       " 'soul': 577,\n",
       " 'remained': 578,\n",
       " 'bear': 579,\n",
       " 'leaves': 580,\n",
       " 'save': 581,\n",
       " 'branches': 582,\n",
       " 'deep': 583,\n",
       " 'play': 584,\n",
       " 'hall': 585,\n",
       " 'stones': 586,\n",
       " 'noise': 587,\n",
       " 'everyone': 588,\n",
       " 'road': 589,\n",
       " 'bad': 590,\n",
       " 'fox': 591,\n",
       " 'wedding': 592,\n",
       " 'since': 593,\n",
       " 'instead': 594,\n",
       " 'anger': 595,\n",
       " 'having': 596,\n",
       " 'played': 597,\n",
       " 'oh': 598,\n",
       " 'seek': 599,\n",
       " 'listened': 600,\n",
       " 'thick': 601,\n",
       " 'hid': 602,\n",
       " 'friends': 603,\n",
       " 'broke': 604,\n",
       " 'christian': 605,\n",
       " 'twelve': 606,\n",
       " 'yes': 607,\n",
       " 'catch': 608,\n",
       " 'covered': 609,\n",
       " 'blue': 610,\n",
       " 'dressed': 611,\n",
       " 'peace': 612,\n",
       " 'arm': 613,\n",
       " 'steps': 614,\n",
       " 'heads': 615,\n",
       " 'court': 616,\n",
       " 'shoes': 617,\n",
       " 'waiting': 618,\n",
       " 'squire': 619,\n",
       " 'yourself': 620,\n",
       " 'path': 621,\n",
       " 'flung': 622,\n",
       " 'ordered': 623,\n",
       " 'six': 624,\n",
       " 'wolf': 625,\n",
       " 'service': 626,\n",
       " 'grey': 627,\n",
       " 'baby': 628,\n",
       " 'either': 629,\n",
       " 'knowing': 630,\n",
       " 'spent': 631,\n",
       " 'paid': 632,\n",
       " 'slowly': 633,\n",
       " 'carefully': 634,\n",
       " 'cry': 635,\n",
       " 'slept': 636,\n",
       " 'kind': 637,\n",
       " 'thanked': 638,\n",
       " 'lie': 639,\n",
       " 'perseus': 640,\n",
       " 'journey': 641,\n",
       " 'further': 642,\n",
       " 'talk': 643,\n",
       " 'buy': 644,\n",
       " 'robbers': 645,\n",
       " 'jackal': 646,\n",
       " 'feel': 647,\n",
       " 'hearts': 648,\n",
       " 'show': 649,\n",
       " 'courage': 650,\n",
       " 'therefore': 651,\n",
       " 'touched': 652,\n",
       " 'declared': 653,\n",
       " 'heavy': 654,\n",
       " 'stand': 655,\n",
       " 'branch': 656,\n",
       " 'appeared': 657,\n",
       " 'iron': 658,\n",
       " 'shut': 659,\n",
       " 'late': 660,\n",
       " 'crept': 661,\n",
       " 'besides': 662,\n",
       " 'distance': 663,\n",
       " 'crow': 664,\n",
       " 'pot': 665,\n",
       " 'maurice': 666,\n",
       " 'shoemaker': 667,\n",
       " 'doing': 668,\n",
       " 'itself': 669,\n",
       " 'terrible': 670,\n",
       " 'honour': 671,\n",
       " 'sorrow': 672,\n",
       " 'hope': 673,\n",
       " 'fairy': 674,\n",
       " 'comes': 675,\n",
       " 'die': 676,\n",
       " 'lifted': 677,\n",
       " 'glad': 678,\n",
       " 'fat': 679,\n",
       " 'blood': 680,\n",
       " 'shouted': 681,\n",
       " 'walking': 682,\n",
       " 'easily': 683,\n",
       " 'ring': 684,\n",
       " 'already': 685,\n",
       " 'cottage': 686,\n",
       " 'foot': 687,\n",
       " 'shark': 688,\n",
       " 'knocked': 689,\n",
       " 'princes': 690,\n",
       " 'inn': 691,\n",
       " 'glass': 692,\n",
       " 'cow': 693,\n",
       " 'snowman': 694,\n",
       " 'splendid': 695,\n",
       " 'wonderful': 696,\n",
       " 'sky': 697,\n",
       " 'gift': 698,\n",
       " 'shape': 699,\n",
       " 'low': 700,\n",
       " 'sang': 701,\n",
       " 'brown': 702,\n",
       " 'counsel': 703,\n",
       " 'wild': 704,\n",
       " 'bit': 705,\n",
       " 'sit': 706,\n",
       " 'break': 707,\n",
       " 'deirdrÃª': 708,\n",
       " 'taking': 709,\n",
       " 'bread': 710,\n",
       " 'hot': 711,\n",
       " 'making': 712,\n",
       " 'order': 713,\n",
       " 'least': 714,\n",
       " 'wept': 715,\n",
       " 'pass': 716,\n",
       " 'brave': 717,\n",
       " 'legs': 718,\n",
       " 'grip': 719,\n",
       " 'walk': 720,\n",
       " 'listen': 721,\n",
       " 'clever': 722,\n",
       " 'greatly': 723,\n",
       " 'bright': 724,\n",
       " 'kissed': 725,\n",
       " 'meanwhile': 726,\n",
       " 'fellow': 727,\n",
       " 'agreed': 728,\n",
       " 'wide': 729,\n",
       " 'worse': 730,\n",
       " 'silk': 731,\n",
       " 'tailor': 732,\n",
       " 'stretched': 733,\n",
       " 'born': 734,\n",
       " 'surely': 735,\n",
       " 'lid': 736,\n",
       " 'rushed': 737,\n",
       " 'stayed': 738,\n",
       " 'wings': 739,\n",
       " 'spread': 740,\n",
       " 'grow': 741,\n",
       " 'lonely': 742,\n",
       " 'within': 743,\n",
       " 'maid': 744,\n",
       " 'follow': 745,\n",
       " 'sisters': 746,\n",
       " 'sooner': 747,\n",
       " 'watch': 748,\n",
       " 'floor': 749,\n",
       " 'thread': 750,\n",
       " 'send': 751,\n",
       " 'shook': 752,\n",
       " 'stop': 753,\n",
       " 'warm': 754,\n",
       " 'quick': 755,\n",
       " 'drank': 756,\n",
       " 'crying': 757,\n",
       " 'village': 758,\n",
       " 'gate': 759,\n",
       " 'except': 760,\n",
       " 'hermit': 761,\n",
       " 'begin': 762,\n",
       " 'knife': 763,\n",
       " 'sack': 764,\n",
       " 'talking': 765,\n",
       " 'servant': 766,\n",
       " 'firtree': 767,\n",
       " 'don': 768,\n",
       " 'purse': 769,\n",
       " 'lÃ®r': 770,\n",
       " 'apollo': 771,\n",
       " 'ugly': 772,\n",
       " 'box': 773,\n",
       " 'thousand': 774,\n",
       " 'escape': 775,\n",
       " 'meant': 776,\n",
       " 'storm': 777,\n",
       " 'around': 778,\n",
       " 'soft': 779,\n",
       " 'does': 780,\n",
       " 'feathers': 781,\n",
       " 'marriage': 782,\n",
       " 'broken': 783,\n",
       " 'started': 784,\n",
       " 'forward': 785,\n",
       " 'farewell': 786,\n",
       " 'waited': 787,\n",
       " 'gladly': 788,\n",
       " 'hour': 789,\n",
       " 'pulled': 790,\n",
       " 'host': 791,\n",
       " 'received': 792,\n",
       " 'jumped': 793,\n",
       " 'pocket': 794,\n",
       " 'try': 795,\n",
       " 'liked': 796,\n",
       " 'immediately': 797,\n",
       " 'jogi': 798,\n",
       " 'animal': 799,\n",
       " 'dress': 800,\n",
       " 'basket': 801,\n",
       " 'fool': 802,\n",
       " 'peasant': 803,\n",
       " 'darkness': 804,\n",
       " 'times': 805,\n",
       " 'grown': 806,\n",
       " 'fresh': 807,\n",
       " 'war': 808,\n",
       " 'eye': 809,\n",
       " 'flying': 810,\n",
       " 'elder': 811,\n",
       " 'wait': 812,\n",
       " 'amongst': 813,\n",
       " 'beat': 814,\n",
       " 'forget': 815,\n",
       " 'dared': 816,\n",
       " 'ago': 817,\n",
       " 'blow': 818,\n",
       " 'cast': 819,\n",
       " 'placed': 820,\n",
       " 'merry': 821,\n",
       " 'snake': 822,\n",
       " 'ice': 823,\n",
       " 'goat': 824,\n",
       " 'sometimes': 825,\n",
       " 'seeing': 826,\n",
       " 'city': 827,\n",
       " 'ganelon': 828,\n",
       " 'person': 829,\n",
       " 'fallen': 830,\n",
       " 'reach': 831,\n",
       " 'ones': 832,\n",
       " 'bound': 833,\n",
       " 'dry': 834,\n",
       " 'tomorrow': 835,\n",
       " 'delighted': 836,\n",
       " 'nice': 837,\n",
       " 'noticed': 838,\n",
       " 'clean': 839,\n",
       " 'gruagach': 840,\n",
       " 'woke': 841,\n",
       " 'bottom': 842,\n",
       " 'clerk': 843,\n",
       " 'tanuki': 844,\n",
       " 'eleven': 845,\n",
       " 'read': 846,\n",
       " 'lose': 847,\n",
       " 'royal': 848,\n",
       " 'terror': 849,\n",
       " 'alive': 850,\n",
       " 'pain': 851,\n",
       " 'fierce': 852,\n",
       " 'silent': 853,\n",
       " 'women': 854,\n",
       " 'fly': 855,\n",
       " 'mistress': 856,\n",
       " 'tale': 857,\n",
       " 'waves': 858,\n",
       " 'fruit': 859,\n",
       " 'rolled': 860,\n",
       " 'burning': 861,\n",
       " 'promise': 862,\n",
       " 'clear': 863,\n",
       " 'forgot': 864,\n",
       " 'forgotten': 865,\n",
       " 'sailed': 866,\n",
       " 'cross': 867,\n",
       " 'cost': 868,\n",
       " 'dancing': 869,\n",
       " 'grass': 870,\n",
       " 'running': 871,\n",
       " 'loud': 872,\n",
       " 'plenty': 873,\n",
       " 'single': 874,\n",
       " 'shining': 875,\n",
       " 'winter': 876,\n",
       " 'climbed': 877,\n",
       " 'determined': 878,\n",
       " 'managed': 879,\n",
       " 'coat': 880,\n",
       " 'pushed': 881,\n",
       " 'luck': 882,\n",
       " 'jonas': 883,\n",
       " 'roof': 884,\n",
       " 'snowflake': 885,\n",
       " 'less': 886,\n",
       " 'psyche': 887,\n",
       " 'doors': 888,\n",
       " 'picked': 889,\n",
       " 'touch': 890,\n",
       " 'perfect': 891,\n",
       " 'watched': 892,\n",
       " 'veil': 893,\n",
       " 'remembered': 894,\n",
       " 'early': 895,\n",
       " 'leaving': 896,\n",
       " 'changed': 897,\n",
       " 'enemy': 898,\n",
       " 'throw': 899,\n",
       " 'burned': 900,\n",
       " 'hanging': 901,\n",
       " 'bid': 902,\n",
       " 'scarcely': 903,\n",
       " 'playing': 904,\n",
       " 'mounted': 905,\n",
       " 'magic': 906,\n",
       " 'bitterly': 907,\n",
       " 'burst': 908,\n",
       " 'news': 909,\n",
       " 'farmer': 910,\n",
       " 'visit': 911,\n",
       " 'supper': 912,\n",
       " 'dragon': 913,\n",
       " 'peter': 914,\n",
       " 'although': 915,\n",
       " 'change': 916,\n",
       " 'learned': 917,\n",
       " 'company': 918,\n",
       " 'song': 919,\n",
       " 'wine': 920,\n",
       " 'saved': 921,\n",
       " 'cruel': 922,\n",
       " 'o': 923,\n",
       " 'lest': 924,\n",
       " 'destiny': 925,\n",
       " 'danger': 926,\n",
       " 'truth': 927,\n",
       " 'chamber': 928,\n",
       " 'moved': 929,\n",
       " 'judge': 930,\n",
       " 'sought': 931,\n",
       " 'cave': 932,\n",
       " 'fight': 933,\n",
       " 'showed': 934,\n",
       " 'shoulder': 935,\n",
       " 'hole': 936,\n",
       " 'understand': 937,\n",
       " 'hidden': 938,\n",
       " 'salt': 939,\n",
       " 'windows': 940,\n",
       " 'quietly': 941,\n",
       " 'hurry': 942,\n",
       " 'pair': 943,\n",
       " 'hunting': 944,\n",
       " 'houses': 945,\n",
       " 'parents': 946,\n",
       " 'eating': 947,\n",
       " 'orders': 948,\n",
       " 'notice': 949,\n",
       " 'corn': 950,\n",
       " 'manage': 951,\n",
       " 'carriage': 952,\n",
       " 'eight': 953,\n",
       " 'bag': 954,\n",
       " 'slave': 955,\n",
       " 'rapunzel': 956,\n",
       " 'shirtcollar': 957,\n",
       " 'lamp': 958,\n",
       " 'giovanni': 959,\n",
       " 'mighty': 960,\n",
       " 'prometheus': 961,\n",
       " 'eros': 962,\n",
       " 'fishes': 963,\n",
       " 'proud': 964,\n",
       " 'hastened': 965,\n",
       " 'animals': 966,\n",
       " 'tongue': 967,\n",
       " 'piece': 968,\n",
       " 'dragged': 969,\n",
       " 'sad': 970,\n",
       " 'search': 971,\n",
       " 'meet': 972,\n",
       " 'choose': 973,\n",
       " 'furious': 974,\n",
       " 'dreams': 975,\n",
       " 'nurse': 976,\n",
       " 'awoke': 977,\n",
       " 'cage': 978,\n",
       " 'offered': 979,\n",
       " 'remember': 980,\n",
       " 'summer': 981,\n",
       " 'edge': 982,\n",
       " 'fond': 983,\n",
       " 'haste': 984,\n",
       " 'manner': 985,\n",
       " 'crowns': 986,\n",
       " 'wandered': 987,\n",
       " 'beginning': 988,\n",
       " 'continued': 989,\n",
       " 'bushes': 990,\n",
       " 'jump': 991,\n",
       " 'idea': 992,\n",
       " 'gallows': 993,\n",
       " 'heed': 994,\n",
       " 'slipped': 995,\n",
       " 'instantly': 996,\n",
       " 'ought': 997,\n",
       " 'pull': 998,\n",
       " 'breakfast': 999,\n",
       " 'exactly': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer() #assign the tokenizer function to a variable\n",
    "tokenizer.fit_on_texts(lines) #function of keras finds all of the unique words in the data and assigns each a unique integer\n",
    "sequences = tokenizer.texts_to_sequences(lines) #translating the input lines into integers\n",
    "tokenizer.word_index #checking the dictionary of the transformed wordsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8767"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating vocabulary size to estimate the size of the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1 #since indexing of array are zero-offset, the index of the vocabulary must be one larger than the length\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences) #transforming the sequens of integers to arrays\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #defining X (input sequences) and y (output words)\n",
    "y = to_categorical(y, num_classes=vocab_size) #to_categorical converts a class vector (integers) to binary class matrix\n",
    "seq_length = X.shape[1] #gives you the dimension of the array, which we put to be 50\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            438350    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8767)              885467    \n",
      "=================================================================\n",
      "Total params: 1,474,717\n",
      "Trainable params: 1,474,717\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "# fit model\n",
    "#model.fit(X, y, batch_size=128, epochs=100) #training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "doc = load_doc('fairytales_sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying input sequences length to prompt the model\n",
    "seq_length = len(lines[3].split()) - 1 #splitting the sequences into words, counting them -1\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and like a candle clear in this dark country of the world thou art my woe my early light my music dying stephen phillips then idas in the humility that comes from perfect love drooped low his head and was silent in silence for a minute stood the threea god a\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating the input text to integers\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 191ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up the index in the Tokenizers mapping to get the associated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bullcalf'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a loop to translate integer to word\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\tif index == yhat:\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "\n",
    "out_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences will get too long, in order to keep them to 50 items using the following function, which pads sequences to the same lengt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the max length to be 50 items by removing items from the beginnning of the sequence\n",
    "#encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function, which generates the predicted output\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list() #make an empty list\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bullcalf bullcalf bullcalf hearing hearing hearing hearing hearing pitifully pitifully pitifully pitifully pitifully pitifully sparrow liberty liberty liberty liberty gael rise translation translation translation summer bullcalf bullcalf bullcalf hearing hearing kindly hearing limpets universe summer universe summer quickset quickset quickset taverner taverner taverner bee bee spanned spanned bullcalf bullcalf roguishly roguishly roguishly plaiting clerk walked walked walked probing probing vineyards vineyards overhear violets violets excursion reveals eva eva iveragh stately stately stainless stainless stainless stainless boating boating refreshed liberty glimmer glimmer stir stir stir triton enclosure lurking lurking lurking lurking lurking surprises surprises sparrow sparrow roguishly roguishly roguishly clerk clerk\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'first_output.txt'\n",
    "save_doc(generated, out_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Advancing the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Word Embedding. Extend the model to use pre-trained word2vec or GloVe vectors to see if it results in a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "os.chdir('C:\\\\Users\\\\soil\\\\Desktop')\n",
    "f = open('glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 100)           876700    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8767)              885467    \n",
      "=================================================================\n",
      "Total params: 1,933,067\n",
      "Trainable params: 1,056,367\n",
      "Non-trainable params: 876,700\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=seq_length, trainable = False)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "178753/178753 [==============================] - 422s 2ms/step - loss: 6.2823 - acc: 0.0745\n",
      "Epoch 2/100\n",
      "178753/178753 [==============================] - 480s 3ms/step - loss: 5.9009 - acc: 0.0967\n",
      "Epoch 3/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 5.6119 - acc: 0.1148\n",
      "Epoch 4/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 5.4314 - acc: 0.1244\n",
      "Epoch 5/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 5.2870 - acc: 0.1316\n",
      "Epoch 6/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 5.1701 - acc: 0.1355\n",
      "Epoch 7/100\n",
      "178753/178753 [==============================] - 390s 2ms/step - loss: 5.0733 - acc: 0.1394\n",
      "Epoch 8/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.9883 - acc: 0.1424\n",
      "Epoch 9/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.9139 - acc: 0.1462\n",
      "Epoch 10/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 4.8488 - acc: 0.1487\n",
      "Epoch 11/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 4.7872 - acc: 0.1512\n",
      "Epoch 12/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 4.7307 - acc: 0.1542\n",
      "Epoch 13/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 4.6768 - acc: 0.1556\n",
      "Epoch 14/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 4.6290 - acc: 0.1576\n",
      "Epoch 15/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.5835 - acc: 0.1595\n",
      "Epoch 16/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 4.5416 - acc: 0.1613\n",
      "Epoch 17/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.5033 - acc: 0.1629\n",
      "Epoch 18/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.4682 - acc: 0.1654\n",
      "Epoch 19/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.4347 - acc: 0.1667\n",
      "Epoch 20/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.4012 - acc: 0.1686\n",
      "Epoch 21/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.3745 - acc: 0.1703\n",
      "Epoch 22/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.3458 - acc: 0.1719\n",
      "Epoch 23/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 4.3204 - acc: 0.1732\n",
      "Epoch 24/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 4.2968 - acc: 0.1754\n",
      "Epoch 25/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.2753 - acc: 0.1771\n",
      "Epoch 26/100\n",
      "178753/178753 [==============================] - 391s 2ms/step - loss: 4.2523 - acc: 0.1790\n",
      "Epoch 27/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 4.2294 - acc: 0.1806\n",
      "Epoch 28/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 4.2078 - acc: 0.1829\n",
      "Epoch 29/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 4.1875 - acc: 0.1843\n",
      "Epoch 30/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.1666 - acc: 0.1858\n",
      "Epoch 31/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.1466 - acc: 0.1878\n",
      "Epoch 32/100\n",
      "178753/178753 [==============================] - 381s 2ms/step - loss: 4.1279 - acc: 0.1903\n",
      "Epoch 33/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.1081 - acc: 0.1912\n",
      "Epoch 34/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 4.0901 - acc: 0.1932\n",
      "Epoch 35/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 4.0715 - acc: 0.1944\n",
      "Epoch 36/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 4.0543 - acc: 0.1971\n",
      "Epoch 37/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 4.0376 - acc: 0.1976\n",
      "Epoch 38/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 4.0220 - acc: 0.1995\n",
      "Epoch 39/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 4.0075 - acc: 0.2014\n",
      "Epoch 40/100\n",
      "178753/178753 [==============================] - 383s 2ms/step - loss: 3.9911 - acc: 0.2034\n",
      "Epoch 41/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 3.9730 - acc: 0.2045\n",
      "Epoch 42/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 3.9604 - acc: 0.2063\n",
      "Epoch 43/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 3.9441 - acc: 0.2078\n",
      "Epoch 44/100\n",
      "178753/178753 [==============================] - 406s 2ms/step - loss: 3.9294 - acc: 0.2096\n",
      "Epoch 45/100\n",
      "178753/178753 [==============================] - 423s 2ms/step - loss: 3.9132 - acc: 0.2109\n",
      "Epoch 46/100\n",
      "178753/178753 [==============================] - 413s 2ms/step - loss: 3.9003 - acc: 0.2123\n",
      "Epoch 47/100\n",
      "178753/178753 [==============================] - 419s 2ms/step - loss: 3.8833 - acc: 0.2145\n",
      "Epoch 48/100\n",
      "178753/178753 [==============================] - 389s 2ms/step - loss: 3.8678 - acc: 0.2152\n",
      "Epoch 49/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.8535 - acc: 0.2173\n",
      "Epoch 50/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.8359 - acc: 0.2191\n",
      "Epoch 51/100\n",
      "178753/178753 [==============================] - 389s 2ms/step - loss: 3.8247 - acc: 0.2198\n",
      "Epoch 52/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 3.8076 - acc: 0.2217\n",
      "Epoch 53/100\n",
      "178753/178753 [==============================] - 391s 2ms/step - loss: 3.7966 - acc: 0.2236\n",
      "Epoch 54/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.7816 - acc: 0.2250\n",
      "Epoch 55/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.7679 - acc: 0.2260\n",
      "Epoch 56/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.7547 - acc: 0.2280\n",
      "Epoch 57/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.7395 - acc: 0.2301\n",
      "Epoch 58/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.7299 - acc: 0.2308\n",
      "Epoch 59/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.7132 - acc: 0.2327\n",
      "Epoch 60/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.7032 - acc: 0.2345\n",
      "Epoch 61/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.6897 - acc: 0.2353\n",
      "Epoch 62/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.6763 - acc: 0.2370\n",
      "Epoch 63/100\n",
      "178753/178753 [==============================] - 392s 2ms/step - loss: 3.6644 - acc: 0.2397\n",
      "Epoch 64/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.6506 - acc: 0.2399\n",
      "Epoch 65/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.6403 - acc: 0.2419\n",
      "Epoch 66/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.6310 - acc: 0.2428\n",
      "Epoch 67/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.6192 - acc: 0.2439\n",
      "Epoch 68/100\n",
      "178753/178753 [==============================] - 389s 2ms/step - loss: 3.6069 - acc: 0.2462\n",
      "Epoch 69/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.5970 - acc: 0.2471\n",
      "Epoch 70/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.5888 - acc: 0.2482\n",
      "Epoch 71/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.5768 - acc: 0.2507\n",
      "Epoch 72/100\n",
      "178753/178753 [==============================] - 391s 2ms/step - loss: 3.5681 - acc: 0.2509\n",
      "Epoch 73/100\n",
      "178753/178753 [==============================] - 389s 2ms/step - loss: 3.5576 - acc: 0.2524\n",
      "Epoch 74/100\n",
      "178753/178753 [==============================] - 390s 2ms/step - loss: 3.5482 - acc: 0.2536\n",
      "Epoch 75/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.5387 - acc: 0.2545\n",
      "Epoch 76/100\n",
      "178753/178753 [==============================] - 389s 2ms/step - loss: 3.5303 - acc: 0.2560\n",
      "Epoch 77/100\n",
      "178753/178753 [==============================] - 389s 2ms/step - loss: 3.5177 - acc: 0.2580\n",
      "Epoch 78/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.5078 - acc: 0.2588\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.5004 - acc: 0.2598\n",
      "Epoch 80/100\n",
      "178753/178753 [==============================] - 383s 2ms/step - loss: 3.4918 - acc: 0.2598\n",
      "Epoch 81/100\n",
      "178753/178753 [==============================] - 391s 2ms/step - loss: 3.4816 - acc: 0.2609\n",
      "Epoch 82/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.4725 - acc: 0.2631\n",
      "Epoch 83/100\n",
      "178753/178753 [==============================] - 390s 2ms/step - loss: 3.4610 - acc: 0.2643\n",
      "Epoch 84/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 3.4544 - acc: 0.2654\n",
      "Epoch 85/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 3.4469 - acc: 0.2661\n",
      "Epoch 86/100\n",
      "178753/178753 [==============================] - 382s 2ms/step - loss: 3.4371 - acc: 0.2678\n",
      "Epoch 87/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 3.4286 - acc: 0.2696\n",
      "Epoch 88/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.4218 - acc: 0.2692\n",
      "Epoch 89/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 3.4181 - acc: 0.2705\n",
      "Epoch 90/100\n",
      "178753/178753 [==============================] - 390s 2ms/step - loss: 3.4054 - acc: 0.2718\n",
      "Epoch 91/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.4004 - acc: 0.2727\n",
      "Epoch 92/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 3.3929 - acc: 0.2735\n",
      "Epoch 93/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.3859 - acc: 0.2743\n",
      "Epoch 94/100\n",
      "178753/178753 [==============================] - 385s 2ms/step - loss: 3.3761 - acc: 0.2760\n",
      "Epoch 95/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.3720 - acc: 0.2769\n",
      "Epoch 96/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.3646 - acc: 0.2773\n",
      "Epoch 97/100\n",
      "178753/178753 [==============================] - 384s 2ms/step - loss: 3.3578 - acc: 0.2784\n",
      "Epoch 98/100\n",
      "178753/178753 [==============================] - 387s 2ms/step - loss: 3.3509 - acc: 0.2787\n",
      "Epoch 99/100\n",
      "178753/178753 [==============================] - 386s 2ms/step - loss: 3.3481 - acc: 0.2786\n",
      "Epoch 100/100\n",
      "178753/178753 [==============================] - 388s 2ms/step - loss: 3.3383 - acc: 0.2806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c988780>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100) #training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model_advancedK.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizerK.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model_advancedK.h5')\n",
    "tokenizer = load(open('tokenizerK.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while grimace was too much blinded by her fury to notice what was going on the princess was quickly soaring out of her reach all this time souci had been wandering through the world with his precious thread carefully fastened round him seeking every possible and impossible place where his beloved\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grip of the carts of gold and the shells were beaten in the loneliness of the mistress singing a loud neck and thread forth on the immense ruins and plunged out against the trees and sang the gloomy schooner sobbing and knows fight to be the offering of the beasts and grapes i have been softened and i will never do to think of the king and the snowqueen stretch and steal on the ground and bites you wants you like a brushing and the jackal fetched me and i will go to the other said the king you must\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'K_output.txt'\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to be consider to advance the model: \n",
    "\n",
    "- We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "- MORE DATA!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

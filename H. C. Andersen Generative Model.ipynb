{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data (lesson 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting of by downloading data and looking at the data, which we need to clean. We observe following\n",
    "- Illustration texts\n",
    "- The text file contain multiple stories, these should be split into multiple files in order for the model to know there is groupings (Extension: will be reviewed in lesson 5)\n",
    "- The file contains a number of punctuations (,;.* etc.), which are irrelevant for the model\n",
    "- Capital letters as headers\n",
    "\n",
    "The input sequences are at first at 50 across sentences, chapters and stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to load documents into memory\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\") #loading an existing file\n",
    "\ttext = file.read() #opening the file and assigning it to the variable text\n",
    "\tfile.close() #close the file\n",
    "\treturn text #output = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate text files\n",
    "path = 'Data'\n",
    " \n",
    "files = os.listdir(path)\n",
    "\n",
    "filenames = ['Data/22693-0.txt', 'Data/2892-0.txt']\n",
    "\n",
    "with open('Data/adventures.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Just as a little child holds out its hands to catch the sunbeams, to\n",
      "feel and to grasp what, so its eyes tell it, is actually there, so,\n",
      "down through the ages, men have stretched out their hands in eager\n",
      "endeavour to know their God. And because only through the human was\n",
      "the divine knowable, the old peoples of the earth made gods of their\n",
      "heroes and not unfrequently endowed these gods with as many of the\n",
      "vices as of the virtues of their worshippers. As we read the myths of\n",
      "the East and the West we find ever the same story. That portion of the\n",
      "ancient Aryan race which poured from the central plain of Asia,\n",
      "through the rocky defiles of what we now call \"The Frontier,\" to\n",
      "populate the fertile lowlands of India, had gods who must once have\n",
      "been wholly heroic, but who came in time to be more degraded than the\n",
      "most vicious of lustful criminals. And the Greeks, Latins, Teutons,\n",
      "Celts, and Slavonians, who came of the same mighty Aryan stock, did\n",
      "even as those with whom they owned a common ancestry. Originally they\n",
      "gave to their gods of their best. All that was noblest in them, all\n",
      "that was strongest and most selfless, all the higher instincts of\n",
      "their natures were their endowment. And although their worship in time\n",
      "became corrupt and lost its beauty, there yet remains for us, in the\n",
      "old tales of the gods, a wonderful humanity that strikes a vibrant\n",
      "chord in the hearts of those who are the descendants of their\n",
      "worshippers. For though creeds and forms may change, human nature\n",
      "never changes. We are less simple than our fathers: that is all. And,\n",
      "as Professor York Powell[1] most truly says: \"It is not in a man's\n",
      "creed, but in his deeds; not in his knowledge, but in his sympathy,\n",
      "that there lies the essence of what is good and of what will last in\n",
      "human life.\"\n",
      "\n",
      "The most usual habits of mind in our own day are the theoretical and\n",
      "analytical habits. Dissection, vivisection, analysis--those are the\n",
      "processes to which all things not conclusively historical and all\n",
      "things spiritual are bound to pass. Thus we find the old myths\n",
      "classified into Sun Myths and Dawn Myths, Earth Myths and Moon Myths,\n",
      "Fire Myths and Wind Myths, until, as one of the most sane and vigorous\n",
      "thinkers of the present day[2] has justly observed: \"If you take the\n",
      "rhyme of Mary and her little lamb, and call Mary the sun and the lamb\n",
      "the moon, you will achieve astonishing results, both in religion and\n",
      "astronomy, when you find that the lamb followed Mary to school one\n",
      "day.\"\n",
      "\n",
      "In this little collection of Myths, the stories are not presented to\n",
      "the student of folklore as a fresh contribution to his knowledge.\n",
      "Rather is the book intended for those who, in the course of their\n",
      "reading, frequently come across names which possess for them no\n",
      "meaning, and who care to read some old stories, through which runs the\n",
      "same humanity that their own hearts know. For although the old worship\n",
      "has passed away, it is almost impossible for us to open a book that\n",
      "does not contain some mention of the gods of long ago. In our\n",
      "childhood we are given copies of Kingsley's _Heroes_ and of\n",
      "Hawthorne's _Tanglewood Tales_. Later on, we find in Shakespeare,\n",
      "Spenser, Milton, Keats, Shelley, Longfellow, Tennyson, Mrs. Browning,\n",
      "and a host of other writers, constant allusion to the stories of the\n",
      "gods. Scarcely a poet has ever written but makes mention of them in\n",
      "one or other of his poems. It would seem as if there were no get-away\n",
      "from them. We might expect in this twentieth century that the old gods\n",
      "of Greece and of Rome, the gods of our Northern forefathers, the gods\n",
      "of Egypt, the gods of the British race, might be forgotten. But even\n",
      "when we read in a newspaper of aeroplanes, someone is more than likely\n",
      "to quote the story of Bellerophon and his winged steed, or of Icarus,\n",
      "the flyer, and in our daily speech the names of gods and goddesses\n",
      "continually crop up. We drive--or, at least, till lately we drove--in\n",
      "Phaetons. Not only schoolboys swear by Jove or by Jupiter. The silvery\n",
      "substance in our thermometers and barometers is named Mercury.\n",
      "Blacksmiths are accustomed to being referred to as \"sons of Vulcan,\"\n",
      "and beautiful youths to being called \"young Adonises.\" We accept the\n",
      "names of newspapers and debating societies as being the \"Argus,\"\n",
      "without perhaps quite realising who was Argus, the many-eyed. We talk\n",
      "of \"a panic,\" and forget that the great god Pan is father of the word.\n",
      "Even in our religious services we go back to heathenism. Not only are\n",
      "the crockets on our cathedral spires and church pews remnants of\n",
      "fire-worship, but one of our own most beautiful Christian blessings is\n",
      "probably of Assyrian origin. \"The Lord bless thee and keep thee....\n",
      "The Lord make His face to shine upon thee.... The Lord lift up the\n",
      "light of His countenance upon thee....\" So did the priests of the\n",
      "sun-gods invoke blessings upon those who worshipped.\n",
      "\n",
      "We make many discoveries as we study the myths of the North and of the\n",
      "South. In the story of Baldur we find that the goddess Hel ultimately\n",
      "gave her name to the place of punishment precious to the Calvinistic\n",
      "mind. And because the Norseman very much disliked the bitter, cruel\n",
      "cold of the long winter, his heaven was a warm, well-fired abode, and\n",
      "his place of punishment one of terrible frigidity. Somewhere on the\n",
      "other side of the Tweed and Cheviots was the spot selected by the Celt\n",
      "of southern Britain. On the other hand, the eastern mind, which knew\n",
      "the terrors of a sun-smitten land and of a heat that was torture, had\n",
      "for a hell a fiery place of constantly burning flames.\n",
      "\n",
      "In the space permitted, it has not been possible to deal with more\n",
      "than a small number of myths, and the well-known stories of Herakles,\n",
      "of Theseus, and of the Argonauts have been purposely omitted. These\n",
      "have been so perfectly told by great writers that to retell them would\n",
      "seem absurd. The same applies to the Odyssey and the Iliad, the\n",
      "translations of which probably take rank amongst the finest\n",
      "translations in any language.\n",
      "\n",
      "The writer will feel that her object has been gained should any\n",
      "readers of these stories feel that for a little while they have left\n",
      "the toilful utilitarianism of the present day behind them, and, with\n",
      "it, its hampering restrictions of sordid actualities that are so\n",
      "murderous to imagination and to all romance.\n",
      "\n",
      "    \"Great God! I'd rather be\n",
      "    A Pagan suckled in a creed outworn;\n",
      "    So might I, standing on this pleasant lea,\n",
      "    Have glimpses that would make me less forlorn;\n",
      "    Have sight of Proteus rising from the sea;\n",
      "    Or hear old Triton blow his wreathèd horn.\"\n",
      "\n",
      "        JEAN LANG.\n",
      "\n",
      "\n",
      "POSTSCRIPT\n",
      "\n",
      "We have come, in those last long months, to date our happenings as\n",
      "they have never until now been dated by those of our own generation.\n",
      "\n",
      "We speak of things that took place \"_Before the War_\"; and between\n",
      "that time and this stands a barrier immeasurable.\n",
      "\n",
      "This book, with its Preface, was completed in 1914--\"_Before the\n",
      "War._\"\n",
      "\n",
      "Since August 1914 the finest humanity of our race has been enduring\n",
      "Promethean agonies. But even as Prometheus unflinchingly bore the\n",
      "cruelties of pain, of heat and of cold, of hunger and of thirst, and\n",
      "the tortures inflicted by an obscene bird of prey, so have endured the\n",
      "men of our nation and of those nations with whom we are proud to be\n",
      "allied. Much more remote than they seemed one little year ago, now\n",
      "seem the old stories of sunny Greece. But if we have studied the\n",
      "strange transmogrification of the ancient gods, we can look with\n",
      "interest, if with horror, at the Teuton representation of the GOD in\n",
      "whom we believe as a GOD of perfect purity, of honour, and of love.\n",
      "According to their interpretation of Him, the God of the Huns would\n",
      "seem to be as much a confederate of the vicious as the most degraded\n",
      "god of ancient worship. And if we turn with shame from the Divinity so\n",
      "often and so glibly referred to by blasphemous lips, and look on a\n",
      "picture that tears our hearts, and yet makes our hearts big with\n",
      "pride, we can understand how it was that those heroes who fought and\n",
      "died in the Valley of the Scamander came in time to be regarded not as\n",
      "men, but as gods.\n",
      "\n",
      "There is no tale in all the world's mythology finer than the tale that\n",
      "began in August 1914. How future generations will tell the tale, who\n",
      "can say?\n",
      "\n",
      "But we, for whom Life can never be the same again, can say with all\n",
      "earnestness: \"It is the memory that the soldier leaves behind him,\n",
      "like the long train of light that follows the sunken sun--that is all\n",
      "which is worth caring for, which distinguishes the death of the brave\n",
      "or the ignoble.\"\n",
      "\n",
      "And, surely, to all those who are fighting, and suffering, and dying\n",
      "for a noble cause, the GOD of gods, the GOD of battles, who is also\n",
      "the GOD of peace, and the GOD of Love, has become an ever near and\n",
      "eternally living entity.\n",
      "\n",
      "    \"Our little systems have their day;\n",
      "    They have their day and cease to be,\n",
      "    They are but broken lights of Thee,\n",
      "    And Thou, oh Lord, art more than they.\"\n",
      "\n",
      "        JEAN LANG.\n",
      "\n",
      "\n",
      "FOOTNOTES:\n",
      "\n",
      "[1] _Teutonic Heathendom._\n",
      "\n",
      "[2] John Kelman, D.D., _Among Famous Books_.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "                                                  PAGE\n",
      "\n",
      "  PROMETHEUS AND PANDORA                             1\n",
      "\n",
      "  PYGMALION                                         11\n",
      "\n",
      "  PHAETON                                           16\n",
      "\n",
      "  ENDYMION                                          26\n",
      "\n",
      "  ORPHEUS                                           31\n",
      "\n",
      "  APOLLO AND DAPHNE                                 42\n",
      "\n",
      "  PSYCHE                                            46\n",
      "\n",
      "  THE CALYDONIAN HUNT                               69\n",
      "\n",
      "  ATALANTA                                          78\n",
      "\n",
      "  ARACHNE                                           82\n",
      "\n",
      "  IDAS AND MARPESSA                                 90\n",
      "\n",
      "  ARETHUSA                                         100\n",
      "\n",
      "  PERSEUS THE HERO                                 105\n",
      "\n",
      "  NIOBE                                            124\n",
      "\n",
      "  HYACINTHUS                                       129\n",
      "\n",
      "  KING MIDAS OF THE GOLDEN TOUCH                   134\n",
      "\n",
      "  CEYX AND HALCYONE       \n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "in_filename = 'Data/adventures.txt'#specifying the filename of the data we wish to load\n",
    "doc = load_doc(in_filename) #loading the file\n",
    "print(doc[:10000]) #printing the first 200 characters of the loading document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data (lesson 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lesson 1, we observed several potential issues in the data, which we need to remove from the data. This is done below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing headers\n",
    "doc = re.sub(r'[A-Z]{2,}','', doc) #replacing capital letters longer than 1 with nothing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing special characters, since one of the deliminators\n",
    "#doc = doc.replace(\"'\", '')\n",
    "\n",
    "#deliminators \n",
    "#deliminators = c('THE FIR TREE', 'LITTLE TUK', 'THE UGLY DUCKLING', 'LITTLE IDA'S FLOWERS', 'THE STEADFAST TIN SOLDIER, LITTLE THUMBELINA, SUNSHINE STORIES', 'THE DARNING-NEEDLE', 'THE LITTLE MATCH GIRL', 'THE LOVING PAIR', 'THE LEAPING MATCH', 'THE HAPPY FAMILY, THE GREENIES, 'OLE-LUK-OIE', 'THE DREAM GOD', 'THE MONEY BOX', 'ELDER-TREE MOTHER', 'THE SNOW QUEEN', 'THE ROSES AND THE SPARROWS', 'THE OLD HOUSE', 'THE CONCEITED APPLE BRANCH' \n",
    "\n",
    "#splitting the file into strings consisting of one adventure\n",
    "#doc = doc.split ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc): #making a function\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token - https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\ttable = str.maketrans('', '', string.punctuation) #Third argument specifies the wished deleted items\n",
    "\ttokens = [w.translate(table) for w in tokens] #translate applies the translation table applied on the looping through the tokens list\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #Replace word with word, if the word is alphabetic\n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens]\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'a', 'little', 'child', 'holds', 'out', 'its', 'hands', 'to', 'catch', 'the', 'sunbeams', 'to', 'feel', 'and', 'to', 'grasp', 'what', 'so', 'its', 'eyes', 'tell', 'it', 'is', 'actually', 'there', 'so', 'down', 'through', 'the', 'ages', 'men', 'have', 'stretched', 'out', 'their', 'hands', 'in', 'eager', 'endeavour', 'to', 'know', 'their', 'god', 'and', 'because', 'only', 'through', 'the', 'human', 'was', 'the', 'divine', 'knowable', 'the', 'old', 'peoples', 'of', 'the', 'earth', 'made', 'gods', 'of', 'their', 'heroes', 'and', 'not', 'unfrequently', 'endowed', 'these', 'gods', 'with', 'as', 'many', 'of', 'the', 'vices', 'as', 'of', 'the', 'virtues', 'of', 'their', 'worshippers', 'as', 'we', 'read', 'the', 'myths', 'of', 'the', 'east', 'and', 'the', 'west', 'we', 'find', 'ever', 'the', 'same', 'story', 'that', 'portion', 'of', 'the', 'ancient', 'aryan', 'race', 'which', 'poured', 'from', 'the', 'central', 'plain', 'of', 'asia', 'through', 'the', 'rocky', 'defiles', 'of', 'what', 'we', 'now', 'call', 'the', 'frontier', 'to', 'populate', 'the', 'fertile', 'lowlands', 'of', 'india', 'had', 'gods', 'who', 'must', 'once', 'have', 'been', 'wholly', 'heroic', 'but', 'who', 'came', 'in', 'time', 'to', 'be', 'more', 'degraded', 'than', 'the', 'most', 'vicious', 'of', 'lustful', 'criminals', 'and', 'the', 'greeks', 'latins', 'teutons', 'celts', 'and', 'slavonians', 'who', 'came', 'of', 'the', 'same', 'mighty', 'aryan', 'stock', 'did', 'even', 'as', 'those', 'with', 'whom', 'they', 'owned', 'a', 'common', 'ancestry', 'originally', 'they', 'gave', 'to', 'their', 'gods', 'of', 'their', 'best', 'all', 'that', 'was', 'noblest', 'in']\n",
      "Total Tokens: 81430\n",
      "Unique Tokens: 7873\n"
     ]
    }
   ],
   "source": [
    "tokens = clean_doc(doc) #using the clean function to cleaning our document\n",
    "print(tokens[:200])\n",
    "print('Total Tokens: %d' % len(tokens)) #printing the number of tokens\n",
    "print('Unique Tokens: %d' % len(set(tokens))) #printing the number of unique tokens by grouping similair tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 81379\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1 #defining the length of the sequence\n",
    "sequences = list() #creating an empty list\n",
    "for i in range(length, len(tokens)): #range takes two arguments: start point and end point\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i-length:i]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq) #-join returns a string in which the string elements of sequence have been joined by str separator\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename): #creating a saving function\n",
    "\tdata = '\\n'.join(lines) #joining the lines seperated by 'enter'\n",
    "\tfile = open(filename, 'w', encoding=\"utf8\") #create empty new file\n",
    "\tfile.write(data) #input the data in the empty file\n",
    "\tfile.close() #close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "out_filename = 'Data/fairytales_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model (Lesson 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the cleaned data\n",
    "in_filename = 'Data/fairytales_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "\n",
    "#split the text by lineshifts creating a list of 51 items long string\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'a': 5,\n",
       " 'he': 6,\n",
       " 'that': 7,\n",
       " 'in': 8,\n",
       " 'was': 9,\n",
       " 'his': 10,\n",
       " 'it': 11,\n",
       " 'for': 12,\n",
       " 'with': 13,\n",
       " 'said': 14,\n",
       " 'her': 15,\n",
       " 'is': 16,\n",
       " 'as': 17,\n",
       " 'on': 18,\n",
       " 'not': 19,\n",
       " 'had': 20,\n",
       " 'she': 21,\n",
       " 'they': 22,\n",
       " 'i': 23,\n",
       " 'but': 24,\n",
       " 'at': 25,\n",
       " 'from': 26,\n",
       " 'him': 27,\n",
       " 'be': 28,\n",
       " 'you': 29,\n",
       " 'were': 30,\n",
       " 'all': 31,\n",
       " 'there': 32,\n",
       " 'my': 33,\n",
       " 'by': 34,\n",
       " 'when': 35,\n",
       " 'one': 36,\n",
       " 'fionn': 37,\n",
       " 'their': 38,\n",
       " 'then': 39,\n",
       " 'would': 40,\n",
       " 'so': 41,\n",
       " 'this': 42,\n",
       " 'king': 43,\n",
       " 'have': 44,\n",
       " 'who': 45,\n",
       " 'them': 46,\n",
       " 'will': 47,\n",
       " 'no': 48,\n",
       " 'or': 49,\n",
       " 'an': 50,\n",
       " 'which': 51,\n",
       " 'could': 52,\n",
       " 'me': 53,\n",
       " 'great': 54,\n",
       " 'into': 55,\n",
       " 'did': 56,\n",
       " 'man': 57,\n",
       " 'came': 58,\n",
       " 'are': 59,\n",
       " 'out': 60,\n",
       " 'what': 61,\n",
       " 'we': 62,\n",
       " 'do': 63,\n",
       " 'more': 64,\n",
       " 'been': 65,\n",
       " 'if': 66,\n",
       " 'your': 67,\n",
       " 'time': 68,\n",
       " 'men': 69,\n",
       " 'mongan': 70,\n",
       " 'went': 71,\n",
       " 'than': 72,\n",
       " 'like': 73,\n",
       " 'come': 74,\n",
       " 'mac': 75,\n",
       " 'these': 76,\n",
       " 'world': 77,\n",
       " 'yet': 78,\n",
       " 'love': 79,\n",
       " 'other': 80,\n",
       " 'up': 81,\n",
       " 'heart': 82,\n",
       " 'son': 83,\n",
       " 'where': 84,\n",
       " 'each': 85,\n",
       " 'own': 86,\n",
       " 'away': 87,\n",
       " 'must': 88,\n",
       " 'saw': 89,\n",
       " 'day': 90,\n",
       " 'any': 91,\n",
       " 'before': 92,\n",
       " 'down': 93,\n",
       " 'its': 94,\n",
       " 'again': 95,\n",
       " 'ireland': 96,\n",
       " 'eyes': 97,\n",
       " 'should': 98,\n",
       " 'through': 99,\n",
       " 'looked': 100,\n",
       " 'can': 101,\n",
       " 'those': 102,\n",
       " 'back': 103,\n",
       " 'hand': 104,\n",
       " 'go': 105,\n",
       " 'know': 106,\n",
       " 'only': 107,\n",
       " 'see': 108,\n",
       " 'made': 109,\n",
       " 'over': 110,\n",
       " 'shall': 111,\n",
       " 'little': 112,\n",
       " 'cried': 113,\n",
       " 'every': 114,\n",
       " 'gods': 115,\n",
       " 'now': 116,\n",
       " 'has': 117,\n",
       " 'long': 118,\n",
       " 'sea': 119,\n",
       " 'even': 120,\n",
       " 'am': 121,\n",
       " 'very': 122,\n",
       " 'land': 123,\n",
       " 'until': 124,\n",
       " 'himself': 125,\n",
       " 'never': 126,\n",
       " 'people': 127,\n",
       " 'knew': 128,\n",
       " 'head': 129,\n",
       " 'god': 130,\n",
       " 'might': 131,\n",
       " 'about': 132,\n",
       " 'woman': 133,\n",
       " 'duv': 134,\n",
       " 'look': 135,\n",
       " 'against': 136,\n",
       " 'replied': 137,\n",
       " 'goll': 138,\n",
       " 'nor': 139,\n",
       " 'place': 140,\n",
       " 'two': 141,\n",
       " 'good': 142,\n",
       " 'ever': 143,\n",
       " 'took': 144,\n",
       " 'asked': 145,\n",
       " 'gave': 146,\n",
       " 'such': 147,\n",
       " 'night': 148,\n",
       " 'while': 149,\n",
       " 'three': 150,\n",
       " 'young': 151,\n",
       " 'told': 152,\n",
       " 'roland': 153,\n",
       " 'also': 154,\n",
       " 'under': 155,\n",
       " 'still': 156,\n",
       " 'us': 157,\n",
       " 'upon': 158,\n",
       " 'after': 159,\n",
       " 'laca': 160,\n",
       " 'some': 161,\n",
       " 'well': 162,\n",
       " 'art': 163,\n",
       " 'how': 164,\n",
       " 'earth': 165,\n",
       " 'first': 166,\n",
       " 'way': 167,\n",
       " 'red': 168,\n",
       " 'eye': 169,\n",
       " 'beautiful': 170,\n",
       " 'heard': 171,\n",
       " 'father': 172,\n",
       " 'sweet': 173,\n",
       " 'thought': 174,\n",
       " 'tell': 175,\n",
       " 'our': 176,\n",
       " 'may': 177,\n",
       " 'face': 178,\n",
       " 'pan': 179,\n",
       " 'water': 180,\n",
       " 'white': 181,\n",
       " 'leinster': 182,\n",
       " 'another': 183,\n",
       " 'left': 184,\n",
       " 'last': 185,\n",
       " 'continued': 186,\n",
       " 'old': 187,\n",
       " 'stood': 188,\n",
       " 'seemed': 189,\n",
       " 'behind': 190,\n",
       " 'dear': 191,\n",
       " 'things': 192,\n",
       " 'nothing': 193,\n",
       " 'life': 194,\n",
       " 'loved': 195,\n",
       " 'many': 196,\n",
       " 'give': 197,\n",
       " 'brought': 198,\n",
       " 'person': 199,\n",
       " 'dog': 200,\n",
       " 'thing': 201,\n",
       " 'began': 202,\n",
       " 'thou': 203,\n",
       " 'put': 204,\n",
       " 'fianna': 205,\n",
       " 'wife': 206,\n",
       " 'sat': 207,\n",
       " 'called': 208,\n",
       " 'here': 209,\n",
       " 'set': 210,\n",
       " 'death': 211,\n",
       " 'although': 212,\n",
       " 'palace': 213,\n",
       " 'master': 214,\n",
       " 'because': 215,\n",
       " 'among': 216,\n",
       " 'dead': 217,\n",
       " 'held': 218,\n",
       " 'mind': 219,\n",
       " 'house': 220,\n",
       " 'once': 221,\n",
       " 'turned': 222,\n",
       " 'found': 223,\n",
       " 'known': 224,\n",
       " 'side': 225,\n",
       " 'voice': 226,\n",
       " 'trees': 227,\n",
       " 'hands': 228,\n",
       " 'whose': 229,\n",
       " 'too': 230,\n",
       " 'moment': 231,\n",
       " 'grew': 232,\n",
       " 'get': 233,\n",
       " 'though': 234,\n",
       " 'fell': 235,\n",
       " 'given': 236,\n",
       " 'sent': 237,\n",
       " 'joy': 238,\n",
       " 'indeed': 239,\n",
       " 'spoke': 240,\n",
       " 'women': 241,\n",
       " 'take': 242,\n",
       " 'charlemagne': 243,\n",
       " 'tara': 244,\n",
       " 'river': 245,\n",
       " 'end': 246,\n",
       " 'high': 247,\n",
       " 'make': 248,\n",
       " 'cael': 249,\n",
       " 'carl': 250,\n",
       " 'became': 251,\n",
       " 'together': 252,\n",
       " 'years': 253,\n",
       " 'arms': 254,\n",
       " 'servant': 255,\n",
       " 'knowledge': 256,\n",
       " 'round': 257,\n",
       " 'without': 258,\n",
       " 'wind': 259,\n",
       " 'fergus': 260,\n",
       " 'fear': 261,\n",
       " 'sun': 262,\n",
       " 'daughter': 263,\n",
       " 'got': 264,\n",
       " 'boy': 265,\n",
       " 'off': 266,\n",
       " 'stared': 267,\n",
       " 'let': 268,\n",
       " 'gone': 269,\n",
       " 'name': 270,\n",
       " 'dogs': 271,\n",
       " 'done': 272,\n",
       " 'sword': 273,\n",
       " 'children': 274,\n",
       " 'sky': 275,\n",
       " 'black': 276,\n",
       " 'words': 277,\n",
       " 'honour': 278,\n",
       " 'right': 279,\n",
       " 'looking': 280,\n",
       " 'towards': 281,\n",
       " 'lady': 282,\n",
       " 'most': 283,\n",
       " 'air': 284,\n",
       " 'soul': 285,\n",
       " 'finnian': 286,\n",
       " 'ulster': 287,\n",
       " 'golden': 288,\n",
       " 'grey': 289,\n",
       " 'word': 290,\n",
       " 'being': 291,\n",
       " 'terrible': 292,\n",
       " 'far': 293,\n",
       " 'between': 294,\n",
       " 'conn': 295,\n",
       " 'faery': 296,\n",
       " 'thus': 297,\n",
       " 'times': 298,\n",
       " 'think': 299,\n",
       " 'sound': 300,\n",
       " 'thy': 301,\n",
       " 'thee': 302,\n",
       " 'lay': 303,\n",
       " 'mother': 304,\n",
       " 'passed': 305,\n",
       " 'returned': 306,\n",
       " 'say': 307,\n",
       " 'salmon': 308,\n",
       " 'hag': 309,\n",
       " 'whom': 310,\n",
       " 'terror': 311,\n",
       " 'music': 312,\n",
       " 'dark': 313,\n",
       " 'days': 314,\n",
       " 'queen': 315,\n",
       " 'forward': 316,\n",
       " 'silver': 317,\n",
       " 'perseus': 318,\n",
       " 'wish': 319,\n",
       " 'fight': 320,\n",
       " 'chief': 321,\n",
       " 'becfola': 322,\n",
       " 'story': 323,\n",
       " 'darkness': 324,\n",
       " 'morning': 325,\n",
       " 'fire': 326,\n",
       " 'poet': 327,\n",
       " 'thousand': 328,\n",
       " 'tree': 329,\n",
       " 'child': 330,\n",
       " 'ask': 331,\n",
       " 'gold': 332,\n",
       " 'bring': 333,\n",
       " 'fair': 334,\n",
       " 'green': 335,\n",
       " 'ran': 336,\n",
       " 'reached': 337,\n",
       " 'hundred': 338,\n",
       " 'fiachna': 339,\n",
       " 'beauty': 340,\n",
       " 'alone': 341,\n",
       " 'something': 342,\n",
       " 'full': 343,\n",
       " 'body': 344,\n",
       " 'sons': 345,\n",
       " 'almost': 346,\n",
       " 'run': 347,\n",
       " 'answered': 348,\n",
       " 'much': 349,\n",
       " 'eat': 350,\n",
       " 'deirdrê': 351,\n",
       " 'nose': 352,\n",
       " 'find': 353,\n",
       " 'feet': 354,\n",
       " 'home': 355,\n",
       " 'others': 356,\n",
       " 'year': 357,\n",
       " 'small': 358,\n",
       " 'champion': 359,\n",
       " 'part': 360,\n",
       " 'beside': 361,\n",
       " 'always': 362,\n",
       " 'brothers': 363,\n",
       " 'battle': 364,\n",
       " 'going': 365,\n",
       " 'felt': 366,\n",
       " 'half': 367,\n",
       " 'die': 368,\n",
       " 'above': 369,\n",
       " 'demanded': 370,\n",
       " 'want': 371,\n",
       " 'morna': 372,\n",
       " 'uct': 373,\n",
       " 'dealv': 374,\n",
       " 'lîr': 375,\n",
       " 'cold': 376,\n",
       " 'answer': 377,\n",
       " 'birds': 378,\n",
       " 'apollo': 379,\n",
       " 'peace': 380,\n",
       " 'seen': 381,\n",
       " 'dream': 382,\n",
       " 'hill': 383,\n",
       " 'truth': 384,\n",
       " 'rose': 385,\n",
       " 'return': 386,\n",
       " 'deep': 387,\n",
       " 'leaped': 388,\n",
       " 'silence': 389,\n",
       " 'branch': 390,\n",
       " 'murmured': 391,\n",
       " 'spear': 392,\n",
       " 'cannot': 393,\n",
       " 'four': 394,\n",
       " 'call': 395,\n",
       " 'hearts': 396,\n",
       " 'surely': 397,\n",
       " 'coming': 398,\n",
       " 'new': 399,\n",
       " 'anything': 400,\n",
       " 'past': 401,\n",
       " 'leave': 402,\n",
       " 'help': 403,\n",
       " 'waves': 404,\n",
       " 'moved': 405,\n",
       " 'both': 406,\n",
       " 'blood': 407,\n",
       " 'feast': 408,\n",
       " 'legs': 409,\n",
       " 'wild': 410,\n",
       " 'ganelon': 411,\n",
       " 'iollan': 412,\n",
       " 'crimthann': 413,\n",
       " 'human': 414,\n",
       " 'same': 415,\n",
       " 'hounds': 416,\n",
       " 'light': 417,\n",
       " 'ground': 418,\n",
       " 'blue': 419,\n",
       " 'lived': 420,\n",
       " 'evil': 421,\n",
       " 'true': 422,\n",
       " 'kings': 423,\n",
       " 'meet': 424,\n",
       " 'within': 425,\n",
       " 'followed': 426,\n",
       " 'around': 427,\n",
       " 'hair': 428,\n",
       " 'sleep': 429,\n",
       " 'hear': 430,\n",
       " 'hard': 431,\n",
       " 'seven': 432,\n",
       " 'rage': 433,\n",
       " 'better': 434,\n",
       " 'tuan': 435,\n",
       " 'stranger': 436,\n",
       " 'happened': 437,\n",
       " 'becuma': 438,\n",
       " 'mighty': 439,\n",
       " 'psyche': 440,\n",
       " 'brother': 441,\n",
       " 'living': 442,\n",
       " 'creatures': 443,\n",
       " 'creature': 444,\n",
       " 'gift': 445,\n",
       " 'length': 446,\n",
       " 'everything': 447,\n",
       " 'bear': 448,\n",
       " 'wings': 449,\n",
       " 'island': 450,\n",
       " 'storm': 451,\n",
       " 'save': 452,\n",
       " 'mortal': 453,\n",
       " 'sight': 454,\n",
       " 'ears': 455,\n",
       " 'lovely': 456,\n",
       " 'killed': 457,\n",
       " 'fawn': 458,\n",
       " 'lost': 459,\n",
       " 'none': 460,\n",
       " 'across': 461,\n",
       " 'song': 462,\n",
       " 'sorrow': 463,\n",
       " 'hope': 464,\n",
       " 'rock': 465,\n",
       " 'mountain': 466,\n",
       " 'room': 467,\n",
       " 'played': 468,\n",
       " 'waters': 469,\n",
       " 'vast': 470,\n",
       " 'met': 471,\n",
       " 'mouth': 472,\n",
       " 'heavy': 473,\n",
       " 'country': 474,\n",
       " 'cave': 475,\n",
       " 'journey': 476,\n",
       " 'rest': 477,\n",
       " 'magic': 478,\n",
       " 'along': 479,\n",
       " 'cows': 480,\n",
       " 'agreed': 481,\n",
       " 'learned': 482,\n",
       " 'why': 483,\n",
       " 'prometheus': 484,\n",
       " 'eros': 485,\n",
       " 'power': 486,\n",
       " 'longer': 487,\n",
       " 'wonder': 488,\n",
       " 'live': 489,\n",
       " 'therefore': 490,\n",
       " 'wine': 491,\n",
       " 'wise': 492,\n",
       " 'walked': 493,\n",
       " 'stone': 494,\n",
       " 'drew': 495,\n",
       " 'low': 496,\n",
       " 'big': 497,\n",
       " 'sitting': 498,\n",
       " 'filled': 499,\n",
       " 'breast': 500,\n",
       " 'used': 501,\n",
       " 'died': 502,\n",
       " 'shore': 503,\n",
       " 'beloved': 504,\n",
       " 'ear': 505,\n",
       " 'close': 506,\n",
       " 'ye': 507,\n",
       " 'iron': 508,\n",
       " 'memory': 509,\n",
       " 'thin': 510,\n",
       " 'cleric': 511,\n",
       " 'conor': 512,\n",
       " 'swung': 513,\n",
       " 'staring': 514,\n",
       " 'caelte': 515,\n",
       " 'foot': 516,\n",
       " 'sheep': 517,\n",
       " 'wonderful': 518,\n",
       " 'speak': 519,\n",
       " 'itself': 520,\n",
       " 'born': 521,\n",
       " 'perfect': 522,\n",
       " 'athené': 523,\n",
       " 'lips': 524,\n",
       " 'laugh': 525,\n",
       " 'leaves': 526,\n",
       " 'sang': 527,\n",
       " 'themselves': 528,\n",
       " 'flame': 529,\n",
       " 'minute': 530,\n",
       " 'listened': 531,\n",
       " 'hall': 532,\n",
       " 'caught': 533,\n",
       " 'ten': 534,\n",
       " 'next': 535,\n",
       " 'naoise': 536,\n",
       " 'noise': 537,\n",
       " 'standing': 538,\n",
       " 'door': 539,\n",
       " 'however': 540,\n",
       " 'neck': 541,\n",
       " 'banquet': 542,\n",
       " 'hound': 543,\n",
       " 'ben': 544,\n",
       " 'race': 545,\n",
       " 'soon': 546,\n",
       " 'chariot': 547,\n",
       " 'anger': 548,\n",
       " 'fierce': 549,\n",
       " 'desire': 550,\n",
       " 'flew': 551,\n",
       " 'silent': 552,\n",
       " 'sad': 553,\n",
       " 'combat': 554,\n",
       " 'chase': 555,\n",
       " 'spring': 556,\n",
       " 'does': 557,\n",
       " 'lives': 558,\n",
       " 'fish': 559,\n",
       " 'beyond': 560,\n",
       " 'present': 561,\n",
       " 'near': 562,\n",
       " 'girl': 563,\n",
       " 'lord': 564,\n",
       " 'seeing': 565,\n",
       " 'move': 566,\n",
       " 'swans': 567,\n",
       " 'distance': 568,\n",
       " 'magicians': 569,\n",
       " 'bad': 570,\n",
       " 'remember': 571,\n",
       " 'treasure': 572,\n",
       " 'drove': 573,\n",
       " 'pain': 574,\n",
       " 'need': 575,\n",
       " 'having': 576,\n",
       " 'bird': 577,\n",
       " 'cruel': 578,\n",
       " 'rather': 579,\n",
       " 'o': 580,\n",
       " 'shame': 581,\n",
       " 'hung': 582,\n",
       " 'beast': 583,\n",
       " 'giving': 584,\n",
       " 'lonely': 585,\n",
       " 'sisters': 586,\n",
       " 'game': 587,\n",
       " 'smiled': 588,\n",
       " 'beat': 589,\n",
       " 'comes': 590,\n",
       " 'married': 591,\n",
       " 'messenger': 592,\n",
       " 'lifted': 593,\n",
       " 'glad': 594,\n",
       " 'forest': 595,\n",
       " 'delight': 596,\n",
       " 'pass': 597,\n",
       " 'taken': 598,\n",
       " 'gently': 599,\n",
       " 'scarcely': 600,\n",
       " 'often': 601,\n",
       " 'marsile': 602,\n",
       " 'food': 603,\n",
       " 'keep': 604,\n",
       " 'during': 605,\n",
       " 'kemoc': 606,\n",
       " 'bound': 607,\n",
       " 'husband': 608,\n",
       " 'direction': 609,\n",
       " 'finegas': 610,\n",
       " 'cow': 611,\n",
       " 'edair': 612,\n",
       " 'marpessa': 613,\n",
       " 'seem': 614,\n",
       " 'gifts': 615,\n",
       " 'flowers': 616,\n",
       " 'souls': 617,\n",
       " 'work': 618,\n",
       " 'forth': 619,\n",
       " 'thrust': 620,\n",
       " 'arm': 621,\n",
       " 'fate': 622,\n",
       " 'danger': 623,\n",
       " 'hold': 624,\n",
       " 'remembered': 625,\n",
       " 'angry': 626,\n",
       " 'changed': 627,\n",
       " 'heads': 628,\n",
       " 'poets': 629,\n",
       " 'bitter': 630,\n",
       " 'cheeks': 631,\n",
       " 'kind': 632,\n",
       " 'cup': 633,\n",
       " 'ere': 634,\n",
       " 'poor': 635,\n",
       " 'below': 636,\n",
       " 'knows': 637,\n",
       " 'grass': 638,\n",
       " 'calling': 639,\n",
       " 'chess': 640,\n",
       " 'shoulders': 641,\n",
       " 'finola': 642,\n",
       " 'just': 643,\n",
       " 'wide': 644,\n",
       " 'whatever': 645,\n",
       " 'kill': 646,\n",
       " 'road': 647,\n",
       " 'bran': 648,\n",
       " 'branduv': 649,\n",
       " 'tales': 650,\n",
       " 'less': 651,\n",
       " 'clouds': 652,\n",
       " 'horn': 653,\n",
       " 'whole': 654,\n",
       " 'carried': 655,\n",
       " 'zeus': 656,\n",
       " 'tongue': 657,\n",
       " 'enough': 658,\n",
       " 'rushed': 659,\n",
       " 'tears': 660,\n",
       " 'else': 661,\n",
       " 'fled': 662,\n",
       " 'soft': 663,\n",
       " 'touched': 664,\n",
       " 'tale': 665,\n",
       " 'wounded': 666,\n",
       " 'ease': 667,\n",
       " 'forget': 668,\n",
       " 'youth': 669,\n",
       " 'top': 670,\n",
       " 'stream': 671,\n",
       " 'clear': 672,\n",
       " 'blow': 673,\n",
       " 'lie': 674,\n",
       " 'bed': 675,\n",
       " 'suddenly': 676,\n",
       " 'placed': 677,\n",
       " 'ship': 678,\n",
       " 'ceased': 679,\n",
       " 'reason': 680,\n",
       " 'broke': 681,\n",
       " 'brow': 682,\n",
       " 'host': 683,\n",
       " 'prince': 684,\n",
       " 'servants': 685,\n",
       " 'myself': 686,\n",
       " 'matter': 687,\n",
       " 'aillen': 688,\n",
       " 'dermod': 689,\n",
       " 'cairell': 690,\n",
       " 'manycoloured': 691,\n",
       " 'questions': 692,\n",
       " 'either': 693,\n",
       " 'powerful': 694,\n",
       " 'olympus': 695,\n",
       " 'watched': 696,\n",
       " 'says': 697,\n",
       " 'force': 698,\n",
       " 'woe': 699,\n",
       " 'strength': 700,\n",
       " 'front': 701,\n",
       " 'laid': 702,\n",
       " 'follow': 703,\n",
       " 'strong': 704,\n",
       " 'wait': 705,\n",
       " 'angrily': 706,\n",
       " 'dreams': 707,\n",
       " 'dared': 708,\n",
       " 'doom': 709,\n",
       " 'winged': 710,\n",
       " 'whispered': 711,\n",
       " 'idas': 712,\n",
       " 'endless': 713,\n",
       " 'arethusa': 714,\n",
       " 'forgotten': 715,\n",
       " 'bent': 716,\n",
       " 'stepped': 717,\n",
       " 'brave': 718,\n",
       " 'venomous': 719,\n",
       " 'become': 720,\n",
       " 'roared': 721,\n",
       " 'running': 722,\n",
       " 'arose': 723,\n",
       " 'army': 724,\n",
       " 'question': 725,\n",
       " 'marry': 726,\n",
       " 'try': 727,\n",
       " 'movement': 728,\n",
       " 'champions': 729,\n",
       " 'monarch': 730,\n",
       " 'seat': 731,\n",
       " 'bargain': 732,\n",
       " 'astonished': 733,\n",
       " 'judgement': 734,\n",
       " 'stretched': 735,\n",
       " 'higher': 736,\n",
       " 'seized': 737,\n",
       " 'form': 738,\n",
       " 'beasts': 739,\n",
       " 'pallas': 740,\n",
       " 'since': 741,\n",
       " 'able': 742,\n",
       " 'upwards': 743,\n",
       " 'named': 744,\n",
       " 'grown': 745,\n",
       " 'care': 746,\n",
       " 'open': 747,\n",
       " 'order': 748,\n",
       " 'courage': 749,\n",
       " 'revenge': 750,\n",
       " 'stayed': 751,\n",
       " 'free': 752,\n",
       " 'mine': 753,\n",
       " 'thine': 754,\n",
       " 'shape': 755,\n",
       " 'chain': 756,\n",
       " 'equal': 757,\n",
       " 'gaze': 758,\n",
       " 'maid': 759,\n",
       " 'lad': 760,\n",
       " 'herself': 761,\n",
       " 'longing': 762,\n",
       " 'nymph': 763,\n",
       " 'sped': 764,\n",
       " 'second': 765,\n",
       " 'gazed': 766,\n",
       " 'beaten': 767,\n",
       " 'noble': 768,\n",
       " 'strange': 769,\n",
       " 'neither': 770,\n",
       " 'marched': 771,\n",
       " 'deeply': 772,\n",
       " 'grief': 773,\n",
       " 'breath': 774,\n",
       " 'hills': 775,\n",
       " 'promise': 776,\n",
       " 'yellow': 777,\n",
       " 'shield': 778,\n",
       " 'shoulder': 779,\n",
       " 'tooth': 780,\n",
       " 'play': 781,\n",
       " 'places': 782,\n",
       " 'ocean': 783,\n",
       " 'moon': 784,\n",
       " 'listen': 785,\n",
       " 'understand': 786,\n",
       " 'happy': 787,\n",
       " 'gentleman': 788,\n",
       " 'city': 789,\n",
       " 'sister': 790,\n",
       " 'pressed': 791,\n",
       " 'bones': 792,\n",
       " 'burst': 793,\n",
       " 'boar': 794,\n",
       " 'news': 795,\n",
       " 'usna': 796,\n",
       " 'dun': 797,\n",
       " 'kissed': 798,\n",
       " 'magician': 799,\n",
       " 'fingers': 800,\n",
       " 'instant': 801,\n",
       " 'uail': 802,\n",
       " 'finished': 803,\n",
       " 'captain': 804,\n",
       " 'groaned': 805,\n",
       " 'cloak': 806,\n",
       " 'tuiren': 807,\n",
       " 'despair': 808,\n",
       " 'certain': 809,\n",
       " 'lochlann': 810,\n",
       " 'climb': 811,\n",
       " 'watcher': 812,\n",
       " 'segda': 813,\n",
       " 'heroes': 814,\n",
       " 'west': 815,\n",
       " 'covered': 816,\n",
       " 'stopped': 817,\n",
       " 'growing': 818,\n",
       " 'happen': 819,\n",
       " 'epimethus': 820,\n",
       " 'proud': 821,\n",
       " 'remained': 822,\n",
       " 'royal': 823,\n",
       " 'led': 824,\n",
       " 'later': 825,\n",
       " 'exquisite': 826,\n",
       " 'opened': 827,\n",
       " 'minds': 828,\n",
       " 'dreamed': 829,\n",
       " 'horror': 830,\n",
       " 'makes': 831,\n",
       " 'friend': 832,\n",
       " 'companion': 833,\n",
       " 'grow': 834,\n",
       " 'rough': 835,\n",
       " 'pure': 836,\n",
       " 'smile': 837,\n",
       " 'war': 838,\n",
       " 'laughed': 839,\n",
       " 'joyously': 840,\n",
       " 'blast': 841,\n",
       " 'gentle': 842,\n",
       " 'wood': 843,\n",
       " 'winds': 844,\n",
       " 'cry': 845,\n",
       " 'shalt': 846,\n",
       " 'asleep': 847,\n",
       " 'slept': 848,\n",
       " 'seek': 849,\n",
       " 'remain': 850,\n",
       " 'short': 851,\n",
       " 'swept': 852,\n",
       " 'loving': 853,\n",
       " 'chill': 854,\n",
       " 'rocks': 855,\n",
       " 'birth': 856,\n",
       " 'turn': 857,\n",
       " 'woods': 858,\n",
       " 'deeds': 859,\n",
       " 'bush': 860,\n",
       " 'fought': 861,\n",
       " 'hour': 862,\n",
       " 'claim': 863,\n",
       " 'warriors': 864,\n",
       " 'bit': 865,\n",
       " 'messengers': 866,\n",
       " 'send': 867,\n",
       " 'saracen': 868,\n",
       " 'received': 869,\n",
       " 'wished': 870,\n",
       " 'household': 871,\n",
       " 'pushed': 872,\n",
       " 'monstrous': 873,\n",
       " 'hunt': 874,\n",
       " 'savage': 875,\n",
       " 'insisted': 876,\n",
       " 'eaten': 877,\n",
       " 'disappeared': 878,\n",
       " 'inquired': 879,\n",
       " 'few': 880,\n",
       " 'nobles': 881,\n",
       " 'coat': 882,\n",
       " 'conaran': 883,\n",
       " 'morgan': 884,\n",
       " 'catch': 885,\n",
       " 'abbot': 886,\n",
       " 'east': 887,\n",
       " 'plain': 888,\n",
       " 'change': 889,\n",
       " 'nature': 890,\n",
       " 'doing': 891,\n",
       " 'swiftly': 892,\n",
       " 'goddess': 893,\n",
       " 'dread': 894,\n",
       " 'wrath': 895,\n",
       " 'sacred': 896,\n",
       " 'council': 897,\n",
       " 'crown': 898,\n",
       " 'company': 899,\n",
       " 'gazing': 900,\n",
       " 'knowing': 901,\n",
       " 'refuse': 902,\n",
       " 'deathless': 903,\n",
       " 'secret': 904,\n",
       " 'raised': 905,\n",
       " 'ended': 906,\n",
       " 'till': 907,\n",
       " 'wall': 908,\n",
       " 'jealous': 909,\n",
       " 'greater': 910,\n",
       " 'sure': 911,\n",
       " 'parted': 912,\n",
       " 'straight': 913,\n",
       " 'daphne': 914,\n",
       " 'struck': 915,\n",
       " 'loves': 916,\n",
       " 'lover': 917,\n",
       " 'threw': 918,\n",
       " 'amongst': 919,\n",
       " 'mountains': 920,\n",
       " 'marriage': 921,\n",
       " 'sick': 922,\n",
       " 'awoke': 923,\n",
       " 'means': 924,\n",
       " 'flight': 925,\n",
       " 'hawk': 926,\n",
       " 'rivers': 927,\n",
       " 'horses': 928,\n",
       " 'sharp': 929,\n",
       " 'trouble': 930,\n",
       " 'course': 931,\n",
       " 'sought': 932,\n",
       " 'gloom': 933,\n",
       " 'merry': 934,\n",
       " 'terrified': 935,\n",
       " 'court': 936,\n",
       " 'counsel': 937,\n",
       " 'whisper': 938,\n",
       " 'wrapped': 939,\n",
       " 'skin': 940,\n",
       " 'slender': 941,\n",
       " 'waiting': 942,\n",
       " 'de': 943,\n",
       " 'beating': 944,\n",
       " 'stronghold': 945,\n",
       " 'thinking': 946,\n",
       " 'shining': 947,\n",
       " 'oliver': 948,\n",
       " 'sit': 949,\n",
       " 'erin': 950,\n",
       " 'songs': 951,\n",
       " 'singing': 952,\n",
       " 'anguish': 953,\n",
       " 'teeth': 954,\n",
       " 'beginning': 955,\n",
       " 'names': 956,\n",
       " 'welcome': 957,\n",
       " 'window': 958,\n",
       " 'drink': 959,\n",
       " 'chance': 960,\n",
       " 'observed': 961,\n",
       " 'admitted': 962,\n",
       " 'instruction': 963,\n",
       " 'five': 964,\n",
       " 'wolves': 965,\n",
       " 'cat': 966,\n",
       " 'lean': 967,\n",
       " 'tall': 968,\n",
       " 'gathered': 969,\n",
       " 'added': 970,\n",
       " 'mor': 971,\n",
       " 'deed': 972,\n",
       " 'pulse': 973,\n",
       " 'growled': 974,\n",
       " 'saeve': 975,\n",
       " 'ae': 976,\n",
       " 'noticed': 977,\n",
       " 'oscar': 978,\n",
       " 'talk': 979,\n",
       " 'ivell': 980,\n",
       " 'holy': 981,\n",
       " 'eager': 982,\n",
       " 'ill': 983,\n",
       " 'feel': 984,\n",
       " 'doors': 985,\n",
       " 'begged': 986,\n",
       " 'baby': 987,\n",
       " 'regard': 988,\n",
       " 'stories': 989,\n",
       " 'pandora': 990,\n",
       " 'titan': 991,\n",
       " 'hastened': 992,\n",
       " 'aphrodite': 993,\n",
       " 'hermes': 994,\n",
       " 'happiness': 995,\n",
       " 'marvelled': 996,\n",
       " 'slay': 997,\n",
       " 'beings': 998,\n",
       " 'loose': 999,\n",
       " 'tried': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer() #assign the tokenizer function to a variable\n",
    "tokenizer.fit_on_texts(lines) #function of keras finds all of the unique words in the data and assigns each a unique integer\n",
    "sequences = tokenizer.texts_to_sequences(lines) #translating the input lines into integers\n",
    "tokenizer.word_index #checking the dictionary of the transformed wordsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7874"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating vocabulary size to estimate the size of the embedding layer\n",
    "vocab_size = len(tokenizer.word_index) + 1 #since indexing of array are zero-offset, the index of the vocabulary must be one larger than the length\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences) #transforming the sequens of integers to arrays\n",
    "X, y = sequences[:,:-1], sequences[:,-1] #defining X (input sequences) and y (output words)\n",
    "y = to_categorical(y, num_classes=vocab_size) #to_categorical converts a class vector (integers) to binary class matrix\n",
    "seq_length = X.shape[1] #gives you the dimension of the array, which we put to be 50\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 50)            393700    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 7874)              795274    \n",
      "=================================================================\n",
      "Total params: 1,339,874\n",
      "Trainable params: 1,339,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 50\n",
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Embedding(vocab_size, embed_dim, input_length=seq_length)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "# fit model\n",
    "#model.fit(X, y, batch_size=128, epochs=100) #training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned text sequences\n",
    "doc = load_doc('Data/fairytales_sequences.txt')\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying input sequences length to prompt the model\n",
    "seq_length = len(lines[3].split()) - 1 #splitting the sequences into words, counting them -1\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protect he continued see you killed said he lamentably am getting down now to fight the will not hurt said the stranger are the king asked am the son of fiachna knew then that the stranger could not be hurt will you give me if i deliver you from the asked\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting a seed text\n",
    "seed_text = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translating the input text to integers\n",
    "encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can predict the next word directly by calling model.predict_classes() that will return the index of the word with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 1s 527ms/step\n"
     ]
    }
   ],
   "source": [
    "# predict probabilities for each word\n",
    "yhat = model.predict_classes(encoded, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up the index in the Tokenizers mapping to get the associated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certainly'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making a loop to translate integer to word\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "\tif index == yhat:\n",
    "\t\tout_word = word\n",
    "\t\tbreak\n",
    "\n",
    "out_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequences will get too long, in order to keep them to 50 items using the following function, which pads sequences to the same lengt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the max length to be 50 items by removing items from the beginnning of the sequence\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a function, which generates the predicted output\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list() #make an empty list\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "certainly younger younger bees line ratio ratio agreeable agreeable agreeable inspires dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dizzy dew dew dizzy dew dew dew dew soundly soundly rage rage seventh seventh doorway duck duck duck feel feel heaven heaven gigantic gigantic gigantic line pluck lamentably lamentably lamentably lamentably twist ink ink ink ink ink ink ihr ihr besought besought pluck pluck wellpacked wellpacked wellpacked line ihr ihr line prepared protects ensued ensued loves loves loves curse curse dodge dodge dodge complained complained finnachy finnachy finnachy easier easier easier easier informant\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'Data/first_output.txt'\n",
    "save_doc(generated, out_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Advancing the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Word Embedding. Extend the model to use pre-trained word2vec or GloVe vectors to see if it results in a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('Data/glove.6B/glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(encoded, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to be consider to advance the model: \n",
    "\n",
    "- We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "- MORE DATA!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ldm)",
   "language": "python",
   "name": "ldm_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

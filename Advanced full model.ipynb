{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A generative model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we did was to download Hans Christian Andersen fairytales from Gutenberg.\n",
    "\n",
    "What do we need to look out for when cleaning the data:\n",
    "\n",
    "- Punctuation \n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "- Capital letters as headers, might want to delete these\n",
    "- Change everything to lower caser \n",
    "- Illustrations, delete them, written as ([Illustration: _His limbs were numbed, his beautiful eyes were closing.-])\n",
    "\n",
    "\n",
    "We have to decide the input sequences, which is the number of words that the model will train on. This is also the number of words which will be used as the seed text, when using the model to actually generate new sequences. \n",
    "\n",
    "In the tutorial 50 words are suggested. We will start with that and then in the fifth lesson we might try to only use sequences inside sentences, though this will minimize the amount of training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on the following two coding blocks:\n",
    "https://www.pythonforbeginners.com/files/reading-and-writing-files-in-python\n",
    "- How to open text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "\n",
    "from __future__ import print_function\n",
    "import re\n",
    "import string\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "from pickle import load\n",
    "from random import randint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Masking\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to load the text into memory\n",
    "def load_text(filename): #we decide that the function is called load_text\n",
    "    file = open(filename, 'r', encoding = 'utf-8') #open() is used to open/loading a filename, the mode indicates how it should be opened, r = read, w = writing, a = appending \n",
    "    text = file.read() # read the file and assign it to the variable text\n",
    "    file.close() #close the file again\n",
    "    return text #ends function and specify what output the want, we want text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path and working directory\n",
    "os.chdir(\"C:\\\\Users\\\\au591024\\\\Desktop\\\\Data\")\n",
    "path = \"C:\\\\Users\\\\au591024\\\\Desktop\\\\Data\"\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "#concatenate text files\n",
    "filenames = files\n",
    "\n",
    "\n",
    "with open('adventures.txt', 'w', encoding=\"utf8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf8\") as infile:\n",
    "            outfile.write(infile.read())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Just as a little child holds out its hands to catch the sunbeams, to\n",
      "feel and to grasp what, so its eyes tell it, is actually there, so,\n",
      "down through the ages, men have stretched out their hands in e\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load document\n",
    "in_filename = 'adventures.txt'\n",
    "doc = load_text(in_filename)\n",
    "print(doc[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2 - clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing illustration descriptions\n",
    "doc = re.sub(r'\\[[^)]*\\]', '', doc) #using re.sub function and regular expressions \n",
    "#[ - an opening bracket \n",
    "#[^()]* - zero or more characters other than those defined, that is, any characters other than [ and ]\n",
    "#\\] - a closing bracket\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Just as a little child holds out its hands to catch the sunbeams, to\n",
      "feel and to grasp what, so its eyes tell it, is actually there, so,\n",
      "down through the ages, men have stretched out their hands in eager\n",
      "endeavour to know their God. And because only through the human was\n",
      "the divine knowable, the old peoples of the earth made gods of their\n",
      "heroes and not unfrequently endowed these gods with as many of the\n",
      "vices as of the virtues of their worshippers. As we read the myths of\n",
      "the East and the West we find ever the same story. That portion of the\n",
      "ancient Aryan race which poured from the central plain of Asia,\n",
      "through the rocky defiles of what we now call \"The Frontier,\" to\n",
      "populate the fertile lowlands of India, had gods who must once have\n",
      "been wholly heroic, but who came in time to be more degraded than the\n",
      "most vicious of lustful criminals. And the Greeks, Latins, Teutons,\n",
      "Celts, and Slavonians, who came of the same mighty Aryan stock, did\n",
      "even as those with whom they owned a common anc\n"
     ]
    }
   ],
   "source": [
    "#remove headers\n",
    "doc = re.sub(r'[A-Z]{2,}', '', doc) #remove capital letters everytime there is more than two \n",
    "\n",
    "print(doc[:1000]) #lets look at it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for the following code: https://www.geeksforgeeks.org/python-maketrans-translate-functions/\n",
    "\n",
    "How to use translate together with the helper function maketrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the document into clean tokens\n",
    "\n",
    "\n",
    "def clean_doc(doc): #make a function called clean_doc\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split() #get a list with all words (tokens)\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation) #third argument specifies the character we want to delete\n",
    "\ttokens = [w.translate(table) for w in tokens] #use translation mapping from table and loop through all words in token\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()] #isalpha() checks whether the string consists of alphabetic characters only. Replace word with word if it is alphabetical. \n",
    "\t# make lower case\n",
    "\ttokens = [word.lower() for word in tokens] #replace word in lower case with word in token \n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following coding blocks are part of lesson 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\au591024\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "80842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Originally they\\ngave to their gods of their best.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load nltk library\n",
    "#nltk.download('punkt')\n",
    "\n",
    "#define tokenizer, use pretrained from the corpus\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#assign doc to data\n",
    "data = doc\n",
    "\n",
    "#divide into sentences\n",
    "sentence_list = sent_tokenize(data)\n",
    "\n",
    "#print the number of sentences\n",
    "print(len(sentence_list))\n",
    "\n",
    "#check the type, it's a list\n",
    "type(sentence_list)\n",
    "\n",
    "#index to see an example\n",
    "sentence_list[5] #need to remove the punctuation and have in lower case\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that', 'portion', 'of', 'the', 'ancient', 'aryan', 'race', 'which', 'poured', 'from', 'the', 'central', 'plain', 'of', 'asia', 'through', 'the', 'rocky', 'defiles', 'of', 'what', 'we', 'now', 'call', 'the', 'frontier', 'to', 'populate', 'the', 'fertile', 'lowlands', 'of', 'india', 'had', 'gods', 'who', 'must', 'once', 'have', 'been', 'wholly', 'heroic', 'but', 'who', 'came', 'in', 'time', 'to', 'be', 'more', 'degraded', 'than', 'the', 'most', 'vicious', 'of', 'lustful', 'criminals']\n",
      "['originally', 'they', 'gave', 'to', 'their', 'gods', 'of', 'their', 'best']\n"
     ]
    }
   ],
   "source": [
    "#check if it works on one sentence in the sentence list\n",
    "print(clean_doc(sentence_list[3])) #it does\n",
    "\n",
    "#make an empty list\n",
    "sentence_clean_list = []\n",
    "\n",
    "#make a loop which runs through the list of sentences and uses the clean_doc() function\n",
    "for sentence in sentence_list:\n",
    "    y = clean_doc(sentence) #save in y and append to empty list\n",
    "    sentence_clean_list.append(y)\n",
    "    \n",
    "\n",
    "#print(sentence_clean_list[:10])\n",
    "\n",
    "#check to see if the punctuation is removed and if it's in lower case\n",
    "print(sentence_clean_list[5]) #it is\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to check the length of the sentences, as we need to use this then either padding or truncating the sentences into sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "348\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#check the length of a random sentence\n",
    "print(len(sentence_clean_list[3]))\n",
    "\n",
    "length = []\n",
    "for sentence in sentence_clean_list:\n",
    "    l = len(sentence)\n",
    "    length.append(l)\n",
    "\n",
    "    \n",
    "print(max(length))\n",
    "\n",
    "print(min(length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "1\n",
      "Sentences removed after deleting sentences shorter than 1: 393\n",
      "Sentences removed after deleting sentences longer than : 97\n"
     ]
    }
   ],
   "source": [
    "#remove sentences less than 1 word long and longer than 130\n",
    "sentence_new_list1 = [s for s in sentence_clean_list if len(s) >= 1] \n",
    "sentence_new_list = [s for s in sentence_new_list1 if len(s) <= 130]\n",
    "\n",
    "\n",
    "length_new = []\n",
    "for sentence in sentence_new_list:\n",
    "    l = len(sentence)\n",
    "    length_new.append(l)\n",
    "\n",
    "    \n",
    "print(max(length_new))\n",
    "\n",
    "print(min(length_new))\n",
    "\n",
    "\n",
    "#check how many sentences we have removed\n",
    "print('Sentences removed after deleting sentences shorter than 1:', len(sentence_clean_list) - len(sentence_new_list1) )\n",
    "print('Sentences removed after deleting sentences longer than :', len(sentence_new_list1) - len(sentence_new_list)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originally they gave to their gods of their best\n",
      "as a little child holds out its hands to catch the sunbeams to feel and to grasp what so its eyes tell it is actually there so down through the ages men have stretched out their hands in eager endeavour to know their god\n"
     ]
    }
   ],
   "source": [
    "# turn into (not separated) sentence, check method on number 5 in the list\n",
    "line = ' '.join(sentence_new_list[5]) \n",
    "\n",
    "print(line) #it works\n",
    "\n",
    "#create empty list\n",
    "sequences_sent = []\n",
    "\n",
    "#create a loop to do it at all the sentences\n",
    "for sentence in sentence_new_list:\n",
    "    line = ' '.join(sentence)\n",
    "    sequences_sent.append(line)\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentences to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w', encoding = 'utf-8')\n",
    "\tfile.write(data)\n",
    "\tfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sentences to file\n",
    "out_filename = 'clean_text_sentences.txt'\n",
    "save_doc(sequences_sent, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as a little child holds out its hands to catch the sunbeams to feel and to grasp what so its eyes tell it is actually there so down through the ages men have stretched out their hands in eager endeavour to know their god\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80352"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the cleaned data with sentences\n",
    "doc = load_text('clean_text_sentences.txt')\n",
    "\n",
    "#split the text according to line shift\n",
    "lines = doc.split('\\n')\n",
    "\n",
    "#check the type\n",
    "type(lines)#its a list\n",
    "\n",
    "\n",
    "print(lines[0])\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 5, 49, 265, 4280, 36, 135, 262, 3, 860, 1, 4529, 3, 616, 2, 3, 4401, 51, 27, 135, 168, 156, 11, 31, 2426, 45, 27, 63, 145, 1, 4530, 210, 34, 832, 36, 50, 262, 8, 2387, 12099, 3, 125, 50, 639]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer() #define function Tokenizer() as the variable tokenizer\n",
    "tokenizer.fit_on_texts(lines) #fit_on_text() tells it what data to train on. We train it on the entire training data, finds all of the unique words in the data and assigns each a unique integer, from 1 to the total number of words\n",
    "sequences_sent = tokenizer.texts_to_sequences(lines) #convert each sequence from a list of words to a list of integers\n",
    "\n",
    "#print(sequences_sent[:100]) #all sequences are represented by vectors, consisting of an integer corresponding to each word in the vector\n",
    "\n",
    "tokenizer.word_index #check out the mapping of integers to words\n",
    "\n",
    "print(sequences_sent[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model all the input sequences need to be the same length. We therefore need to pad or truncate the different sentences. When you pad the sequences, you add zeros, so that they all become the same length (lenght of the longest sentence. In that case we will have to create an masking input layer which will ignore padded values. This means that padded inputs have no impact on learning. But we will start by padding all the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   125    50   639]\n",
      " [    0     0     0 ...     4    50 14117]\n",
      " [    0     0     0 ...     1   214   347]\n",
      " ...\n",
      " [    0     0     0 ...     2  1308    82]\n",
      " [    0     0     0 ...     1   633   863]\n",
      " [    0     0     0 ...     4    13   242]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#will padd zeros in front, pre is the default option\n",
    "padded = pad_sequences(sequences_sent)\n",
    "print(padded)\n",
    "\n",
    "\n",
    "len(padded[3]) #all sequences are now 130 long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28730"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1 #+1 because indexing of arrays starts at zero, and the first word is assigned the integer of 1\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...    3  125   50]\n",
      " [   0    0    0 ... 4402    4   50]\n",
      " [   0    0    0 ...  188    1  214]\n",
      " ...\n",
      " [   0    0    0 ... 1260    2 1308]\n",
      " [   0    0    0 ...   19    1  633]\n",
      " [   0    0    0 ...  255    4   13]]\n",
      "[  639 14117   347 ...    82   863   242]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "1.0\n",
      "0.0\n",
      "129\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences_sent = array(padded) #create an array with the sequences\n",
    "\n",
    "X, y = sequences_sent[:,:-1], sequences_sent[:,-1] #separate into input and output sequences\n",
    "print(X) #input\n",
    "print(y) #output\n",
    "y = to_categorical(y, num_classes=vocab_size) #convert to one-hot encoding\n",
    "seq_length_sent = X.shape[1] #set sequence length to the number of columns in X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function for generating new sequences \n",
    "def generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, n_words): #n_words is the number of words to generate \n",
    "\tresult = list() #create and empty list\n",
    "\tin_text = seed_text_sentence #assign the see_text to a variable \n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words): #range(), first argument is the number of integers to generate, starting from zero\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0] #take the first sequence\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length_sentence, truncating='pre') #make sure that the sequence is 50 words\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word #add the output word to the sequence we are currently working with \n",
    "\t\tresult.append(out_word) #append the word to the empty list\n",
    "\treturn ' '.join(result) #''.join creates a space between each word in the result list and makes a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to look at in lecture 5 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Trained Word Embedding. Extend the model to use pre-trained word2vec or GloVe vectors to see if it results in a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('Data/glove.6B/glove.6B.100d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential() #assigning the sequential function to a model\n",
    "model.add(Masking(mask_value=0, input_shape=(seq_length_sent,)))\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=seq_length_sent, trainable = False)) #defining embedding layer size\n",
    "model.add(LSTM(100, return_sequences=True)) #adding layer of nodes\n",
    "model.add(LSTM(100))  #adding layer of nodes\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu')) #specifying the structure of the hidden layer, recu is an argument of a rectified linear unit. \n",
    "model.add(Dense(vocab_size, activation='softmax')) #using the softmax function to creating probabilities\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #the compile function configures the model for training specifying the categorical cross entropy loss\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=12, epochs=1) #training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model_advanced_full.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_advanced.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "model= load_model('model_advanced_full.h5')\n",
    "tokenizer = load(open('tokenizer_advanced.pkl', 'rb')) #r for read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9bca8c9834b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#selecting a seed text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mseed_text_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#returns random integer between 0 and how many lines there is, and indexes this.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_text\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#prints the selected text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "#selecting a seed text\n",
    "seed_text_sentence = lines[randint(0,len(lines))] #returns random integer between 0 and how many lines there is, and indexes this. \n",
    "print(seed_text + '\\n') #prints the selected text\n",
    "len(seed_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length_sent, seed_text_sentence, 100)\n",
    "print(generated)\n",
    "\n",
    "#saving generated text\n",
    "out_filename = 'Data/advanced_output.txt'\n",
    "save_doc(generated, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input sequences\n",
    "\n",
    "\n",
    "We could process the data so that the model only ever deals with self-contained sentences and pad or truncate the text to meet this requirement for each input sequence. You could explore this as an extension to this tutorial.\n",
    "- Instead, to keep the example brief, we will let all of the text flow together and train the model to predict the next word across sentences, paragraphs, and even books or chapters in the text.\n",
    "\n",
    "- The file consist of multiple stories --> need to divide it into different parts (might be possible to split after headers written in capital letters) --> we will do this in lesson 5\n",
    "\n",
    "- FInd more fairytales, data is not big enough\n",
    "\n",
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
